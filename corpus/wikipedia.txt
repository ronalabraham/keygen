A book is a medium for recording information in the form of writing or images, typically composed of many pages (made of papyrus, parchment, vellum, or paper) bound together and protected by a cover. The technical term for this physical arrangement is codex (plural, codices). In the history of hand-held physical supports for extended written compositions or records, the codex replaces its predecessor, the scroll.  A single sheet in a codex is a leaf and each side of a leaf is a page.
As an intellectual object, a book is prototypically a composition of such great length that it takes a considerable investment of time to compose and still considered as an investment of time to read. In a restricted sense, a book is a self-sufficient section or part of a longer composition, a usage that reflects the fact that, in antiquity, long works had to be written on several scrolls and each scroll had to be identified by the book it contained. Each part of Aristotle's Physics is called a book. In an unrestricted sense, a book is the compositional whole of which such sections, whether called books or chapters or parts, are parts.
The intellectual content in a physical book need not be a composition, nor even be called a book. Books can consist only of drawings, engravings or photographs, crossword puzzles or cut-out dolls.  In a physical book, the pages can be left blank or can feature an abstract set of lines to support entries, such as in an account book, an appointment book, an autograph book, a notebook, a diary or a sketchbook.  Some physical books are made with pages thick and sturdy enough to support other physical objects, like a scrapbook or photograph album. Books may be distributed in electronic form as e-books and other formats.
Although in ordinary academic parlance a monograph is understood to be a specialist academic work, rather than a reference work on a scholarly subject, in library and information science monograph denotes more broadly any non-serial publication complete in one volume (book) or a finite number of volumes (even a novel like Proust's seven-volume In Search of Lost Time), in contrast to serial publications like a magazine, journal or newspaper. An avid reader or collector of books is a bibliophile or colloquially, "bookworm". A place where books are traded is a bookshop or bookstore. Books are also sold elsewhere and can be borrowed from libraries. Google has estimated that by 2010, approximately 130,000,000 titles had been published. In some wealthier nations, the sale of printed books has decreased because of the increased usage of e-books.


Etymology
The word book comes from Old English bc, which in turn comes from the Germanic root *bk-, cognate to 'beech'. In Slavic languages like Russian, Bulgarian, Macedonian  bukva'letter' is cognate with 'beech'. In Russian, Serbian and Macedonian, the word  (bukvar') or  (bukvar) refers to a primary school textbook that helps young children master the techniques of reading and writing. It is thus conjectured that the earliest Indo-European writings may have been carved on beech wood. The Latin word codex, meaning a book in the modern sense (bound and with separate leaves), originally meant 'block of wood'.


History


Antiquity

When writing systems were created in ancient civilizations, a variety of objects, such as stone, clay, tree bark, metal sheets, and bones, were used for writing; these are studied in epigraphy.


Tablet

A tablet is a physically robust writing medium, suitable for casual transport and writing. Clay tablets were flattened and mostly dry pieces of clay that could be easily carried, and impressed with a stylus. They were used as a writing medium, especially for writing in cuneiform, throughout the Bronze Age and well into the Iron Age. Wax tablets were pieces of wood covered in a coating of wax thick enough to record the impressions of a stylus. They were the normal writing material in schools, in accounting, and for taking notes. They had the advantage of being reusable: the wax could be melted, and reformed into a blank.
The custom of binding several wax tablets together (Roman pugillares) is a possible precursor of modern bound (codex) books. The etymology of the word codex (block of wood) also suggests that it may have developed from wooden wax tablets.


Scroll

Scrolls can be made from papyrus, a thick paper-like material made by weaving the stems of the papyrus plant, then pounding the woven sheet with a hammer-like tool until it is flattened. Papyrus was used for writing in Ancient Egypt, perhaps as early as the First Dynasty, although the first evidence is from the account books of King Neferirkare Kakai of the Fifth Dynasty (about 2400 BC). Papyrus sheets were glued together to form a scroll. Tree bark such as lime and other materials were also used.According to Herodotus (History 5:58), the Phoenicians brought writing and papyrus to Greece around the 10th or 9th century BC. The Greek word for papyrus as writing material (biblion) and book (biblos) come from the Phoenician port town Byblos, through which papyrus was exported to Greece. From Greek we also derive the word tome (Greek: ), which originally meant a slice or piece and from there began to denote "a roll of papyrus". Tomus was used by the Latins with exactly the same meaning as volumen (see also below the explanation by Isidore of Seville).
Whether made from papyrus, parchment, or paper, scrolls were the dominant form of book in the Hellenistic, Roman, Chinese, Hebrew, and Macedonian cultures. The more modern codex book format form took over the Roman world by late antiquity, but the scroll format persisted much longer in Asia.


Codex

Isidore of Seville (d. 636) explained the then-current relation between codex, book and scroll in his Etymologiae (VI.13): "A codex is composed of many books; a book is of one scroll. It is called codex by way of metaphor from the trunks (codex) of trees or vines, as if it were a wooden stock, because it contains in itself a multitude of books, as it were of branches." Modern usage differs.
A codex (in modern usage) is the first information repository that modern people would recognize as a "book": leaves of uniform size bound in some manner along one edge, and typically held between two covers made of some more robust material. The first written mention of the codex as a form of book is from Martial, in his Apophoreta CLXXXIV at the end of the first century, where he praises its compactness. However, the codex never gained much popularity in the pagan Hellenistic world, and only within the Christian community did it gain widespread use. This change happened gradually during the 3rd and 4th centuries, and the reasons for adopting the codex form of the book are several: the format is more economical, as both sides of the writing material can be used; and it is portable, searchable, and easy to conceal. A book is much easier to read, to find a page that you want, and to flip through. A scroll is more awkward to use. The Christian authors may also have wanted to distinguish their writings from the pagan and Judaic texts written on scrolls. In addition, some metal books were made, that required smaller pages of metal, instead of an impossibly long, unbending scroll of metal. A book can also be easily stored in more compact places, or side by side in a tight library or shelf space.


Manuscripts

The fall of the Roman Empire in the 5th century AD saw the decline of the culture of ancient Rome. Papyrus became difficult to obtain due to lack of contact with Egypt, and parchment, which had been used for centuries, became the main writing material. Parchment is a material made from processed animal skin and usedmainly in the pastfor writing on.
Parchment is most commonly made of calfskin, sheepskin, or goatskin. It was historically used for writing documents, notes, or the pages of a book. Parchment is limed, scraped and dried under tension. It is not tanned, and is thus different from leather. This makes it more suitable for writing on, but leaves it very reactive to changes in relative humidity and makes it revert to rawhide if overly wet.
Monasteries carried on the Latin writing tradition in the Western Roman Empire. Cassiodorus, in the monastery of Vivarium (established around 540), stressed the importance of copying texts. St. Benedict of Nursia, in his Rule of Saint Benedict (completed around the middle of the 6th century) later also promoted reading. The Rule of Saint Benedict (Ch. XLVIII), which set aside certain times for reading, greatly influenced the monastic culture of the Middle Ages and is one of the reasons why the clergy were the predominant readers of books. The tradition and style of the Roman Empire still dominated, but slowly the peculiar medieval book culture emerged.

Before the invention and adoption of the printing press, almost all books were copied by hand, which made books expensive and comparatively rare. Smaller monasteries usually had only a few dozen books, medium-sized perhaps a few hundred. By the 9th century, larger collections held around 500 volumes and even at the end of the Middle Ages, the papal library in Avignon and Paris library of the Sorbonne held only around 2,000 volumes.The scriptorium of the monastery was usually located over the chapter house. Artificial light was forbidden for fear it may damage the manuscripts. There were five types of scribes:

Calligraphers, who dealt in fine book production
Copyists, who dealt with basic production and correspondence
Correctors, who collated and compared a finished book with the manuscript from which it had been produced
Illuminators, who painted illustrations
Rubricators, who painted in the red letters
The bookmaking process was long and laborious. The parchment had to be prepared, then the unbound pages were planned and ruled with a blunt tool or lead, after which the text was written by the scribe, who usually left blank areas for illustration and rubrication. Finally, the book was bound by the bookbinder.

Different types of ink were known in antiquity, usually prepared from soot and gum, and later also from gall nuts and iron vitriol. This gave writing a brownish black color, but black or brown were not the only colors used. There are texts written in red or even gold, and different colors were used for illumination. For very luxurious manuscripts the whole parchment was colored purple, and the text was written on it with gold or silver (for example, Codex Argenteus).Irish monks introduced spacing between words in the 7th century. This facilitated reading, as these monks tended to be less familiar with Latin. However, the use of spaces between words did not become commonplace before the 12th century. It has been argued that the use of spacing between words shows the transition from semi-vocalized reading into silent reading.The first books used parchment or vellum (calfskin) for the pages. The book covers were made of wood and covered with leather. Because dried parchment tends to assume the form it had before processing, the books were fitted with clasps or straps. During the later Middle Ages, when public libraries appeared, up to the 18th century, books were often chained to a bookshelf or a desk to prevent theft. These chained books are called libri catenati.
At first, books were copied mostly in monasteries, one at a time. With the rise of universities in the 13th century, the Manuscript culture of the time led to an increase in the demand for books, and a new system for copying books appeared. The books were divided into unbound leaves (pecia), which were lent out to different copyists, so the speed of book production was considerably increased. The system was maintained by secular stationers guilds, which produced both religious and non-religious material.Judaism has kept the art of the scribe alive up to the present. According to Jewish tradition, the Torah scroll placed in a synagogue must be written by hand on parchment and a printed book would not do, though the congregation may use printed prayer books and printed copies of the Scriptures are used for study outside the synagogue. A sofer "scribe" is a highly respected member of any observant Jewish community.


Middle East
People of various religious (Jews, Christians, Zoroastrians, Muslims) and ethnic backgrounds (Syriac, Coptic, Persian, Arab etc.) in the Middle East also produced and bound books in the Islamic Golden Age (mid 8th century to 1258), developing advanced techniques in Islamic calligraphy, miniatures and bookbinding. A number of cities in the medieval Islamic world had book production centers and book markets. Yaqubi (d. 897) says that in his time Baghdad had over a hundred booksellers. Book shops were often situated around the town's principal mosque as in Marrakesh, Morocco, that has a street named Kutubiyyin or book sellers in English and the famous Koutoubia Mosque is named so because of its location in this street.
The medieval Muslim world also used a method of reproducing reliable copies of a book in large quantities known as check reading, in contrast to the traditional method of a single scribe producing only a single copy of a single manuscript. In the check reading method, only "authors could authorize copies, and this was done in public sessions in which the copyist read the copy aloud in the presence of the author, who then certified it as accurate." With this check-reading system, "an author might produce a dozen or more copies from a single reading," and with two or more readings, "more than one hundred copies of a single book could easily be produced." By using as writing material the relatively cheap paper instead of parchment or papyrus the Muslims, in the words of Pedersen "accomplished a feat of crucial significance not only to the history of the Islamic book, but also to the whole world of books".


Wood block printing

In woodblock printing, a relief image of an entire page was carved into blocks of wood, inked, and used to print copies of that page. This method originated in China, in the Han dynasty (before 220 AD), as a method of printing on textiles and later paper, and was widely used throughout East Asia. The oldest dated book printed by this method is The Diamond Sutra (868 AD). The method (called woodcut when used in art) arrived in Europe in the early 14th century. Books (known as block-books), as well as playing-cards and religious pictures, began to be produced by this method. Creating an entire book was a painstaking process, requiring a hand-carved block for each page; and the wood blocks tended to crack, if stored for long. The monks or people who wrote them were paid highly.


Movable type and incunabula

The Chinese inventor Bi Sheng made movable type of earthenware c. 1045, but there are no known surviving examples of his printing. Around 1450, in what is commonly regarded as an independent invention, Johannes Gutenberg invented movable type in Europe, along with innovations in casting the type based on a matrix and hand mould. This invention gradually made books less expensive to produce, and more widely available.
Early printed books, single sheets and images which were created before 1501 in Europe are known as incunables or incunabula. "A man born in 1453, the year of the fall of Constantinople, could look back from his fiftieth year on a lifetime in which about eight million books had been printed, more perhaps than all the scribes of Europe had produced since Constantine founded his city in AD 330."


19th century to 21st centuries
Steam-powered printing presses became popular in the early 19th century. These machines could print 1,100 sheets per hour, but workers could only set 2,000 letters per hour. Monotype and linotype typesetting machines were introduced in the late 19th century. They could set more than 6,000 letters per hour and an entire line of type at once. There have been numerous improvements in the printing press. As well, the conditions for freedom of the press have been improved through the gradual relaxation of restrictive censorship laws. See also intellectual property, public domain, copyright. In mid-20th century, European book production had risen to over 200,000 titles per year.
Throughout the 20th century, libraries have faced an ever-increasing rate of publishing, sometimes called an information explosion. The advent of electronic publishing and the internet means that much new information is not printed in paper books, but is made available online through a digital library, on CD-ROM, in the form of e-books or other online media. An on-line book is an e-book that is available online through the internet. Though many books are produced digitally, most digital versions are not available to the public, and there is no decline in the rate of paper publishing. There is an effort, however, to convert books that are in the public domain into a digital medium for unlimited redistribution and infinite availability. This effort is spearheaded by Project Gutenberg combined with Distributed Proofreaders. There have also been new developments in the process of publishing books. Technologies such as POD or "print on demand", which make it possible to print as few as one book at a time, have made self-publishing (and vanity publishing) much easier and more affordable. On-demand publishing has allowed publishers, by avoiding the high costs of warehousing, to keep low-selling books in print rather than declaring them out of print.


Indian manuscripts

Goddess Saraswati image dated 132 AD excavated from Kankali tila depicts her holding a manuscript in her left hand represented as a bound and tied palm leaf or birch bark manuscript. In India a bounded manuscript made of birch bark or palm leaf existed side by side since antiquity.  The text in palm leaf manuscripts was inscribed with a knife pen on rectangular cut and cured palm leaf sheets; colourings were then applied to the surface and wiped off, leaving the ink in the incised grooves. Each sheet typically had a hole through which a string could pass, and with these the sheets were tied together with a string to bind like a book.


Mesoamerican Codex
The codices of pre-Columbian Mesoamerica (Mexico and Central America) had the same form as the European codex, but were instead made with long folded strips of either fig bark (amatl) or plant fibers, often with a layer of whitewash applied before writing. New World codices were written as late as the 16th century (see Maya codices and Aztec codices). Those written before the Spanish conquests seem all to have been single long sheets folded concertina-style, sometimes written on both sides of the local amatl paper.


Modern manufacturing

The methods used for the printing and binding of books continued fundamentally unchanged from the 15th century into the early 20th century. While there was more mechanization, a book printer in 1900 had much in common with Gutenberg. Gutenberg's invention was the use of movable metal types, assembled into words, lines, and pages and then printed by letterpress to create multiple copies. Modern paper books are printed on papers designed specifically for printed books. Traditionally, book papers are off-white or low-white papers (easier to read), are opaque to minimise the show-through of text from one side of the page to the other and are (usually) made to tighter caliper or thickness specifications, particularly for case-bound books. Different paper qualities are used depending on the type of book: Machine finished coated papers, woodfree uncoated papers, coated fine papers and special fine papers are common paper grades.
Today, the majority of books are printed by offset lithography. When a book is printed, the pages are laid out on the plate so that after the printed sheet is folded the pages will be in the correct sequence. Books tend to be manufactured nowadays in a few standard sizes. The sizes of books are usually specified as "trim size": the size of the page after the sheet has been folded and trimmed. The standard sizes result from sheet sizes (therefore machine sizes) which became popular 200 or 300 years ago, and have come to dominate the industry. British conventions in this regard prevail throughout the English-speaking world, except for the USA. The European book manufacturing industry works to a completely different set of standards.


Processes


Layout

Modern bound books are organized according to a particular format called the book's layout. Although there is great variation in layout, modern books tend to adhere to as set of rules with regard to what the parts of the layout are and what their content usually includes. A basic layout will include a front cover, a back cover and the book's content which is called its body copy or content pages. The front cover often bears the book's title (and subtitle, if any) and the name of its author or editor(s). The inside front cover page is usually left blank in both hardcover and paperback books. The next section, if present, is the book's front matter, which includes all textual material after the front cover but not part of the book's content such as a foreword, a dedication, a table of contents and publisher data such as the book's edition or printing number and place of publication. Between the body copy and the back cover goes the end matter which would include any indices, sets of tables, diagrams, glossaries or lists of cited works (though an edited book with several authors usually places cited works at the end of each authored chapter). The inside back cover page, like that inside the front cover, is usually blank. The back cover is the usual place for the book's ISBN and maybe a photograph of the author(s)/ editor(s), perhaps with a short introduction to them.  Also here often appear plot summaries, barcodes and excerpted reviews of the book.


Printing

Some books, particularly those with shorter runs (i.e. fewer copies) will be printed on sheet-fed offset presses, but most books are now printed on web presses, which are fed by a continuous roll of paper, and can consequently print more copies in a shorter time. As the production line circulates, a complete "book" is collected together in one stack, next to another, and another web press carries out the folding itself, delivering bundles of signatures (sections) ready to go into the gathering line. Note that the pages of a book are printed two at a time, not as one complete book. Excess numbers are printed to make up for any spoilage due to make-readies or test pages to assure final print quality.
A make-ready is the preparatory work carried out by the pressmen to get the printing press up to the required quality of impression. Included in make-ready is the time taken to mount the plate onto the machine, clean up any mess from the previous job, and get the press up to speed. As soon as the pressman decides that the printing is correct, all the make-ready sheets will be discarded, and the press will start making books. Similar make readies take place in the folding and binding areas, each involving spoilage of paper.


Binding
After the signatures are folded and gathered, they move into the bindery. In the middle of last century there were still many trade binders  stand-alone binding companies which did no printing, specializing in binding alone. At that time, because of the dominance of letterpress printing, typesetting and printing took place in one location, and binding in a different factory. When type was all metal, a typical book's worth of type would be bulky, fragile and heavy. The less it was moved in this condition the better: so printing would be carried out in the same location as the typesetting. Printed sheets on the other hand could easily be moved. Now, because of increasing computerization of preparing a book for the printer, the typesetting part of the job has flowed upstream, where it is done either by separately contracting companies working for the publisher, by the publishers themselves, or even by the authors. Mergers in the book manufacturing industry mean that it is now unusual to find a bindery which is not also involved in book printing (and vice versa).
If the book is a hardback its path through the bindery will involve more points of activity than if it is a paperback. Unsewn binding, is now increasingly common. The signatures of a book can also be held together by "Smyth sewing" using needles, "McCain sewing", using drilled holes often used in schoolbook binding, or "notch binding", where gashes about an inch long are made at intervals through the fold in the spine of each signature. The rest of the binding process is similar in all instances. Sewn and notch bound books can be bound as either hardbacks or paperbacks.


Finishing

"Making cases" happens off-line and prior to the book's arrival at the binding line. In the most basic case-making, two pieces of cardboard are placed onto a glued piece of cloth with a space between them into which is glued a thinner board cut to the width of the spine of the book. The overlapping edges of the cloth (about 5/8" all round) are folded over the boards, and pressed down to adhere. After case-making the stack of cases will go to the foil stamping area for adding decorations and type.


Digital printing
Recent developments in book manufacturing include the development of digital printing. Book pages are printed, in much the same way as an office copier works, using toner rather than ink. Each book is printed in one pass, not as separate signatures. Digital printing has permitted the manufacture of much smaller quantities than offset, in part because of the absence of make readies and of spoilage. One might think of a web press as printing quantities over 2000, quantities from 250 to 2000 being printed on sheet-fed presses, and digital presses doing quantities below 250. These numbers are of course only approximate and will vary from supplier to supplier, and from book to book depending on its characteristics. Digital printing has opened up the possibility of print-on-demand, where no books are printed until after an order is received from a customer.


E-book

In the 2000s, due to the rise in availability of affordable handheld computing devices, the opportunity to share texts through electronic means became an appealing option for media publishers. Thus, the "e-book" was made. The term e-book is a contraction of "electronic book"; it refers to a book-length publication in digital form. An e-book is usually made available through the internet, but also on CD-ROM and other forms. E-Books may be read either via a computing device with an LED display such as a traditional computer, a smartphone or a tablet computer; or by means of a portable e-ink display device known as an e-book reader, such as the Sony Reader, Barnes & Noble Nook, Kobo eReader, or the Amazon Kindle. E-book readers attempt to mimic the experience of reading a print book by using this technology, since the displays on e-book readers are much less reflective.


Design

Book design is the art of incorporating the content, style, format, design, and sequence of the various components of a book into a coherent whole. In the words of Jan Tschichold, book design "though largely forgotten today, methods and rules upon which it is impossible to improve have been developed over centuries. To produce perfect books these rules have to be brought back to life and applied." Richard Hendel describes book design as "an arcane subject" and refers to the need for a context to understand what that means. Many different creators can contribute to book design, including graphic designers, artists and editors.


Sizes

The size of a modern book is based on the printing area of a common flatbed press. The pages of type were arranged and clamped in a frame, so that when printed on a sheet of paper the full size of the press, the pages would be right side up and in order when the sheet was folded, and the folded edges trimmed.
The most common book sizes are:

Quarto (4to): the sheet of paper is folded twice, forming four leaves (eight pages) approximately 1113 inches (c. 30 cm) tall
Octavo (8vo): the most common size for current hardcover books. The sheet is folded three times into eight leaves (16 pages) up to 9 34 inches (c. 23 cm) tall.
DuoDecimo (12mo): a size between 8vo and 16mo, up to 7 34 inches (c. 18 cm) tall
Sextodecimo (16mo): the sheet is folded four times, forming 16 leaves (32 pages) up to 6 34 inches (c. 15 cm) tallSizes smaller than 16mo are:

24mo: up to 5 34 inches (c. 13 cm) tall.
32mo: up to 5 inches (c. 12 cm) tall.
48mo: up to 4 inches (c. 10 cm) tall.
64mo: up to 3 inches (c. 8 cm) tall.Small books can be called booklets.
Sizes larger than quarto are:

Folio: up to 15 inches (c. 38 cm) tall.
Elephant Folio: up to 23 inches (c. 58 cm) tall.
Atlas Folio: up to 25 inches (c. 63 cm) tall.
Double Elephant Folio: up to 50 inches (c. 127 cm) tall.The largest extant medieval manuscript in the world is Codex Gigas 92  50  22 cm. The world's largest book is made of stone and is in Kuthodaw Pagoda (Burma).


Types


By content

A common separation by content are fiction and non-fiction books. This simple separation can be found in most collections, libraries, and bookstores.  There are other types such as books of  sheet music.


Fiction
Many of the books published today are "fiction", meaning that they contain invented material, and are creative literature.  Other literary forms such as poetry are included in the broad category.  Most fiction is additionally categorized by literary form and genre.  
The novel is the most common form of fiction book. Novels are stories that typically feature a plot, setting, themes and characters. Stories and narrative are not restricted to any topic; a novel can be whimsical, serious or controversial. The novel has had a tremendous impact on entertainment and publishing markets. A novella is a term sometimes used for fiction prose typically between 17,500 and 40,000 words, and a novelette between 7,500 and 17,500. A short story may be any length up to 10,000 words, but these word lengths vary.
Comic books or graphic novels are books in which the story is illustrated. The characters and narrators use speech or thought bubbles to express verbal language.


Non-fiction

Non-fiction books are in principle based on fact, on subjects such as history, politics, social and cultural issues, as well as autobiographies and memoirs.  Nearly all academic literature is non-fiction.  A reference book is a general type of non-fiction book which provides information as opposed to telling a story, essay, commentary, or otherwise supporting a point of view. 
An almanac is a very general reference book, usually one-volume, with lists of data and information on many topics. An encyclopedia is a book or set of books designed to have more in-depth articles on many topics. A book listing words, their etymology, meanings, and other information is called a dictionary. A book which is a collection of maps is an atlas. A more specific reference book with tables or lists of data and information about a certain topic, often intended for professional use, is often called a handbook. Books which try to list references and abstracts in a certain broad area may be called an index, such as Engineering Index, or abstracts such as chemical abstracts and biological abstracts.

Books with technical information on how to do something or how to use some equipment are called instruction manuals. Other popular how-to books include cookbooks and home improvement books.
Students typically store and carry textbooks and schoolbooks for study purposes.


Non-published books
Many types of book are private, often filled in by the owner, for a variety of personal records.  Elementary school pupils often use workbooks, which are published with spaces or blanks to be filled by them for study or homework. In US higher education, it is common for a student to take an exam using a blue book.

There is a large set of books that are made only to write private ideas, notes, and accounts. These books are rarely published and are typically destroyed or remain private. Notebooks are blank papers to be written in by the user. Students and writers commonly use them for taking notes. Scientists and other researchers use lab notebooks to record their notes. They often feature spiral coil bindings at the edge so that pages may easily be torn out.

Address books, phone books, and calendar/appointment books are commonly used on a daily basis for recording appointments, meetings and personal contact information.  Books for recording periodic entries by the user, such as daily information about a journey, are called logbooks or simply logs. A similar book for writing the owner's daily private personal events, information, and ideas is called a diary or personal journal.  Businesses use accounting books such as journals and ledgers to record financial data in a practice called bookkeeping (now usually held on computers rather than in hand-written form).


Other types
There are several other types of books which are not commonly found under this system. Albums are books for holding a group of items belonging to a particular theme, such as a set of photographs, card collections, and memorabilia. One common example is stamp albums, which are used by many hobbyists to protect and organize their collections of postage stamps. Such albums are often made using removable plastic pages held inside in a ringed binder or other similar holder. Picture books are books for children with pictures on every page and less text (or even no text).
Hymnals are books with collections of musical hymns that can typically be found in churches. Prayerbooks or missals are books that contain written prayers and are commonly carried by monks, nuns, and other devoted followers or clergy. Lap books are a learning tool created by students.


Decodable readers and leveled books
A leveled book collection is a set of books organized in levels of difficulty from the easy books appropriate for an emergent reader to longer more complex books adequate for advanced readers.  Decodable readers or books are a specialized type of leveled books that use decodable text only including controlled lists of words, sentences and stories consistent with the letters and phonics that have been taught to the emergent reader.  New sounds and letters are added to higher level decodable books, as the level of instruction progresses, allowing for higher levels of accuracy, comprehension and fluency.


By physical format

Hardcover books have a stiff binding. Paperback books have cheaper, flexible covers which tend to be less durable. An alternative to paperback is the glossy cover, otherwise known as a dust cover, found on magazines, and comic books. Spiral-bound books are bound by spirals made of metal or plastic. Examples of spiral-bound books include teachers' manuals and puzzle books (crosswords, sudoku).
Publishing is a process for producing pre-printed books, magazines, and newspapers for the reader/user to buy.
Publishers may produce low-cost, pre-publication copies known as galleys or 'bound proofs' for promotional purposes, such as generating reviews in advance of publication. Galleys are usually made as cheaply as possible, since they are not intended for sale.


Libraries

Private or personal libraries made up of non-fiction and fiction books, (as opposed to the state or institutional records kept in archives) first appeared in classical Greece. In the ancient world, the maintaining of a library was usually (but not exclusively) the privilege of a wealthy individual. These libraries could have been either private or public, i.e. for people who were interested in using them. The difference from a modern public library lies in the fact that they were usually not funded from public sources. It is estimated that in the city of Rome at the end of the 3rd century there were around 30 public libraries. Public libraries also existed in other cities of the ancient Mediterranean region (for example, Library of Alexandria). Later, in the Middle Ages, monasteries and universities had also libraries that could be accessible to general public. Typically not the whole collection was available to public, the books could not be borrowed and often were chained to reading stands to prevent theft.
The beginning of modern public library begins around 15th century when individuals started to donate books to towns. The growth of a public library system in the United States started in the late 19th century and was much helped by donations from Andrew Carnegie. This reflected classes in a society: The poor or the middle class had to access most books through a public library or by other means while the rich could afford to have a private library built in their homes. In the United States the Boston Public Library 1852 Report of the Trustees established the justification for the public library as a tax-supported institution intended to extend educational opportunity and provide for general culture.The advent of paperback books in the 20th century led to an explosion of popular publishing. Paperback books made owning books affordable for many people. Paperback books often included works from genres that had previously been published mostly in pulp magazines. As a result of the low cost of such books and the spread of bookstores filled with them (in addition to the creation of a smaller market of extremely cheap used paperbacks) owning a private library ceased to be a status symbol for the rich.
In library and booksellers' catalogues, it is common to include an abbreviation such as "Crown 8vo" to indicate the paper size from which the book is made.
When rows of books are lined on a book holder, bookends are sometimes needed to keep them from slanting.


Identification and classification

During the 20th century, librarians were concerned about keeping track of the many books being added yearly to the Gutenberg Galaxy. Through a global society called the International Federation of Library Associations and Institutions (IFLA), they devised a series of tools including the International Standard Bibliographic Description (ISBD). Each book is specified by an International Standard Book Number, or ISBN, which is unique to every edition of every book produced by participating publishers, worldwide. It is managed by the ISBN Society. An ISBN has four parts: the first part is the country code, the second the publisher code, and the third the title code. The last part is a check digit, and can take values from 09 and X (10). The EAN Barcodes numbers for books are derived from the ISBN by prefixing 978, for Bookland, and calculating a new check digit.
Commercial publishers in industrialized countries generally assign ISBNs to their books, so buyers may presume that the ISBN is part of a total international system, with no exceptions. However, many government publishers, in industrial as well as developing countries, do not participate fully in the ISBN system, and publish books which do not have ISBNs. A large or public collection requires a catalogue. Codes called "call numbers" relate the books to the catalogue, and determine their locations on the shelves. Call numbers are based on a Library classification system. The call number is placed on the spine of the book, normally a short distance before the bottom, and inside. Institutional or national standards, such as ANSI/NISO Z39.41 - 1997, establish the correct way to place information (such as the title, or the name of the author) on book spines, and on "shelvable" book-like objects, such as containers for DVDs, video tapes and software.

One of the earliest and most widely known systems of cataloguing books is the Dewey Decimal System. Another widely known system is the Library of Congress Classification system. Both systems are biased towards subjects which were well represented in US libraries when they were developed, and hence have problems handling new subjects, such as computing, or subjects relating to other cultures. Information about books and authors can be stored in databases like online general-interest book databases. Metadata, which means "data about data" is information about a book. Metadata about a book may include its title, ISBN or other classification number (see above), the names of contributors (author, editor, illustrator) and publisher, its date and size, the language of the text, its subject matter, etc.


Classification systems
Bliss bibliographic classification (BC)
Chinese Library Classification (CLC)
Colon Classification
Dewey Decimal Classification (DDC)
Harvard-Yenching Classification
Library of Congress Classification (LCC)
New Classification Scheme for Chinese Libraries
Universal Decimal Classification (UDC)


Uses
Aside from the primary purpose of reading them, books are also used for other ends:

A book can be an artistic artifact, a piece of art; this is sometimes known as an artists' book.
A book may be evaluated by a reader or professional writer to create a book review.
A book may be read by a group of people to use as a spark for social or academic discussion, as in a book club.
A book may be studied by students as the subject of a writing and analysis exercise in the form of a book report.
Books are sometimes used for their exterior appearance to decorate a room, such as a study.


Book marketing
Once the book is published, it is put on the market by the distributors and the bookstores. Meanwhile, his promotion comes from various media reports. Book marketing is governed by the law in many states.


Other forms of secondary spread
In recent years, the book had a second life in the form of reading aloud. This is called public readings of published works, with the assistance of professional readers (often known actors) and in close collaboration with writers, publishers, booksellers, librarians, leaders of the literary world and artists.
Many individual or collective practices exist to increase the number of readers of a book. Among them:

abandonment of books in public places, coupled or not with the use of the Internet, known as the bookcrossing;
provision of free books in third places like bars or cafes;
itinerant or temporary libraries;
free public libraries in the area.


Evolution of the book industry
This form of the book chain has hardly changed since the eighteenth century, and has not always been this way. Thus, the author has asserted gradually with time, and the copyright dates only from the nineteenth century. For many centuries, especially before the invention of printing, each freely copied out books that passed through his hands, adding if necessary his own comments. Similarly, bookseller and publisher jobs have emerged with the invention of printing, which made the book an industrial product, requiring structures of production and marketing.
The invention of the Internet, e-readers, tablets, and projects like Wikipedia and Gutenberg, are likely to strongly change the book industry in the years to come.


Paper and conservation

Paper was first made in China as early as 200 BC, and reached Europe through Muslim territories. At first made of rags, the industrial revolution changed paper-making practices, allowing for paper to be made out of wood pulp. Papermaking in Europe began in the 11th century, although vellum was also common there as page material up until the beginning of the 16th century, vellum being the more expensive and durable option. Printers or publishers would often issue the same publication on both materials, to cater to more than one market.
Paper made from wood pulp became popular in the early 20th century, because it was cheaper than linen or abaca cloth-based papers. Pulp-based paper made books less expensive to the general public. This paved the way for huge leaps in the rate of literacy in industrialised nations, and enabled the spread of information during the Second Industrial Revolution.
Pulp paper, however, contains acid which eventually destroys the paper from within. Earlier techniques for making paper used limestone rollers, which neutralized the acid in the pulp. Books printed between 1850 and 1950 are primarily at risk; more recent books are often printed on acid-free or alkaline paper. Libraries today have to consider mass deacidification of their older collections in order to prevent decay.
Stability of the climate is critical to the long-term preservation of paper and book material. Good air circulation is important to keep fluctuation in climate stable. The HVAC system should be up to date and functioning efficiently. Light is detrimental to collections. Therefore, care should be given to the collections by implementing light control. General housekeeping issues can be addressed, including pest control. In addition to these helpful solutions, a library must also make an effort to be prepared if a disaster occurs, one that they cannot control. Time and effort should be given to create a concise and effective disaster plan to counteract any damage incurred through "acts of God", therefore an emergency management plan should be in place.


Francis Scott Key Fitzgerald (September 24, 1896  December 21, 1940) was an American novelist, essayist, screenwriter, and Short story writer. He was best known for his novels depicting the flamboyance and excess of the Jazz Agea term which he popularized. During his lifetime, he published four novels, four collections of short stories, and 164 short stories.  Although he temporarily achieved popular success and fortune in the 1920s, Fitzgerald only received wide critical and popular acclaim after his death. He is widely regarded as one of the greatest American writers of the 20th century.
Fitzgerald was born into an upper-middle-class family in St. Paul, Minnesota, but was primarily raised in New York. He attended Princeton University, but due to a failed relationship and a preoccupation with writing, he dropped out in 1917 to join the Army. While stationed in Alabama, he fell in love with rich socialite Zelda Sayre. Although she initially rejected him due to his financial situation, Zelda agreed to marry Fitzgerald after he had published the commercially successful This Side of Paradise (1920).
In the 1920s, Fitzgerald  frequented Europe, where he was influenced by the modernist writers and artists of the "Lost Generation" expatriate community, particularly Ernest Hemingway. His second novel, The Beautiful and Damned (1922), propelled him into the New York City elite. To maintain his lifestyle during this time, he also wrote several stories for magazines. His third novel, The Great Gatsby (1925), was inspired by his rise to fame and relationship with Zelda. Although it received mixed reviews, The Great Gatsby is now widely praised, with some even labeling it the "Great American Novel". While Zelda was placed at a mental institute for her schizophrenia, Fitzgerald completed his final novel, Tender Is the Night (1934).
Faced with financial difficulties due to the declining popularity of his works, Fitzgerald turned to Hollywood, writing and revising screenplays. After a long struggle with alcoholism, he died in 1940, at the age of 44. A fifth, unfinished novel, The Last Tycoon (1941), was completed by Edmund Wilson and published after Fitzgerald's death.


Life


Early life

Born on September 24, 1896, in Saint Paul, Minnesota, to an upper-middle-class family, Fitzgerald was named after his second cousin thrice removed, Francis Scott Key, but was always known as Scott Fitzgerald. Fitzgerald was also named after his deceased sister, Louise Scott Fitzgerald, one of two sisters who died shortly before his birth. "Well, three months before I was born," he wrote as an adult, "my mother lost her other two children ... I think I started then to be a writer." His father, Edward Fitzgerald, was of Irish and English ancestry, and had moved to St. Paul from Maryland after the American Civil War. His mother was Mary "Molly" McQuillan Fitzgerald, the daughter of an Irish immigrant who had made his fortune in the wholesale grocery business. Edward's first cousin once removed, Mary Surratt, was hanged in 1865 for conspiring to assassinate Abraham Lincoln.

Fitzgerald spent the first decade of his childhood primarily in Buffalo, New York, where his father worked for Procter & Gamble, with a short interlude in Syracuse, (between January 1901 and September 1903). Edward Fitzgerald had earlier worked as a wicker furniture salesman; he joined Procter & Gamble when the business failed. His parents, both Catholic, sent him to two Catholic schools on the West Side of Buffalo, first Holy Angels Convent (19031904, now disused) and then Nardin Academy (19051908). Fitzgerald's formative years in Buffalo revealed him to be a boy of unusual intelligence with a keen early interest in literature. His mother's inheritance and donations from an aunt allowed the family to live a comfortable lifestyle. In a rather unconventional style of parenting, Fitzgerald attended Holy Angels with the arrangement that he go for only half a dayand was allowed to choose which half.In 1908, his father was fired from Procter & Gamble, and the family returned to Minnesota, where Fitzgerald attended St. Paul Academy in St. Paul from 1908 to 1911. At the age of 13, Fitzgerald had his first work published, a detective story in the school newspaper. In 1911, Fitzgerald's parents sent him to the Newman School, a Catholic prep school in Hackensack, New Jersey. Fitzgerald played on the 1912 Newman football team. At Newman, he was taught by Father Sigourney Fay, who recognized his literary potential and encouraged him to become a writer. After graduating from Newman in 1913, Fitzgerald enrolled at Princeton University, where he tried out for the football team and was cut the first day of practice. At Princeton, he became friends with future critics and writers, including Edmund Wilson and John Peale Bishop. Fitzgerald wrote for the Princeton Triangle Club, the Nassau Lit, and the Princeton Tiger. He also was involved in the American Whig-Cliosophic Society, which ran the Nassau Lit. His absorption in the Trianglea kind of musical-comedy societyled to his submission of a novel to Charles Scribner's Sons where the editor praised the writing but ultimately rejected the book. Four of the University's eating clubs sent him bids at midyear, and he chose the University Cottage Club (where Fitzgerald's desk and writing materials are still displayed in its library).

While attending Princeton, Fitzgerald met Chicago socialite and debutante Ginevra King. King and Fitzgerald had a romantic relationship from 1915 to 1917. Immediately infatuated with her, according to Mizner, Fitzgerald "remained devoted to Ginevra as long as she would allow him to", and wrote to her "daily the incoherent, expressive letters all young lovers write". She would become his inspiration for the character of Isabelle Borg, Amory Blaine's first love in This Side of Paradise, for Daisy in The Great Gatsby, and several other characters in his novels and short stories. Her father reportedly warned Fitzgerald that "Poor boys shouldn't think of marrying rich girls." After their relationship ended in 1917, Fitzgerald requested that Ginevra destroy the letters that he had written to her. However, he never destroyed the letters that King had sent him. After his death in 1940, his daughter "Scottie" sent the letters back to King where she kept them until her death. She never shared the letters with anyone.At Princeton, Fitzgerald's writing pursuits came at the expense of his studies, causing him to be placed on academic probation. In 1917, Fitzgerald pivoted, dropping out of Princeton to join the Army. During that winter, he was stationed at Fort Leavenworth, under the command of future United States President and General of the Army Dwight Eisenhower, whom he intensely disliked. Worried that he could die in the War without ever publishing anything, Fitzgerald hastily wrote The Romantic Egotist in the weeks before reporting for dutyand, although Scribners rejected it, the reviewer praised Fitzgerald's writing and encouraged him to resubmit the novel after further revisions. Fitzgerald would later regret not serving in combat, as detailed in his short story "I Didnt Get Over" (1936).


Zelda Fitzgerald

In 1918, Fitzgerald was commissioned a second lieutenant and dispatched to Camp Sheridan near Montgomery, Alabama, serving with the 45th and 67th Infantry Regiments. While at a local country club, Fitzgerald met and fell in love with Zelda Sayre, the youngest daughter of Alabama Supreme Court Justice Anthony D. Sayre and the "golden girl", in Fitzgerald's terms, of Montgomery society. They began a courtship, but were briefly interrupted in October when he was summoned north. He expected to be sent to France, but was instead assigned to Camp Mills, Long Island. While stationed there, the Armistice with Germany was signed. He then returned to the base near Montgomery and began meeting Zelda again. Together again, they embarked on what he would later call "sexual recklessness," and by December, they were inseparable. In what became a lifetime practice, Fitzgerald relied on Zelda for literary inspiration, going so far as to plagiarize her diary while revising his first novel.Upon his discharge on February 14, 1919, he relocated to New York City, where he unsuccessfully begged each of the city editors of the seven newspapers for a job. He then turned to a career in advertising, hopeful that it would be lucrative enough to persuade Zelda to marry him. Fitzgerald wrote to Zelda frequently, and by March 1920, he had sent Zelda his mother's ring, and the two had become engaged. Many of Zelda's friends and members of her family were wary of the relationship, as they did not approve of his excessive drinking, and Zelda's Episcopalian family did not like the fact that he was a Catholic. At the time, Fitzgerald was working for the Barron Collier advertising agency, living in a single room at 200 Claremont Avenue in the Morningside Heights neighborhood on Manhattan's west side. Although he received a raise for creating a slogan for a laundry in Iowa: "We keep you clean in Muscatine", Fitzgerald was still relatively poor. Still aspiring to a career in literature, he wrote several short stories and satires in his spare time. Rejected over 120 times, he was only able to sell a single story, for which he was paid $30.

With his dreams of a lucrative career in New York dashed, he was unable to convince Zelda that he would be able to support her, leading her to break off the engagement. Fitzgerald returned to his parents' house at 599 Summit Avenue, on Cathedral Hill, in St. Paul, to revise The Romantic Egotist, recast as This Side of Paradise, a semi-autobiographical account of Fitzgerald's undergraduate years at Princeton. Fitzgerald was so short of money that he took up a job repairing car roofs. His revised novel was accepted by Scribner's in the fall of 1919 and was published on March 26, 1920 and became an instant success, selling 41,075 copies in the first year. It launched Fitzgerald's career as a writer and provided a steady income suitable to Zelda's needs.  They resumed their engagement and were married on April 3, 1920 at St. Patrick's Cathedral, New York.On Valentine's Day in 1921, while Fitzgerald was working to finish his second novel, The Beautiful and Damned, Zelda discovered she was pregnant. They decided to go to his home in St. Paul, Minnesota to have the baby. On October 26, 1921, she gave birth to their daughter and only child Frances Scott "Scottie" Fitzgerald. As she emerged from the anesthesia, he recorded Zelda saying, "Oh, God, goofo I'm drunk. Mark Twain. Isn't she smartshe has the hiccups. I hope it's beautiful and a foola beautiful little fool." Fitzgerald utilized some of her rambling in his later writing; the words appear almost verbatim in Daisy Buchanan's dialogue from The Great Gatsby.


New York and the Jazz Age

After the birth of Scottie, Fitzgerald returned to writing The Beautiful and Damned, but in early 1922, Zelda became pregnant for a second time. Although some writers have claimed that his diaries include an entry referring to "Zelda and her abortionist", there is, in fact, no such entry. Zelda's thoughts on the second pregnancy are unknown, but in the first draft of The Beautiful and Damned, he wrote a scene in which the main female character Gloria believes she is pregnant and Anthony suggests she "talk to some woman and find out what's best to be done. Most of them fix it some way." Anthony's suggestion was removed from the final version, a change which shifted focus from the abortion choice to Gloria's concern that a baby would ruin her figure. Chapters of the book were serialized in Metropolitan Magazine in late 1921, and in March 1922, the book was published. Scribner's prepared an initial print run of 20,000 copies, and mounted an advertising campaign. It sold well enough to warrant additional print runs reaching 50,000 copies. That year, Fitzgerald also released Tales of the Jazz Age, which was composed of 11 short stories, all but two written before 1920. This collection's title would lend itself to the eponymous time period.

In New York City, the Fitzgeralds quickly became celebrities, as much for their wild behavior as for the success of This Side of Paradise. They were ordered to leave both the Biltmore Hotel and the Commodore Hotel for their drunkenness. Zelda once jumped into the fountain at Union Square. When Dorothy Parker first met them, they were sitting atop a taxi. Parker said, "They did both look as though they had just stepped out of the sun; their youth was striking. Everyone wanted to meet him." Their social life was fueled with alcohol. Publicly, this meant little more than napping when they arrived at parties, but privately it increasingly led to bitter fights. The couple would later be seen as the epitome of the period, with Ring Lardner Jr. labelling them "the prince and princess of their generation."Following Fitzgerald's adaptation of his short story "The Vegetable" into a play, he and Zelda moved to Great Neck, Long Island to be near Broadway. Although he hoped that this was the beginning of a lucrative career in theater, the play's November 1923 premiere was a critical and commercial disaster. In a letter, Zelda claimed that the audience were so obviously bored and that some even walked out during the second act. Fitzgerald himself wrote that "I wanted to stop the show and say it was all a mistake but the actors struggled heroically on." During the second intermission, Fitzgerald and Lardner asked the lead actor, Ernest Truex, "Are you going to stay and do the last act?" The actor replied that he was, at which the pair of writers declared that they were leaving for the nearest bar. Fitzgerald turned to short stories to pay the debt he had incurred in developing his play. He despised his short stories, saying they were "all trash and it nearly broke my heart."


Europe and the Lost Generation

In spring 1924, Fitzgerald and his family moved to France, where he would begin writing his third novel, which would eventually become The Great Gatsby. Fitzgerald had been planning the novel since 1923, when he told his publisher Maxwell Perkins of his plans "to write something new  something extraordinary and beautiful and simple and intricately patterned." Initially titled Trimalchio, an allusion to the Latin work Satyricon, the rough manuscript followed the rise of a freedman to wealth and power. During the Fitzgeralds' sojourn in Rome in late 1924, Fitzgerald would rewrite the text several times, replacing the freedman with arriviste Jay Gatsby. Fitzgerald declined an offer of $10,000 for the serial rights, fearing it would delay the book's publication, set for April 10, 1925. Upon its release, fellow writers Willa Cather, T. S. Eliot, and Edith Wharton praised Fitzgerald's latest work, but it was snubbed by most critics and audiences. The New York World ran a headline declaring "Fitzgerald's Latest A Dud". For the rest of his life, The Great Gatsby experienced tepid sales. For example, in 1929 Fitzgerald only received royalties of $5.10 from the American edition and just $0.34 from the English edition.  His final royalty check was for only $13.13, all of which was from Fitzgerald buying his own books. It would take many decades for the novel to gain its present acclaim and popularity.While Fitzgerald had been writing The Great Gatsby, Zelda had become infatuated with a young French aviator, Edouard S. Jozan. She spent afternoons swimming at the beach and evenings dancing at the casinos with Jozan. After six weeks, Zelda asked for a divorce. Fitzgerald at first demanded to confront Jozan, but instead dealt with Zelda's request by locking her in their house, until she abandoned her request for divorce. Jozan was not aware that she had asked for a divorce. He left the Riviera later that year, and the Fitzgeralds never saw him again. Later in life he told Zelda's biographer Milford that any infidelity was imaginary: "They both had a need of drama, they made it up and perhaps they were the victims of their own unsettled and a little unhealthy imagination."

After spending winter in Italy, the Fitzgeralds returned to France, where they would alternate between Paris and the French Riviera until 1926. Fitzgerald began writing his fourth novel, provisionally titled The Boy Who Killed His Mother, Our Type, and then The Worlds Fair. During this period, he became friends with many members of the American expatriate community in Paris, later known as the Lost Generation. Most notable among them was a relatively unknown Ernest Hemingway, whom Fitzgerald greatly admired. Fitzgerald's friendship with Hemingway was quite effusive, as many of Fitzgerald's relationships would prove to be.  Hemingway did not get on well with Zelda, however, and in addition to describing her as "insane" in his memoir A Moveable Feast, Hemingway claimed that Zelda "encouraged her husband to drink so as to distract Fitzgerald from his work on his novel", so he could work on the short stories he sold to magazines to help support their lifestyle. Like most professional authors at the time, Fitzgerald supplemented his income by writing short stories for such magazines as The Saturday Evening Post, Collier's Weekly, and Esquire, and sold his stories and novels to Hollywood studios. This "whoring", as Fitzgerald and Hemingway called these sales, was a sore point in the two authors' friendship. Fitzgerald claimed that he would first write his stories in an 'authentic' manner, then rewrite them to put in the "twists that made them into salable magazine stories". Upon reading The Great Gatsby, Hemingway was so impressed with the book that he vowed to put any differences with Fitzgerald aside and "be of any help [he] could" with a subsequent novel, fearing Fitzgerald's personal problems may hamper his writing career.In A Moveable Feast, Hemingway claimed that Zelda taunted Fitzgerald over the size of his penis. After examining it in a public restroom, Hemingway told Fitzgerald "You're perfectly fine," assuring him that it was larger than those of statues at the Louvre. One of the most serious rifts occurred when Zelda told him that their sex life had declined because he was "a fairy" and was likely having a homosexual affair with Hemingway. There is no evidence that either was homosexual, but Fitzgerald nonetheless decided to have sex with a prostitute to prove his heterosexuality. Zelda found condoms that he had purchased before any encounter occurred, and a bitter fight ensued, resulting in lingering jealousy. She later threw herself down a flight of marble stairs at a party because Fitzgerald, engrossed in talking to Isadora Duncan, was ignoring her. In September 1924, Zelda overdosed on sleeping pills. The couple never spoke of the incident and refused to discuss whether it was a suicide attempt. The episode propelled Fitzgerald to write in his notebook, "That September 1924, I knew something had happened that could never be repaired." This breakdown of their relationship worsened Fitzgerald's alcoholism.


Foray into Hollywood and Tender Is the Night

In 1926, Fitzgerald was invited by producer John W. Considine Jr. to temporarily relocate to Hollywood in order to write a flapper comedy for United Artists. He agreed, moving into a studio-owned bungalow in January 1927. He soon met and began an affair with the 17 year-old starlet Lois Moran. Jealous of the attention Fitzgerald gave Moran, Zelda burned her own clothing in a self destructive act. The starlet became a temporary muse for the author and he rewrote Rosemary Hoyt, one of the central characters in Tender is the Nightwho had been a male in earlier draftsto closely mirror her. The trip further exacerbated the Fitzgeralds' marital difficulties, and they left Hollywood after two months.They then rented "Ellerslie", a mansion near Wilmington, Delaware until 1929. Fitzgerald tried to continue working on his fourth novel, but by this point it had become clear that Zelda had an extreme mental illness as her behavior grew increasingly erratic. In 1930, she was diagnosed with schizophrenia. The couple travelled to Switzerland, where she was treated at a mental clinic. They returned to America in September 1931. In February 1932, she was hospitalized at the Phipps Clinic at Johns Hopkins University in Baltimore, Maryland.During this time, Fitzgerald rented the "La Paix" estate in the suburb of Towson, Maryland to work on his latest novel, the story of the rise and fall of Dick Diver, a promising young psychiatrist who becomes smitten with and marries Nicole Warren, one of his patients. The book went through many versions, the first of which was to be a story of matricide.  Some critics have seen the book as a thinly veiled autobiographical novel recounting Fitzgerald's problems with his wife, the corrosive effects of wealth and a decadent lifestyle, his own egoism and self-confidence, and his continuing alcoholism. Fitzgerald was extremely protective of his "material" (i.e., their life together).  
In 1932, Zelda wrote and sent to Scribner's her own fictional version of their lives in Europe, Save Me the Waltz (1932). Infuriated by what he saw as theft of his source material, Fitzgerald labeled Zelda "plagiaristic" and a "third-rate writer". He was able to make some changes prior to the novel's publication, and convinced her doctors to keep her from writing any more about their relationship. 
Fitzgerald's own novel was finally published in 1934 as Tender Is the Night. The novel received mixed opinions from critics, most were thrown off by its three-part structure, and many felt that Fitzgerald had not lived up to their expectations. Hemingway and others have argued that such overly harsh criticism stemmed from superficial readings of the material and from Depression-era America's reaction to Fitzgerald's status as a symbol of Jazz Age excess. The novel did not sell well upon publication, with only 12,000 sold in the first 3 months, but, like the earlier The Great Gatsby, the book's reputation has since risen significantly.


Decline

With the arrival of the Great Depression, many of Fitzgerald's works were seen as elitist and materialistic. In 1933, Matthew Josephson scolded Fitzgerald: "There are ever so many Americans, we recall, who can't be drinking champagne from morning to night, can't ever go to Princeton or Montpar-nasse or even Greenwich Village for their finishing process."However, Fitzgerald began to feel the effects of the Depression himself. By the mid 1930s, his popularity and fame had greatly decreased, and consequently, he had begun to suffer financially. Public demand had decreased so much for Fitzgerald's works that by 1936, his book royalties barely amounted to $80. The cost of his opulent lifestyle and Zelda's medical bills quickly caught up, placing Fitzgerald in constant financial trouble. He relied on loans from his literary agent, Harold Ober, and his publisher Perkins. When Ober decided not to continue advancing money to Fitzgerald, the author severed ties with his longtime friend and agent.Fitzgerald, an alcoholic since college, became notorious during the 1920s for his extraordinarily heavy drinking, which would undermine his health by the late 1930s. His alcoholism resulted in cardiomyopathy, coronary artery disease, angina, dyspnea, and syncopal spells. According to Zelda's biographer, Nancy Milford, Fitzgerald claimed that he had contracted tuberculosis, but Milford dismisses it as a pretext to cover his drinking problems; however, Fitzgerald scholar Matthew J. Bruccoli contends that Fitzgerald did in fact have recurring tuberculosis, and according to Milford, Fitzgerald biographer Arthur Mizener said that Fitzgerald suffered a mild attack of tuberculosis in 1919, and in 1929 he had "what proved to be a tubercular hemorrhage". In the 1930s, Fitzgerald had told Hemingway of his fear of dying from "congestion of the lungs." Others have suggested that the writer's hemorrhage was caused by bleeding from esophageal varices.Fitzgerald's alcoholism and financial difficulties, in addition to Zelda's mental illness, made for difficult years in Baltimore. He was hospitalized nine times at Johns Hopkins Hospital, and his friend H. L. Mencken noted in a 1934 letter that "The case of F. Scott Fitzgerald has become distressing. He is boozing in a wild manner and has become a nuisance." In 1935, Fitzgerald wrote Perkins, admitting that alcohol was disrupting his writing, limiting his "mental speed." From 1933 to 1937, Fitzgerald was hospitalized for alcoholism 8 times and arrested several times. Fitzgerald's deteriorating mental state and drinking habits were captured publicly in an article published by Michel Mok titled "The Other Side of Paradise, Scott Fitzgerald, 40, Engulfed in Despair", first published in the New York Post, September 25, 1936. The article is considered to have caused considerable damage to Fitzgerald's reputation and his mental state, allegedly pushing him to attempt suicide after reading it.By that year, Zelda had become extremely violent and emotionally distressed, and Fitzgerald had her placed in the Highland Hospital in Asheville, North Carolina. Nearly bankrupt, Fitzgerald spent most of 1936 and 1937 living in various hotels near Asheville. His attempts to write and sell more short stories faltered. He later referred to this period of decline in his life as "The Crack-Up" in the short story. Shortly after the release of this story, Hemingway referred to Fitzgerald as "poor Scott" in his short story "The Snows of Kilimanjaro". Zelda's institutionalization further deteriorated what was left of their marriage. The last time the two saw each other was on a 1939 trip to Cuba. During this trip, Fitzgerald was assaulted when he tried to stop a cockfight and returned to the United States so intoxicated and exhausted that he was hospitalized.


Return to Hollywood

Although he reportedly found movie work degrading, Fitzgerald entered into a lucrative exclusive deal with Metro-Goldwyn-Mayer in 1937 that necessitated his moving to Hollywood, where he earned his highest annual income up to that point: $29,757.87 (equivalent to $529,235 in 2019). During his two years in California, Fitzgerald rented a room at the Garden of Allah bungalow complex on Sunset Boulevard. In an effort to abstain from alcohol, Fitzgerald resorted to drinking large amounts of bottled Coca-Cola.Completely estranged from Zelda, he began an affair with gossip columnist Sheilah Graham. After a heart-attack in Schwab's Drug Store, he was ordered by his doctor to avoid strenuous exertion. He moved in with Graham, who lived in Hollywood on North Hayworth Avenue, one block east of Fitzgerald's apartment on North Laurel Avenue. Fitzgerald had two flights of stairs to climb to his apartment; Graham's was on the ground floor. At one point during their affair, Fitzgerald attempted to give her one of his books, but after visiting several bookstores, he realized that they had stopped carrying his books. On occasions that Fitzgerald failed his attempt at sobriety, he would tell others, "I'm F. Scott Fitzgerald. You've read my books. You've read "The Great Gatsby," haven't you? Remember?"
The projects Fitzgerald worked on included two weeks' unused dialog work on loanout to David Selznick for Gone with the Wind (1939) for which he received no credit, and, for MGM, revisions on Madame Curie (1943) which also went uncredited. His only screenplay credit is for Three Comrades (1938). He also spent time during this period working on his fifth and final novel, based on film executive Irving Thalberg. Fitzgerald often ignored scriptwriting rules, writing prose and description more fitting for a novel, annoying the studio. In 1939, MGM terminated the contract, and Fitzgerald became a freelance screenwriter. During his work on Winter Carnival (1939), Fitzgerald went on another alcoholic binge and was treated by New York psychiatrist Richard H. Hoffmann.Director Billy Wilder described Fitzgerald's foray into Hollywood as like that of "a great sculptor who is hired to do a plumbing job." Edmund Wilson and Aaron Latham later suggested that Hollywood sucked Fitzgerald's creativity like a vampire. His failure in Hollywood pushed him to return to drinking, imbibing nearly 40 beers a day in 1939. Beginning that year, Fitzgerald mocked himself as a Hollywood hack through the character of Pat Hobby in a sequence of 17 short stories, later collected as "The Pat Hobby Stories", which garnered many positive reviews. The Pat Hobby Stories were originally published in Esquire between January 1940 and July 1941, even after his death. In his final year of life, Fitzgerald wrote his daughter: "I wish now I'd never relaxed or looked back - but said at the end of 'The Great Gatsby': I've found my line - from now on this comes first. This is my immediate duty - without this I am nothing."


Death
On the night of December 20, 1940, Fitzgerald and Graham attended the premiere of This Thing Called Love starring Rosalind Russell and Melvyn Douglas. As the two were leaving the Pantages Theater, Fitzgerald experienced a dizzy spell and had trouble walking; upset, he said to Graham, "They think I am drunk, don't they?"The following day, as Fitzgerald ate a candy bar and made notes in his newly arrived Princeton Alumni Weekly, Graham saw him jump from his armchair, grab the mantelpiece, gasp, and fall to the floor. She ran to the manager of the building, Harry Culver. Upon entering the apartment to assist Fitzgerald, Culver stated, "I'm afraid he's dead." Fitzgerald had died of a heart attack, aged just 44.

Among the attendees at a visitation held at a funeral home was Dorothy Parker, who reportedly cried and murmured "the poor son-of-a-bitch", a line from Jay Gatsby's funeral in Fitzgerald's The Great Gatsby. His body was transported to Bethesda, Maryland, where his funeral was attended by only thirty people; among the attendees were his only child, Scottie Fitzgerald, and his editor, Maxwell Perkins.At the time of his death, the Roman Catholic Church denied the family's request that Fitzgerald, a non-practicing Catholic, be buried in the family plot in the Catholic Saint Mary's Cemetery in Rockville, Maryland. Fitzgerald was instead buried at Rockville Union Cemetery. When Zelda Fitzgerald died in 1948, in a fire at the Highland Mental Hospital, she was originally buried next to him at Rockville Union. In 1975, Scottie successfully petitioned to have the earlier decision revisited, and her parents' remains were moved to the family plot in Saint Mary's.


Legacy


Critical reevaluation

By the time of his death, Fitzgerald was essentially unknown to the general public. The few who were familiar saw Fitzgerald as an alcoholic, the embodiment of Jazz Age decadence. Well after his death, Scribners still had many unsold editions of The Great Gatsby from its first printing. As late as the 1940s, many of Fitzgerald's works were labelled period pieces, with critic Peter Quennell dismissing The Great Gatsby as having "the sadness and the remote jauntiness of a Gershwin tune."Fitzgerald died before he could complete his fifth novel. His manuscript, which included extensive notes for the unwritten part of the novel's story, was completed by his friend, the literary critic Edmund Wilson. When Wilson published his finished version, titled The Last Tycoon, in 1941, he included The Great Gatsby within the edition, sparking new interest and discussion. The novel gained further popularity during World War II, when it was selected to be part of the Armed Services Editions, books which were printed for American troops. Through an arrangement with the Red Cross, some novels were even sent to Japanese and German POW camps. By 1945, over 123,000 copies of The Great Gatsby had been distributed among American troops. In 1960, New York Times editorialist Arthur Mizener declared that it was "probably safe now to say that it is a classic of twentieth-century American fiction." Into the 21st century, millions of copies of The Great Gatsby and his other works have been sold, and The Great Gatsby is required reading in many high school and college classes.The popular resurgence of The Great Gatsby also led to greater admiration and appreciation for Fitzgerald himself. In the 1950s, Wilson, who had attended Princeton with Fitzgerald, noted that Fitzgerald had taken on "the aspect of a martyr, a sacrificial victim, a semi-divine personage." In 1960, William Troy labelled Fitzgerald "one of the few truly mythological creations in our culture." Adam Gopnik noted that, counter to Fitzgerald's famous claim that "there are no second acts in American lives," Fitzgerald has become "not a poignant footnote to an ill-named time but an enduring legend of the West." A mythos has evolved around Fitzgerald and his life. In a 2008 interview, Jay McInerney claimed that "people believe the myth of Fitzgerald isthat he was seduced by this world that he wrote about, and that he ultimately couldnt separate his life and his art." Fitzgerald's momentary success and early death result in many seeing him as a tragic figure.


Influence

Fitzgerald's work has inspired writers ever since he was first published. The publication of The Great Gatsby prompted T. S. Eliot to write, in a letter to Fitzgerald, "It seems to me to be the first step that American fiction has taken since Henry James." Don Birnam, the protagonist of Charles Jackson's The Lost Weekend, says to himself, referring to The Great Gatsby, "There's no such thing ... as a flawless novel. But if there is, this is it." In letters written in the 1940s, J. D. Salinger expressed admiration of Fitzgerald's work, and his biographer Ian Hamilton wrote that Salinger even saw himself for some time as "Fitzgerald's successor". Richard Yates, a writer often compared to Fitzgerald, called The Great Gatsby "the most nourishing novel [he] read ... a miracle of talent ... a triumph of technique". It was written in an editorial in The New York Times after his death that Fitzgerald "was better than he knew, for in fact and in the literary sense he invented a generation ... He might have interpreted them and even guided them, as in their middle years they saw a different and nobler freedom threatened with destruction."


Adaptations and portrayals

Fitzgerald's works have been adapted into films many times. One of the earliest Fitzgerald short stories was adapted into a 1921 silent film The Off-Shore Pirate. Tender Is the Night was the subject of the eponymous 1962 film, and made into a television miniseries in 1985. The Beautiful and Damned was filmed in 1922 and 2010. The Great Gatsby has been adapted into numerous films of the same name, spanning nearly 90 years: 1926, 1949, 1974, 2000, and 2013 adaptations. In 1976, The Last Tycoon was adapted into a film starring Robert de Niro. and in 2016 it was adapted as an Amazon Prime  TV miniseries. starring Matt Bomer. His short story, "The Curious Case of Benjamin Button," was the basis for a 2008 film.Beyond his own characters, Fitzgerald himself has been portrayed in dozens of books, plays, and films. Fitzgerald was the main inspiration for Budd Schulberg's novel The Disenchanted (1950), which followed a screenplay writer in Hollywood working with a drunk and flawed novelist. It was later adopted into a Broadway play starring Jason Robards. A musical about the lives of Fitzgerald and Zelda was composed by Frank Wildhorn titled Waiting for the Moon. Fitzgerald is of international appeal, as even the Japanese Takarazuka Revue has created a musical adaptation of Fitzgerald's life. The last years of Fitzgerald and his affair with Sheilah Graham, was the theme of the movie Beloved Infidel (1959) based on Graham's 1958 memoir by the same name. The film depicts Fitzgerald (played by Gregory Peck) during his final years and his relationship with Graham (played by Deborah Kerr). Another film, Last Call (2002) portrays the relationship between Fitzgerald (Jeremy Irons) and Frances Kroll Ring (Neve Campbell).  David Hoflin and Christina Ricci portray the Fitzgerald's in Amazon Prime's 2015 television series Z: The Beginning of Everything. Others include the TV movies Zelda (1993, with Timothy Hutton), F. Scott Fitzgerald in Hollywood (1976, with Jason Miller), and F. Scott Fitzgerald and 'The Last of the Belles' (1974, with Richard Chamberlain). Tom Hiddleston and Alison Pill appear briefly as Fitzgerald and Zelda in Woody Allen's 2011 feature film Midnight in Paris. Guy Pearce and Vanessa Kirby portray the couple in Genius (2016).


Legacy

Some 2,000 pages of work that Fitzgerald had written for Metro-Goldwyn-Mayer were purchased for $475,000 by the University of South Carolina in 2004. The school's Arlyn Bruccoli, a major archivist of work done by the Lost Generation, explained that the cache "corrects this distorted view of Fitzgerald's Hollywood years, the idea that he was just staggering around drunk all the time and not earning his salary."In 2015, an editor of The Strand Magazine discovered and published for the first time an 8,000-word manuscript, dated July 1939, of a Fitzgerald short story titled "Temperature". Long thought lost, Fitzgerald's manuscript for the story was found in the rare books and manuscript archives at Princeton University, his alma mater. As described by Strand, "Temperature", set in Los Angeles, tells the story of the failure, illness and decline of a once successful writer and his life among Hollywood idols, while suffering lingering fevers and indulging in light-hearted romance. The protagonist is a 31-year-old self-destructive, alcoholic named Emmet Monsen, whom Fitzgerald describes in his story as "notably photogenic, slender and darkly handsome". It tells of his personal relationships as his health declined with various doctors, personal assistants, and a Hollywood actress who is his lover. Fitzgerald bibliographies had previously listed the story, sometimes referred to as "The Women in the House", as "unpublished", or as "Lost  mentioned in correspondence, but no surviving transcript or manuscript".In 2017, a rediscovered cache of Fitzgerald's short-stories was published in a collection titled I'd Die For You.An F. Scott Fitzgerald Society was established in 1992 at Hofstra University, and has since become an international association and an affiliate of the American Literature Association. During the COVID-19 pandemic, the society organized a mass online reading of This Side of Paradise to mark its centenary. Fitzgerald is also the namesake of the Fitzgerald Theater in St. Paul, Minnesota, home of the radio broadcast of A Prairie Home Companion.Fitzgerald's childhood Summit Terrace home in St. Paul was listed as a National Historic Landmark in 1971. Fitzgerald reportedly hated the house, labelling it "a mausoleum of American architectural monstrosities."


Selected list of works

This Side of Paradise (1920)
"The Curious Case of Benjamin Button" (1922)
The Beautiful and Damned (1922)
The Diamond as Big as the Ritz (1922)
"Winter Dreams" (1922)
The Great Gatsby (1925)
"Babylon Revisited" (1931)
Tender Is the Night (1934)


Notes and references


Notes


Citations


Works cited


Further reading
Marx, Leo. 1964. The Machine in the Garden: Technology and the Pastoral Ideal in America. New York: Oxford University Press.


External links

F. Scott Fitzgerald Papers at Princeton University
F. Scott Fitzgerald Centenary pagesat the University of South Carolina
Annotated Bibliographyat Scott-Fitzgerald.com
Works by F. Scott Fitzgerald at Project Gutenberg
Works by F. Scott Fitzgerald at Faded Page (Canada)
Works by or about F. Scott Fitzgerald at Internet Archive
Works by F. Scott Fitzgerald at LibriVox (public domain audiobooks) 
Online catalog of F. Scott Fitzgerald's personal library, online at LibraryThing
"Writings of F. Scott Fitzgerald" from C-SPAN's American Writers: A Journey Through History
F. Scott Fitzgerald discussed in Conversations from Penn State interview
F. Scott Fitzgerald in MNopedia, the Minnesota EncyclopediaWhisky or whiskey is a type of distilled alcoholic beverage made from fermented grain mash or by distilling beer. Various grains (which may be malted) are used for different varieties, including barley, corn, rye, and wheat. Whisky is typically aged in wooden casks, generally made of charred white oak.
Whisky is a strictly regulated spirit worldwide with many classes and types. The typical unifying characteristics of the different classes and types are the fermentation of grains, distillation, and aging in wooden barrels.


Etymology
The word whisky (or whiskey) is an anglicisation of the Classical Gaelic word uisce (or uisge) meaning "water" (now written as uisce in Modern Irish, and uisge in Scottish Gaelic). This Gaelic word shares its ultimate origins with Germanic "water" and Slavic "voda" of the same meaning. Distilled alcohol was known in Latin as aqua vitae ("water of life"). This was translated into Old Irish as uisce beatha, which became uisce beatha in Irish and uisge beatha [k bh] in Scottish Gaelic. Early forms of the word in English included uskebeaghe (1581), usquebaugh (1610), usquebath (1621), and usquebae (1715).


Names and spellings
Much is made of the word's two spellings: whisky and whiskey. There are two schools of thought on the issue. One is that the spelling difference is simply a matter of regional language convention for the spelling of a word, indicating that the spelling varies depending on the intended audience or the background or personal preferences of the writer (like the difference between color and colour; or recognize and recognise), and the other is that the spelling should depend on the style or origin of the spirit being described. There is general agreement that when quoting the proper name printed on a label, the spelling on the label should not be altered.The spelling whiskey is common in Ireland and the United States, while whisky is used in all other whisky-producing countries. In the US, the usage has not always been consistent. From the late eighteenth century to the mid twentieth century, American writers used both spellings interchangeably until the introduction of newspaper style guides. Since the 1960s, American writers have increasingly used whiskey as the accepted spelling for aged grain spirits made in the US and whisky for aged grain spirits made outside the US. However, some prominent American brands, such as George Dickel, Maker's Mark, and Old Forester (all made by different companies), use the whisky spelling on their labels, and the Standards of Identity for Distilled Spirits, the legal regulations for spirit in the US, also use the whisky spelling throughout.Within Scotland it is known as whisky, while outside Scotland it is commonly called Scotch whisky, or simply as "Scotch" (especially in North America).


History
It is possible that distillation was practised by the Babylonians in Mesopotamia in the 2nd millennium BC, with perfumes and aromatics being distilled, but this is subject to uncertain and disputed interpretations of evidence.The earliest certain chemical distillations were by Greeks in Alexandria in the 1st century AD, but these were not distillations of alcohol.  The medieval Arabs adopted the distillation technique of the Alexandrian Greeks, and written records in Arabic begin in the 9th century, but again these were not distillations of alcohol. Distilling technology passed from the medieval Arabs to the medieval Latins, with the earliest records in Latin in the early 12th century.The earliest records of the distillation of alcohol are in Italy in the 13th century, where alcohol was distilled from wine. An early description of the technique was given by Ramon Llull (12321315). Its use spread through medieval monasteries, largely for medicinal purposes, such as the treatment of colic and smallpox.The art of distillation spread to Ireland and Scotland no later than the 15th century, as did the common European practice of distilling "aqua vitae", spirit alcohol, primarily for medicinal purposes. The practice of medicinal distillation eventually passed from a monastic setting to the secular via professional medical practitioners of the time, The Guild of Barber Surgeons. The earliest mention of whisky in Ireland comes from the seventeenth-century Annals of Clonmacnoise, which attributes the death of a chieftain in 1405 to "taking a surfeit of aqua vitae" at Christmas. In Scotland, the first evidence of whisky production comes from an entry in the Exchequer Rolls for 1494 where malt is sent "To Friar John Cor, by order of the king, to make aquavitae", enough to make about 500 bottles.James IV of Scotland (r. 14881513) reportedly had a great liking for Scotch whisky, and in 1506 the town of Dundee purchased a large amount of whisky from the Guild of Barber-Surgeons, which held the monopoly on production at the time. Between 1536 and 1541, King Henry VIII of England dissolved the monasteries, sending their monks out into the general public. Whisky production moved out of a monastic setting and into personal homes and farms as newly independent monks needed to find a way to earn money for themselves.

The distillation process was still in its infancy; whisky itself was not allowed to age, and as a result tasted very raw and brutal compared to today's versions. Renaissance-era whisky was also very potent and not diluted. Over time whisky evolved into a much smoother drink.
With a licence to distil Irish whiskey from 1608, the Old Bushmills Distillery in Northern Ireland is the oldest licensed whiskey distillery in the world.In 1707, the Acts of Union merged England and Scotland, and thereafter taxes on it rose dramatically.

After the English Malt Tax of 1725, most of Scotland's distillation was either shut down or forced underground. Scotch whisky was hidden under altars, in coffins, and in any available space to avoid the governmental excisemen or revenuers. Scottish distillers, operating out of homemade stills, took to distilling whisky at night when the darkness hid the smoke from the stills. For this reason, the drink became known as moonshine. At one point, it was estimated that over half of Scotland's whisky output was illegal.In America, whisky was used as currency during the American Revolution; George Washington operated a large distillery at Mount Vernon. Given the distances and primitive transportation network of colonial America, farmers often found it easier and more profitable to convert corn to whisky and transport it to market in that form. It also was a highly coveted sundry and when an additional excise tax was levied against it in 1791, the Whiskey Rebellion erupted.The drinking of Scotch whisky was introduced to India in the nineteenth century. The first distillery in India was built by Edward Dyer at Kasauli in the late 1820s. The operation was soon shifted to nearby Solan (close to the British summer capital Shimla), as there was an abundant supply of fresh spring water there.In 1823, the UK passed the Excise Act, legalizing the distillation (for a fee), and this put a practical end to the large-scale production of Scottish moonshine.In 1831, Aeneas Coffey patented the Coffey still, allowing for a cheaper and more efficient distillation of whisky. In 1850, Andrew Usher began producing a blended whisky that mixed traditional pot still whisky with that from the new Coffey still. The new distillation method was scoffed at by some Irish distillers, who clung to their traditional pot stills. Many Irish contended that the new product was, in fact, not whisky at all.By the 1880s, the French brandy industry was devastated by the phylloxera pest that ruined much of the grape crop; as a result, whisky became the primary liquor in many markets.During the Prohibition era in the United States lasting from 1920 to 1933, all alcohol sales were banned in the country. The federal government made an exemption for whisky prescribed by a doctor and sold through licensed pharmacies. During this time, the Walgreens pharmacy chain grew from 20 retail stores to almost 400.


Production


Distillation
A still for making whisky is usually made of copper, since it removes sulfur-based compounds from the alcohol that would make it unpleasant to drink.  Modern stills are made of stainless steel with copper innards (piping, for example, will be lined with copper along with copper plate inlays along still walls). The simplest standard distillation apparatus is commonly known as a pot still, consisting of a single heated chamber and a vessel to collect purified alcohol.
Column stills are frequently used in the production of grain whisky and are the most commonly used type of still in the production of bourbon and other American whiskeys. Column stills behave like a series of single pot stills, formed in a long vertical tube. Whereas a single pot still charged with wine might yield a vapour enriched to 4060% alcohol, a column still can achieve a vapour alcohol content of 95.6%; an azeotropic mixture of alcohol and water.


Aging
Whiskies do not mature in the bottle, only in the cask, so the "age" of a whisky is only the time between distillation and bottling. This reflects how much the cask has interacted with the whisky, changing its chemical makeup and taste. Whiskies that have been bottled for many years may have a rarity value, but are not "older" and not necessarily "better" than a more recent whisky that matured in wood for a similar time. After a decade or two, additional aging in a barrel does not necessarily improve a whisky.While aging in wooden casks, especially American oak and French oak casks, whisky undergoes six processes that contribute to its final flavour: extraction, evaporation, oxidation, concentration, filtration, and colouration. Extraction in particular results in whisky acquiring a number of compounds, including aldehydes and acids such as vanillin, vanillic acid, and syringaldehyde. Distillers will sometimes age their whiskey in barrels previously used to age other spirits, such as rum or sherry, to impart particular flavours.


Packaging
Most whiskies are sold at or near an alcoholic strength of 40% abv, which is the statutory minimum in some countries  although the strength can vary, and cask-strength whisky may have as much as twice that alcohol percentage.


Exports
Whisky is probably the best known of Scotland's manufactured products. Exports have increased by 87% in the decade to 2012 and it contributes over 4.25 billion to the UK economy, making up a quarter of all its food and drink revenues. In 2012, the US was the largest market for Scotch whisky (655 million), followed by France (535 million). It is also one of the UK's overall top five manufacturing export earners and it supports around 35,000 jobs. Principal whisky producing areas include Speyside and the Isle of Islay, where there are nine distilleries providing a major source of employment. In many places, the industry is closely linked to tourism, with many distilleries also functioning as attractions worth 30 million GVA each year.In 2011, 70% of Canadian whisky was exported, with about 60% going to the US, and the rest mostly to Europe and Asia. 15 million cases of Canadian whisky were sold in the US in 2011.


Types

Whisky or whisky-like products are produced in most grain-growing areas. They differ in base product, alcoholic content, and quality.

Malt whisky is made primarily from malted barley.
Grain whisky is made from any type of grain.Malts and grains are combined in various ways:

Single malt whisky is whisky from a single distillery made from a mash that uses only one particular malted grain. Unless the whisky is described as single-cask, it contains whisky from many casks, and different years, so the blender can achieve a taste recognisable as typical of the distillery. In most cases, single malts bear the name of the distillery, with an age statement and perhaps some indication of some special treatments, such as maturation in a port wine cask.
Blended malt whisky is a mixture of single malt whiskies from different distilleries. If whisky is labelled "pure malt" or just "malt" it is almost certainly a blended malt whisky. This was formerly called a "vatted malt" whisky.
Blended whisky is made from a mixture of different types of whisky. A blend may contain whisky from many distilleries so that the blender can produce a flavour consistent with the brand. The brand name may, therefore, omit the name of a distillery. Most Scotch, Irish and Canadian whisky is sold as part of a blend, even when the spirits are the product of one distillery, as is common in Canada. American blended whisky may contain neutral spirits.
Cask strength (also known as barrel proof) whiskies are rare, and usually, only the very best whiskies are bottled in this way. They are bottled from the cask undiluted or only lightly diluted.
Single cask (also known as single barrel) whiskies are bottled from an individual cask, and often the bottles are labelled with specific barrel and bottle numbers. The taste of these whiskies may vary substantially from cask to cask within a brand.


American

American whiskey is distilled from a fermented mash of cereal grain. It must have the taste, aroma, and other characteristics commonly attributed to whiskey.
Some types of whiskey listed in the United States federal regulations are:

Bourbon whiskey: made from mash that consists of at least 51% corn (maize) and aged in new charred oak barrels.
Corn whiskey: made from mash that consists of at least 80% corn and is not aged, or, if aged, is aged in uncharred or used barrels.
Malt whiskey: made from mash that consists of at least 51% malted barley
Rye whiskey: made from mash that consists of at least 51% rye
Rye malt whiskey: made from mash that consists of at least 51% malted rye
Wheat whiskey: made from mash that consists of at least 51% wheatThese types of American whiskey must be distilled to no more than 80% alcohol by volume, and barrelled at no more than 125 proof. Only water may be added to the final product; the addition of colouring or flavouring is prohibited. These whiskeys must be aged in new charred-oak containers, except for corn whiskey, which does not have to be aged. If it is aged, it must be in uncharred oak barrels or in used barrels. Corn whiskey is usually unaged and sold as a legal version of moonshine.
There is no minimum aging period required for a spirit to legally be called whiskey. If one of these whiskey types reaches two years aging or beyond, it is additionally designated as straight, e.g., straight rye whiskey. A whiskey that fulfils all above requirements but derives from less than 51% of any one specific grain can be called simply a straight whiskey without naming a grain.
US regulations recognize other whiskey categories, including:

Blended whiskey: a mixture that contains a blend of straight whiskeys and neutral grain spirits (NGS), and may also contain flavourings and colourings. The percentage of NGS must be disclosed on the label and may be as much at 80% on a proof gallon basis.
Light whiskey: produced in the US at more than 80% alcohol by volume and stored in used or uncharred new oak containers
Spirit whiskey: a mixture of neutral spirits and at least 5% of certain stricter categories of whiskeyAnother important labelling in the marketplace is Tennessee whiskey, of which Jack Daniel's, George Dickel, Collier and McKeel, and Benjamin Prichard's are the only brands currently bottled. The main difference defining a Tennessee whiskey is that it must be filtered through sugar maple charcoal before aging, known as the Lincoln County Process. (Benjamin Prichard's, which is not so filtered, was grandfathered in when the requirement was introduced in 2017.) The rest of the distillation process of Tennessee Whiskey is identical to bourbon whiskey. Whiskey sold as "Tennessee whiskey" is defined as bourbon under NAFTA and at least one other international trade agreement, and is similarly required to meet the legal definition of bourbon under Canadian law.


Australian

Australian whiskies have won global whisky awards and medals, including the World Whiskies Awards and Jim Murray's Whisky Bible "Liquid Gold Awards".


Canadian

By Canadian law, Canadian whiskies must be produced and aged in Canada, be distilled from a fermented mash of cereal grain, be aged in wood barrels with a capacity limit of 700 litres (185 US gal; 154 imp gal) for not less than three years, and "possess the aroma, taste and character generally attributed to Canadian whisky". The terms "Canadian Whisky", "Rye Whisky", and "Canadian Rye Whisky" are legally indistinguishable in Canada and do not require any specific grain in their production and are often blends of two or more grains. Canadian whiskies may contain caramel and flavouring in addition to the distilled mash spirits, and there is no maximum limit on the alcohol level of the distillation. The minimum bottling proof is 40% ABV. To be exported under one of the "Canadian Whisky" designations, a whisky cannot contain more than 9.09% imported spirits.Canadian whiskies are available throughout the world and are a culturally significant export. Well known brands include Crown Royal, Canadian Club, Seagram's, and Wiser's among others. The historic popularity of Canadian whisky in the United States is partly a result of rum runners illegally importing it into the country during the period of American Prohibition.


Danish
Denmark began producing whisky early in 1974. The first Danish single malt to go on sale was Lille Gadegrd from Bornholm, in 2005. Lille Gadegrd is a winery as well, and uses its own wine casks to mature whisky.
The second Danish distilled single malt whisky for sale was Edition No.1 from the Braunstein microbrewery and distillery. It was distilled in 2007, using water from the Greenlandic ice sheet, and entered the market in March 2010.Another distillery is Stauning Whisky, based in Jutland.


English

There are currently at least six distilleries producing English whisky. Distillers operated in London, Liverpool, and Bristol until the late 19th century, after which production of English single malt whisky ceased until 2003.


Finnish

There are two working distilleries in Finland and a third one is under construction. Whisky retail sales in Finland are controlled solely by the state alcohol monopoly Alko and advertising of strong alcoholic beverages is banned.


Georgia
The first Georgian whisky has been made by Georgian wine-maker, co-founder of "Askaneli Brothers", Jimsher Chkhaidze. JIMSHER whisky is made by traditional Scottish method and is presented from 2016. On the bottle and tag design worked Georgian product designer Zviad Tsikolia.New Georgian blended whisky is presented with three varieties, such are:

Aged in Georgian ex-wine "Saperavi" oak cask;
Aged in Georgian ex-wine "Tsinandali" oak cask;
Aged in ex-Georgian Brandy oak cask;Georgian whisky is available on markets such as China, Poland, Ukraine and Kazakhstan.
In 2017 on "World Whiskey Masters" degustation contest which happened in London, Georgian whisky got silver medal in different categories. On the same year Georgian whisky was rewarded with its first gold medal and silver medals on the contest "Global Travel Retail Spirits Masters".
In March 2018 JIMSHER got rewarded by silver medal in "Best World Whisky" category by "International Whisky Competition".


German

German whisky production is a relatively recent phenomenon having only started in the early 1990s. The styles produced resemble those made in Ireland, Scotland and the United States: single malts, blends, wheat, and bourbon-like styles. There is no standard spelling of German whiskies with distilleries using both "whisky" and "whiskey". In 2008 there were 23 distilleries in Germany producing whisky.


Indian

Distilled alcoholic beverages that are labelled as "whisky" in India are commonly blends based on neutral spirits that are distilled from fermented molasses /Grain with only a small portion consisting of traditional malt whisky, usually about 10 to 12 percent. Outside India, such a drink would more likely be labelled a rum. According to the Scotch Whisky Association's 2013 annual report, "there is no compulsory definition of whisky in India, and the Indian voluntary standard does not require whisky to be distilled from cereals or to be matured." Molasses-based blends make up 90 percent of the spirits consumed as "whisky" in India, although whisky wholly distilled from malt and other grains, is also manufactured and sold. Amrut, the first single malt whisky produced in India, was launched on 24 August 2004.


Irish

Irish whiskeys are normally distilled three times, Cooley Distillery being the exception as they also double distil. Though traditionally distilled using pot stills, the column still is now used to produce grain whiskey for blends. By law, Irish whiskey must be produced in Ireland and aged in wooden casks for a period of no less than three years, although in practice it is usually three or four times that period. Unpeated malt is almost always used, the main exception being Connemara Peated Malt whiskey. There are several types of whiskey common to Ireland: single malt, single grain, blended whiskey and pure pot still whiskey.
Irish whiskey was once the most popular spirit in the world, though a long period of decline from the late 19th century to the late 20th century greatly damaged the industry, so much so that, although Ireland boasted over 30 distilleries in the 1890s, a century later this number had fallen to just three. However, Irish whiskey has seen a great resurgence in popularity since the late twentieth century, and has been the fastest growing spirit in the world every year since 1990. With exports growing by over 15% per annum in recent years, existing distilleries have been expanded and a number of new distilleries constructed. As of mid 2019, Ireland now has twenty-five distilleries in operation, with twenty-four more in either planned or under development. However, many of these have not been operating long enough to have products sufficiently aged for sale, and only one of which was operating prior to 1975.


Mexican

Mexican whisky is relatively young as it has not been as popular in the country as other distilled drinks but recently many distillers in the country have started to make a push to create homegrown whisky and make it as popular as whisky from other countries.


Japanese

Japan produces both single malt and blended whiskies. The base is a mash of malted barley, dried in kilns fired with a little peat (although less than what is used for some peated Scotch whiskies), and is distilled using the pot still method. Before 2000, Japanese whisky was primarily for the domestic market and exports were limited. In recent years, though, Japanese whisky has grown in popularity on the global market. Japanese whiskies such as Suntory and Nikka won many prestigious international awards between 2007 and 2014. Japanese whisky has earned a reputation for quality.


Scotch

Whisky made in Scotland is known as Scotch whisky, or simply as "Scotch" (especially in North America).

Scotch whiskies are generally distilled twice, although some are distilled a third time and others even up to twenty times. Scotch Whisky Regulations require anything bearing the label "Scotch" to be distilled in Scotland and matured for a minimum of three years in oak casks, among other, more specific criteria. Any age statement on the bottle, in the form of a number, must reflect the age of the youngest Scotch whisky used to produce that product. A whisky with an age statement is known as guaranteed age whisky. Scotch whisky without an age statement may, by law, be as young as three years old.The basic types of Scotch are malt and grain, which are combined to create blends. Scotch malt whiskies were divided into five main regions: Highland, Lowland, Islay, Speyside and Campbeltown. Each of the whisky producing regions have a distinct flavour profile and characteristics to the whisky they produce.There is also a sixth region recognized by some sources, though not by the Scotch Whisky Association, The Islands, excluding Islay. This unofficial region, (part of the Highlands according to the Association), includes the following whisky-producing islands making Island single malt: Arran, Jura, Mull, Orkney, and Skye: with their respective distilleries: Arran, Jura, Tobermory, Highland Park, Scapa and Talisker.


Swedish
Whisky started being produced in Sweden in 1955 by the now defunct Skeppets whisky brand. Their last bottle was sold in 1971. In 1999 Mackmyra Whisky was founded and is today the largest producer and has won several awards including European Whisky of the Year in Jim Murray's 2011 Whisky Bible and the International Wine & Spirits Competition (IWSC) 2012 Award for Best European Spirits Producer of 2012.


Taiwanese

Kavalan was the first private whisky distillery in Taiwan. In January 2010, one of the distillery's products caused a stir by beating three Scotch whiskies and one English whisky in a blind tasting organised in Leith, Scotland, to celebrate Burns Night.[4] [5] The distillery was named by Whisky Magazine as the World Icons of Whisky "Whisky Visitor Attraction of the Year" for 2011, and its products have won several other awards.[3] In 2012, Kavalan's Solist Fino Sherry Cask malt whisky was named "new whisky of the year" by Jim Murray in his guide, Jim Murray's Whisky Bible.[6] In 2015, Kavalan's Solist Vinho Barrique Single Cask was named the world's best single malt whisky by World Whiskies Awards.[7] [8] In 2016, Kavalan Solist Amontillado Sherry Single Cask was named the world's best single malt whisky by World Whisky Awards.


Welsh

Although distillation of whisky in Wales began in Middle Ages there were no commercially operated distilleries during the 20th century. The rise of the temperance movement saw the decline the commercial production of liquor during the 19th century and in 1894 Welsh whisky production ceased. The revival of Welsh whisky began in the 1990s.  Initially a "Prince of Wales" malt whisky was sold as Welsh whisky but was simply blended scotch bottled in Wales. A lawsuit by Scotch distillers ended this enterprise. In 2000, Penderyn Distillery started production of Penderyn single malt whisky. The first bottles went on sale on 1 March 2004, Saint David's Day, and it is now sold worldwide. Penderyn Distillery is located in the Brecon Beacons National Park and is considered to be the smallest distillery in the world.


Other
ManX Spirit from the Isle of Man is distilled elsewhere and re-distilled in the country of its nominal "origin". The ManX distillery takes a previously matured Scotch malt whisky and re-distils it.In 2010 a Czech whisky was released, the 21-year-old "Hammer Head".Puni is an Italian distillery in Glurns that makes single malt whisky, including Alba, which is matured in Marsala casks.In 2008 at least two distilleries in the traditionally brandy-producing Caucasus region announced their plans to enter the Russian domestic market with whiskies. The Stavropol-based Praskoveysky distillery bases its product on Irish whiskey, while in Kizlyar, Dagestan's "Russian Whisky" announced a Scotch-inspired drink in single malt, blended and wheat varieties.Destileras y Crianza del Whisky S.A. is a whisky distillery in Spain. Its eight-year-old Whisky DYC is a combination of malts and spirits distilled from barley aged separately a minimum of eight years in American oak barrels.Frysk Hynder is a Frisian single malt, distilled and bottled in the Us Heit Distillery. It is the first single malt produced in Frysln, the Netherlands.Buckwheat whisky is produced by Distillerie des Menhirs in Brittany, France, and by several distillers in the United States.


Chemistry


Overview
Whiskies and other distilled beverages, such as cognac and rum, are complex beverages that contain a vast range of flavouring compounds, of which some 200 to 300 are easily detected by chemical analysis. The flavouring chemicals include "carbonyl compounds, alcohols, carboxylic acids and their esters, nitrogen- and sulfur-containing compounds, tannins, and other polyphenolic compounds, terpenes, and oxygen-containing, heterocyclic compounds" and esters of fatty acids. The nitrogen compounds include pyridines, picolines and pyrazines. The sulfur compounds include thiophenes and polysulfides which seem to contribute to whiskey's roasted character.


Flavours from treating the malt
The distinctive smoky flavour found in various types of whisky, especially Scotch, is due to the use of peat smoke to treat the malt.


Flavours from distillation
The flavouring of whisky is partially determined by the presence of congeners and fusel oils. Fusel oils are higher alcohols than ethanol, are mildly toxic, and have a strong, disagreeable smell and taste. An excess of fusel oils in whisky is considered a defect. A variety of methods are employed in the distillation process to remove unwanted fusel oils.  Traditionally, American distillers focused on secondary filtration using charcoal, gravel, sand, or linen to remove undesired distillates.
Acetals are rapidly formed in distillates and a great many are found in distilled beverages, the most prominent being acetaldehyde diethyl acetal (1,1-diethoxyethane). Among whiskies the highest levels are associated with malt whisky. This acetal is a principal flavour compound in sherry, and contributes fruitiness to the aroma.The diketone diacetyl (2,3-butanedione) has a buttery aroma and is present in almost all distilled beverages. Whiskies and cognacs typically contain more of this than vodkas, but significantly less than rums or brandies.Polysulfides and thiophenes enter whiskey through the distillation process and contribute to its roasted flavour.


Flavours from oak

Whisky that has been aged in oak barrels absorbs substances from the wood. One of these is cis-3-methyl-4-octanolide, known as the "whisky lactone" or "quercus lactone", a compound with a strong coconut aroma.Commercially charred oaks are rich in phenolic compounds. One study identified 40 different phenolic compounds. The coumarin scopoletin is present in whisky, with the highest level reported in Bourbon whiskey.In an experiment, whiskey aged 3 years in orbit on the International Space Station tasted and measured significantly different from similar test subjects in gravity on Earth. Particularly, wood extractives were more present in the space samples.


Flavours and colouring from additives
Depending on the local regulations, additional flavourings and colouring compounds may be added to the whisky. Canadian whisky may contain caramel and flavouring in addition to the distilled mash spirits. Scotch whisky may contain added (E150A) caramel colouring, but no other additives. The addition of flavourings is not allowed in American "straight" whiskey, but is allowed in American blends.


Chill filtration
Whisky is often "chill filtered": chilled to precipitate out fatty acid esters and then filtered to remove them. Most whiskies are bottled this way, unless specified as unchillfiltered or non-chill filtered. This is done primarily for cosmetic reasons. Unchillfiltered whiskies often turn cloudy when stored at cool temperatures or when cool water is added to them, and this is perfectly normal.


The United States of America (USA), commonly known as the United States (U.S. or US) or America, is a country primarily located in North America. It consists of 50 states, a federal district, five major self-governing territories, 326 Indian reservations, and some minor possessions. At 3.8 million square miles (9.8 million square kilometers), it is the world's third- or fourth-largest country by total area. With a population of more than 328 million people, it is the third most populous country in the world. The national capital is Washington, D.C., and the most populous city is New York City.
Paleo-Indians migrated from Siberia to the North American mainland at least 12,000 years ago, and European colonization began in the 16th century. The United States emerged from the thirteen British colonies established along the East Coast. Disputes over taxation and political representation with Great Britain led to the American Revolutionary War (17751783), which established independence. In the late 18th century, the U.S. began vigorously expanding across North America, gradually acquiring new territories, frequently displacing Native Americans, and admitting new states; by 1848, the United States spanned the continent. Slavery was legal in the southern United States until the second half of the 19th century when the American Civil War led to its abolition. The SpanishAmerican War and World War I established the U.S. as a world power, a status confirmed by the outcome of World War II. During the Cold War, the United States and the Soviet Union engaged in various proxy wars but avoided direct military conflict. They also competed in the Space Race, culminating in the 1969 spaceflight that first landed humans on the Moon. The Soviet Union's dissolution in 1991 ended the Cold War, leaving the United States as the world's sole superpower.
The United States is a federal republic and a representative democracy with three separate branches of government, including a bicameral legislature. It is a founding member of the United Nations, World Bank, International Monetary Fund, Organization of American States, NATO, and other international organizations. It is a permanent member of the United Nations Security Council.  Considered a melting pot of cultures and ethnicities, its population has been profoundly shaped by centuries of immigration. The U.S. ranks high in international measures of economic freedom, reduced levels of perceived corruption, quality of life, and quality of higher education. Despite receiving relatively high ratings for its human rights record, the country has received some criticism in regard to inequality related to race, wealth and income, the use of capital punishment, high incarceration rates, and lack of universal health care, among other issues.
The United States is a highly developed country, and continuously ranks high in measures of socioeconomic performance. It accounts for approximately a quarter of global GDP and is the world's largest economy by GDP at market exchange rates. By value, the United States is the world's largest importer and the second-largest exporter of goods. Although its population is only 4.2% of the world total, it holds 29.4% of the total wealth in the world, the largest share held by any country. Making up more than a third of global military spending, it is the foremost military power in the world and is a leading political, cultural, and scientific force internationally.


Etymology

The first known use of the name "America" dates back to 1507, when it appeared on a world map created by the German cartographer Martin Waldseemller. On his map, the name is shown in large letters on what would now be considered South America, in honor of Amerigo Vespucci. The Italian explorer was the first to postulate that the West Indies did not represent Asia's eastern limit but were part of a previously unknown landmass. In 1538, the Flemish cartographer Gerardus Mercator used the name "America" on his own world map, applying it to the entire Western Hemisphere.The first documentary evidence of the phrase "United States of America" dates from a January 2, 1776 letter written by Stephen Moylan to George Washington's aide-de-camp Joseph Reed. Moylan expressed his wish to go "with full and ample powers from the United States of America to Spain" to seek assistance in the revolutionary war effort. The first known publication of the phrase "United States of America" was in an anonymous essay in The Virginia Gazette newspaper in Williamsburg, Virginia, on April 6, 1776.The second draft of the Articles of Confederation, prepared by John Dickinson and completed no later than June 17, 1776, declared "The name of this Confederation shall be the 'United States of America'." The final version of the Articles, sent to the states for ratification in late 1777, stated that "The Stile of this Confederacy shall be 'The United States of America'." In June 1776, Thomas Jefferson wrote the phrase "UNITED STATES OF AMERICA" in all capitalized letters in the headline of his "original Rough draught" of the Declaration of Independence. This draft of the document did not surface until June 21, 1776, and it is unclear whether it was written before or after Dickinson used the term in his June 17 draft of the Articles of Confederation.The short form "United States" is also standard. Other common forms are the "U.S.", the "USA", and "America". The term "America" was seldom used in the United States before the 1890s, and rarely used by presidents before Theodore Roosevelt. It does not appear in patriotic songs composed during the eighteenth and nineteenth centuries, including "The Star Spangled Banner", "My Country, 'Tis of Thee", and the "Battle Hymn of the Republic", although it is common in 20th-century songs like "God Bless America". Colloquial names are the "U.S. of A." and, internationally, the "States". "Columbia", a name popular in American poetry and songs of the late 18th century, derives its origin from Christopher Columbus; it appears in the name "District of Columbia". Many landmarks and institutions in the Western Hemisphere bear his name, including the country of Colombia.
The phrase "United States" was originally plural in American usage. It described a collection of statese.g., "the United States are." The singular form became popular after the end of the Civil War and is now standard usage in the U.S. A citizen of the United States is an "American". "United States", "American" and "U.S." refer to the country adjectivally ("American values", "U.S. forces"). In English, the word "American" rarely refers to topics or subjects not directly connected with the United States.


History


Indigenous peoples and pre-Columbian history

It has been generally accepted that the first inhabitants of North America migrated from Siberia by way of the Bering land bridge and arrived at least 12,000 years ago; however, some evidence suggests an even earlier date of arrival. The Clovis culture, which appeared around 11,000 BC, is believed to represent the first wave of human settlement of the Americas. This was likely the first of three major waves of migration into North America; later waves brought the ancestors of present-day Athabaskans, Aleuts, and Eskimos.Over time, indigenous cultures in North America grew increasingly complex, and some, such as the pre-Columbian Mississippian culture in the southeast, developed advanced agriculture, architecture, and complex societies. The city-state of Cahokia is the largest, most complex pre-Columbian archaeological site in the modern-day United States. In the Four Corners region, Ancestral Puebloan culture developed from centuries of agricultural experimentation. The Haudenosaunee, located in the southern Great Lakes region, was established at some point between the twelfth and fifteenth centuries. Most prominent along the Atlantic coast were the Algonquian tribes, who practiced hunting and trapping, along with limited cultivation.
Estimating the native population of North America at the time of European contact is difficult. Douglas H. Ubelaker of the Smithsonian Institution estimated that there was a population of 92,916 in the south Atlantic states and a population of 473,616 in the Gulf states, but most academics regard this figure as too low. Anthropologist Henry F. Dobyns believed the populations were much higher, suggesting around 1.1 million along the shores of the Gulf of Mexico, 2.2 million people living between Florida and Massachusetts, 5.2 million in the Mississippi Valley and tributaries, and around 700,000 people in the Florida peninsula.


European settlements

The first Europeans to arrive in the continental United States were Spanish conquistadors such as Juan Ponce de Len, who made his first expedition to Florida in 1513. Even earlier, Christopher Columbus had landed in Puerto Rico on his 1493 voyage, and San Juan was settled by the Spanish a decade later. The Spanish set up the first settlements in Florida and New Mexico, such as Saint Augustine, often considered the nation's oldest city, and Santa Fe. The French established their own settlements along the Mississippi River, notably New Orleans. Successful English settlement of the eastern coast of North America began with the Virginia Colony in 1607 at Jamestown and with the Pilgrims colony at Plymouth in 1620. The continent's first elected legislative assembly, Virginia's House of Burgesses, was founded in 1619. Documents such as the Mayflower Compact and the Fundamental Orders of Connecticut established precedents for representative self-government and constitutionalism that would develop throughout the American colonies. Many settlers were dissenting Christians who came seeking religious freedom. In 1784, Russians were the first Europeans to establish a settlement in Alaska, at Three Saints Bay. Russian America once spanned much of the present-day state of Alaska.In the early days of colonization, many European settlers were subject to food shortages, disease, and attacks from Native Americans. Native Americans were also often at war with neighboring tribes and European settlers. In many cases, however, the natives and settlers came to depend on one another. Settlers traded for food and animal pelts; natives for guns, tools and other European goods. Natives taught many settlers to cultivate corn, beans, and other foodstuffs. European missionaries and others felt it was important to "civilize" the Native Americans and urged them to adopt European agricultural practices and lifestyles. However, with the increased European colonization of North America, the Native Americans were displaced and often killed. The native population of America declined after European arrival for various reasons, primarily diseases such as smallpox and measles.

European settlers also began trafficking of African slaves into Colonial America via the transatlantic slave trade. Because of a lower prevalence of tropical diseases and better treatment, slaves had a much higher life expectancy in North America than in South America, leading to a rapid increase in their numbers. Colonial society was largely divided over the religious and moral implications of slavery, and several colonies passed acts both against and in favor of the practice. However, by the turn of the 18th century, African slaves had supplanted European indentured servants as cash crop labor, especially in the American South.The Thirteen Colonies (New Hampshire, Massachusetts, Connecticut, Rhode Island, New York, New Jersey, Pennsylvania, Delaware, Maryland, Virginia, North Carolina, South Carolina, and Georgia) that would become the United States of America were administered by the British as overseas dependencies. All nonetheless had local governments with elections open to most free men. With extremely high birth rates, low death rates, and steady settlement, the colonial population grew rapidly, eclipsing Native American populations. The Christian revivalist movement of the 1730s and 1740s known as the Great Awakening fueled interest both in religion and in religious liberty.During the Seven Years' War (17561763), known in the U.S. as the French and Indian War, British forces captured Canada from the French. With the creation of the Province of Quebec, Canada's francophone population would remain isolated from the English-speaking colonial dependencies of Nova Scotia, Newfoundland and the Thirteen Colonies. Excluding the Native Americans who lived there, the Thirteen Colonies had a population of over 2.1 million in 1770, about a third that of Britain. Despite continuing new arrivals, the rate of natural increase was such that by the 1770s only a small minority of Americans had been born overseas. The colonies' distance from Britain had allowed the development of self-government, but their unprecedented success motivated British monarchs to periodically seek to reassert royal authority.


Independence and expansion

The American Revolutionary War fought by the Thirteen Colonies against the British Empire was the first successful war of independence by a non-European entity against a European power. Americans had developed an ideology of "republicanism", asserting that government rested on the will of the people as expressed in their local legislatures. They demanded their "rights as Englishmen" and "no taxation without representation". The British insisted on administering the empire through Parliament, and the conflict escalated into war.The Second Continental Congress unanimously adopted the Declaration of Independence on July 4, 1776; this day is celebrated annually as Independence Day. In 1777, the Articles of Confederation established a decentralized government that operated until 1789.After its defeat at the Siege of Yorktown in 1781, Britain signed a peace treaty. American sovereignty became internationally recognized, and the country was granted all lands east of the Mississippi River. Tensions with Britain remained, however, leading to the War of 1812, which was fought to a draw. Nationalists led the Philadelphia Convention of 1787 in writing the United States Constitution, ratified in state conventions in 1788. The federal government was reorganized into three branches in 1789, on the principle of creating salutary checks and balances. George Washington, who had led the Continental Army to victory, was the first president elected under the new constitution. The Bill of Rights, forbidding federal restriction of personal freedoms and guaranteeing a range of legal protections, was adopted in 1791.

Although the federal government outlawed American participation in the Atlantic slave trade in 1807, after 1820, cultivation of the highly profitable cotton crop exploded in the Deep South, and along with it, the slave population. The Second Great Awakening, especially in the period 18001840, converted millions to evangelical Protestantism. In the North, it energized multiple social reform movements, including abolitionism; in the South, Methodists and Baptists proselytized among slave populations.Beginning in the late 18th century, American settlers began to expand westward, prompting a long series of American Indian Wars. The 1803 Louisiana Purchase almost doubled the nation's area, Spain ceded Florida and other Gulf Coast territory in 1819, the Republic of Texas was annexed in 1845 during a period of expansionism, and the 1846 Oregon Treaty with Britain led to U.S. control of the present-day American Northwest. Victory in the MexicanAmerican War resulted in the 1848 Mexican Cession of California and much of the present-day American Southwest, making the U.S. span the continent.The California Gold Rush of 184849 spurred migration to the Pacific coast, which led to the California Genocide and the creation of additional western states. The giving away of vast quantities of land to white European settlers as part of the Homestead Acts, nearly 10% of the total area of the United States, and to private railroad companies and colleges as part of land grants spurred economic development. After the Civil War, new transcontinental railways made relocation easier for settlers, expanded internal trade, and increased conflicts with Native Americans. In 1869, a new Peace Policy nominally promised to protect Native Americans from abuses, avoid further war, and secure their eventual U.S. citizenship. Nonetheless, large-scale conflicts continued throughout the West into the 1900s.


Civil War and Reconstruction era

Irreconcilable sectional conflict regarding the enslavement of Africans and African Americans ultimately led to the American Civil War. With the 1860 election of Republican Abraham Lincoln, conventions in thirteen slave states declared secession and formed the Confederate States of America (the "South" or the "Confederacy"), while the federal government (the "Union") maintained that secession was illegal. In order to bring about this secession, military action was initiated by the secessionists, and the Union responded in kind. The ensuing war would become the deadliest military conflict in American history, resulting in the deaths of approximately 618,000 soldiers as well as many civilians. The Union initially simply fought to keep the country united. Nevertheless, as casualties mounted after 1863 and Lincoln delivered his Emancipation Proclamation, the main purpose of the war from the Union's viewpoint became the abolition of slavery. Indeed, when the Union ultimately won the war in April 1865, each of the states in the defeated South was required to ratify the Thirteenth Amendment, which prohibited slavery except as penal labor. Two other amendments were also ratified, ensuring citizenship for blacks and, at least in theory, voting rights for them as well.
Reconstruction began in earnest following the war. While President Lincoln attempted to foster friendship and forgiveness between the Union and the former Confederacy, his assassination on April 14, 1865 drove a wedge between North and South again. Republicans in the federal government made it their goal to oversee the rebuilding of the South and to ensure the rights of African Americans. They persisted until the Compromise of 1877 when the Republicans agreed to cease protecting the rights of African Americans in the South in order for Democrats to concede the presidential election of 1876.
Southern white Democrats, calling themselves "Redeemers", took control of the South after the end of Reconstruction, beginning the nadir of American race relations. From 1890 to 1910, the Redeemers established so-called Jim Crow laws, disenfranchising most blacks and some poor whites throughout the region. Blacks faced racial segregation, especially in the South. They also occasionally experienced vigilante violence, including lynching.


Further immigration, expansion, and industrialization

In the North, urbanization and an unprecedented influx of immigrants from Southern and Eastern Europe supplied a surplus of labor for the country's industrialization and transformed its culture. National infrastructure, including telegraph and transcontinental railroads, spurred economic growth and greater settlement and development of the American Old West. The later invention of electric light and the telephone would also affect communication and urban life.The United States fought Indian Wars west of the Mississippi River from 1810 to at least 1890. Most of these conflicts ended with the cession of Native American territory and their confinement to Indian reservations. Additionally, the Trail of Tears in the 1830s exemplified the Indian removal policy that forcibly resettled Indians. This further expanded acreage under mechanical cultivation, increasing surpluses for international markets. Mainland expansion also included the purchase of Alaska from Russia in 1867. In 1893, pro-American elements in Hawaii overthrew the Hawaiian monarchy and formed the Republic of Hawaii, which the U.S. annexed in 1898. Puerto Rico, Guam, and the Philippines were ceded by Spain in the same year, following the SpanishAmerican War. American Samoa was acquired by the United States in 1900 after the end of the Second Samoan Civil War. The U.S. Virgin Islands were purchased from Denmark in 1917.Rapid economic development during the late 19th and early 20th centuries fostered the rise of many prominent industrialists. Tycoons like Cornelius Vanderbilt, John D. Rockefeller, and Andrew Carnegie led the nation's progress in the railroad, petroleum, and steel industries. Banking became a major part of the economy, with J. P. Morgan playing a notable role. The American economy boomed, becoming the world's largest. These dramatic changes were accompanied by social unrest and the rise of populist, socialist, and anarchist movements. This period eventually ended with the advent of the Progressive Era, which saw significant reforms including women's suffrage, alcohol prohibition, regulation of consumer goods, and greater antitrust measures to ensure competition and attention to worker conditions.


World War I, Great Depression, and World War II

The United States remained neutral from the outbreak of World War I in 1914 until 1917 when it joined the war as an "associated power" alongside the Allies of World War I, helping to turn the tide against the Central Powers. In 1919, President Woodrow Wilson took a leading diplomatic role at the Paris Peace Conference and advocated strongly for the U.S. to join the League of Nations. However, the Senate refused to approve this and did not ratify the Treaty of Versailles that established the League of Nations.In 1920, the women's rights movement won passage of a constitutional amendment granting women's suffrage. The 1920s and 1930s saw the rise of radio for mass communication and the invention of early television. The prosperity of the Roaring Twenties ended with the Wall Street Crash of 1929 and the onset of the Great Depression. After his election as president in 1932, Franklin D. Roosevelt responded with the New Deal. The Great Migration of millions of African Americans out of the American South began before World War I and extended through the 1960s; whereas the Dust Bowl of the mid-1930s impoverished many farming communities and spurred a new wave of western migration.

At first effectively neutral during World War II, the United States began supplying materiel to the Allies in March 1941 through the Lend-Lease program. On December 7, 1941, the Empire of Japan launched a surprise attack on Pearl Harbor, prompting the United States to join the Allies against the Axis powers, and in the following year, to intern about 120,000 U.S. residents (including American citizens) of Japanese descent. Although Japan attacked the United States first, the U.S. nonetheless pursued a "Europe first" defense policy. The United States thus left its vast Asian colony, the Philippines, isolated and fighting a losing struggle against Japanese invasion and occupation. During the war, the United States was one of the "Four Powers" who met to plan the postwar world, along with Britain, the Soviet Union, and China. Although the nation lost around 400,000 military personnel, it emerged relatively undamaged from the war with even greater economic and military influence.The United States played a leading role in the Bretton Woods and Yalta conferences, which signed agreements on new international financial institutions and Europe's postwar reorganization. As an Allied victory was won in Europe, a 1945 international conference held in San Francisco produced the United Nations Charter, which became active after the war. The United States and Japan then fought each other in the largest naval battle in history, the Battle of Leyte Gulf. The United States eventually developed the first nuclear weapons and used them on Japan in the cities of Hiroshima and Nagasaki in August 1945; the Japanese surrendered on September 2, ending World War II.


Cold War and civil rights era

After World War II, the United States and the Soviet Union competed for power, influence, and prestige during what became known as the Cold War, driven by an ideological divide between capitalism and communism. They dominated the military affairs of Europe, with the U.S. and its NATO allies on one side and the Soviet Union and its Warsaw Pact allies on the other. The U.S. developed a policy of containment towards the expansion of communist influence. While the U.S. and Soviet Union engaged in proxy wars and developed powerful nuclear arsenals, the two countries avoided direct military conflict.The United States often opposed Third World movements that it viewed as Soviet-sponsored and occasionally pursued direct action for regime change against left-wing governments, even occasionally supporting authoritarian right-wing regimes. American troops fought communist Chinese and North Korean forces in the Korean War of 19501953. The Soviet Union's 1957 launch of the first artificial satellite and its 1961 launch of the first crewed spaceflight initiated a "Space Race" in which the United States became the first nation to land a man on the Moon in 1969. A proxy war in Southeast Asia eventually evolved into the Vietnam War (19551975), with full American participation.At home, the U.S. had experienced sustained economic expansion and a rapid growth of its population and middle class following World War II. After a surge in female labor participation, especially in the 1970s, by 1985, the majority of women aged 16 and over were employed. Construction of an Interstate Highway System transformed the nation's infrastructure over the following decades. Millions moved from farms and inner cities to large suburban housing developments. In 1959 Hawaii became the 50th and last U.S. state added to the country. The growing Civil Rights Movement used nonviolence to confront segregation and discrimination, with Martin Luther King Jr. becoming a prominent leader and figurehead. A combination of court decisions and legislation, culminating in the Civil Rights Act of 1968, sought to end racial discrimination. Meanwhile, a counterculture movement grew, which was fueled by opposition to the Vietnam war, the Black Power movement, and the sexual revolution.
The launch of a "War on Poverty" expanded entitlements and welfare spending, including the creation of Medicare and Medicaid, two programs that provide health coverage to the elderly and poor, respectively, and the means-tested Food Stamp Program and Aid to Families with Dependent Children.The 1970s and early 1980s saw the onset of stagflation. After his election in 1980, President Ronald Reagan responded to economic stagnation with free-market oriented reforms. Following the collapse of dtente, he abandoned "containment" and initiated the more aggressive "rollback" strategy towards the Soviet Union. The late 1980s brought a "thaw" in relations with the Soviet Union, and its collapse in 1991 finally ended the Cold War. This brought about unipolarity with the U.S. unchallenged as the world's dominant superpower.


Contemporary history

After the Cold War, the conflict in the Middle East triggered a crisis in 1990, when Iraq invaded and annexed Kuwait, an ally of the United States. Fearing the spread of instability, in August, President George H. W. Bush launched and led the Gulf War against Iraq; waged until January 1991 by coalition forces from 34 nations, it ended in the expulsion of Iraqi forces from Kuwait and restoration of the monarchy.Originating within U.S. military defense networks, the Internet spread to international academic platforms and then to the public in the 1990s, greatly affecting the global economy, society, and culture. Due to the dot-com boom, stable monetary policy, and reduced social welfare spending, the 1990s saw the longest economic expansion in modern U.S. history. Beginning in 1994, the U.S. signed the North American Free Trade Agreement (NAFTA), causing trade among the U.S., Canada, and Mexico to soar.On September 11, 2001, Al-Qaeda terrorist hijackers flew passenger planes into the World Trade Center in New York City and the Pentagon near Washington, D.C., killing nearly 3,000 people. In response, President George W. Bush launched the War on Terror, which included a war in Afghanistan and the 20032011 Iraq War. A 2011 military operation in Pakistan led to the death of the leader of Al-Qaeda.Government policy designed to promote affordable housing, widespread failures in corporate and regulatory governance, and historically low interest rates set by the Federal Reserve led to the mid-2000s housing bubble, which culminated with the 2008 financial crisis, the nation's largest economic contraction since the Great Depression. During the crisis, assets owned by Americans lost about a quarter of their value. Barack Obama, the first African-American and multiracial president, was elected in 2008 amid the crisis, and subsequently passed stimulus measures and the DoddFrank Act in an attempt to mitigate its negative effects and ensure there would not be a repeat of the crisis. In 2010, President Obama led efforts to pass the Affordable Care Act, the most sweeping reform to the nation's healthcare system in nearly five decades.In the presidential election of 2016, Republican Donald Trump was elected as the 45th president of the United States. On January 20, 2020, the first case of COVID-19 in the United States was confirmed. As of March 7, 2021, the United States has nearly 29 million COVID-19 cases and over 520,000 deaths. The United States is by far the country with the most confirmed cases of COVID-19 since April 11, 2020.In the presidential election of 2020, Democrat Joe Biden was elected as the 46th president of the United States. On January 6, 2021, supporters of outgoing President Trump stormed the United States Capitol in an unsuccessful effort to disrupt the presidential Electoral College vote count.


Geography

The 48 contiguous states and the District of Columbia occupy a combined area of 3,119,885 square miles (8,080,470 km2). Of this area, 2,959,064 square miles (7,663,940 km2) is contiguous land, composing 83.65% of total U.S. land area. Hawaii, occupying an archipelago in the central Pacific, southwest of North America, is 10,931 square miles (28,311 km2) in area. The populated territories of Puerto Rico, American Samoa, Guam, Northern Mariana Islands, and U.S. Virgin Islands together cover 9,185 square miles (23,789 km2). Measured by only land area, the United States is third in size behind Russia and China, just ahead of Canada.The United States is the world's third- or fourth-largest nation by total area (land and water), ranking behind Russia and Canada and nearly equal to China. The ranking varies depending on how two territories disputed by China and India are counted, and how the total size of the United States is measured.The coastal plain of the Atlantic seaboard gives way further inland to deciduous forests and the rolling hills of the Piedmont. The Appalachian Mountains divide the eastern seaboard from the Great Lakes and the grasslands of the Midwest. The MississippiMissouri River, the world's fourth longest river system, runs mainly northsouth through the heart of the country. The flat, fertile prairie of the Great Plains stretches to the west, interrupted by a highland region in the southeast.The Rocky Mountains, west of the Great Plains, extend north to south across the country, peaking around 14,000 feet (4,300 m) in Colorado. Farther west are the rocky Great Basin and deserts such as the Chihuahua and Mojave. The Sierra Nevada and Cascade mountain ranges run close to the Pacific coast, both ranges reaching altitudes higher than 14,000 feet (4,300 m). The lowest and highest points in the contiguous United States are in the state of California, and only about 84 miles (135 km) apart. At an elevation of 20,310 feet (6,190.5 m), Alaska's Denali is the highest peak in the country and in North America. Active volcanoes are common throughout Alaska's Alexander and Aleutian Islands, and Hawaii consists of volcanic islands. The supervolcano underlying Yellowstone National Park in the Rockies is the continent's largest volcanic feature.The United States, with its large size and geographic variety, includes most climate types. To the east of the 100th meridian, the climate ranges from humid continental in the north to humid subtropical in the south. The Great Plains west of the 100th meridian are semi-arid. Much of the Western mountains have an alpine climate. The climate is arid in the Great Basin, desert in the Southwest, Mediterranean in coastal California, and oceanic in coastal Oregon and Washington and southern Alaska. Most of Alaska is subarctic or polar. Hawaii and the southern tip of Florida are tropical, as well as its territories in the Caribbean and the Pacific. States bordering the Gulf of Mexico are prone to hurricanes, and most of the world's tornadoes occur in the country, mainly in Tornado Alley areas in the Midwest and South. Overall, the United States receives more high-impact extreme weather incidents than any other country in the world.The country joined the Paris Agreement on climate change in 2016 and has many other environmental commitments. It left the Paris Agreement in 2020, and rejoined it in 2021.


Wildlife and conservation

The U.S. is one of 17 megadiverse countries containing a large amount of endemic species: about 17,000 species of vascular plants occur in the contiguous United States and Alaska, and more than 1,800 species of flowering plants are found in Hawaii, few of which occur on the mainland. The United States is home to 428 mammal species, 784 bird species, 311 reptile species, and 295 amphibian species, as well as about 91,000 insect species.There are 62 national parks and hundreds of other federally managed parks, forests, and wilderness areas. Altogether, the government owns about 28% of the country's land area, mostly in the western states. Most of this land is protected, though some is leased for oil and gas drilling, mining, logging, or cattle ranching, and about .86% is used for military purposes.Environmental issues include debates on oil and nuclear energy, dealing with air and water pollution, the economic costs of protecting wildlife, logging and deforestation, and climate change. The most prominent environmental agency is the Environmental Protection Agency (EPA), created by presidential order in 1970. The idea of wilderness has shaped the management of public lands since 1964, with the Wilderness Act. The Endangered Species Act of 1973 is intended to protect threatened and endangered species and their habitats, which are monitored by the United States Fish and Wildlife Service.The United States is ranked 24th among nations in the Environmental Performance Index.


Demographics


Population

The U.S. Census Bureau officially estimated the country's population to be 328,239,523 in 2019, with an unofficial statistical adjustment to 332.6 million as of April 1, 2020. This figure, like most official data for the United States as a whole, excludes the five self-governing territories and minor island possessions. Statistics from the formal 2020 U.S. Census will be reported to the president during early 2021. According to the Bureau's U.S. Population Clock, on January 28, 2021, the U.S. population had a net gain of one person every 100 seconds, or about 864 people per day. The United States is the third most populous nation in the world, after China and India. In 2020 the median age of the United States population was 38.5 years.In 2018, there were almost 90 million immigrants and U.S.-born children of immigrants in the United States, accounting for 28% of the overall U.S. population. The United States has a diverse population; 37 ancestry groups have more than one million members. White Americans of European ancestry, mostly German, Irish, English, Italian, Polish and French, including white Hispanics and Latinos from Latin America, form the largest racial group, at 73.1% of the population. African Americans constitute the nation's largest racial minority and third-largest ancestry group, and are around 13% of the total U.S. population. Asian Americans are the country's second-largest racial minority (the three largest Asian ethnic groups are Chinese, Filipino, and Indian).In 2017, out of the U.S. foreign-born population, some 45% (20.7 million) were naturalized citizens, 27% (12.3 million) were lawful permanent residents, 6% (2.2 million) were temporary lawful residents, and 23% (10.5 million) were unauthorized immigrants. Among current living immigrants to the U.S., the top five countries of birth are Mexico, China, India, the Philippines and El Salvador. Until 2017, the United States led the world in refugee resettlement for decades, admitting more refugees than the rest of the world combined.About 82% of Americans live in urban areas, including suburbs; about half of those reside in cities with populations over 50,000. In 2008, 273 incorporated municipalities had populations over 100,000, nine cities had more than one million residents, and four cities had over two million (namely New York, Los Angeles, Chicago, and Houston). Many U.S. metropolitan populations are growing rapidly, particularly in the South and West.As of 2018, 52% of Americans age 15 and over were married, 6% were widowed, 10% were divorced, and 32% had never been married. The total fertility rate was 1820.5 births per 1000 women in 2016. In 2013, the average age at first birth was 26, and 41% of births were to unmarried women. In 2019, the U.S. had the world's highest rate of children living in single-parent households.


Language

English (specifically, American English) is the de facto national language of the United States. Although there is no official language at the federal level, some lawssuch as U.S. naturalization requirementsstandardize English, and most states have declared English as the official language. Three states and four U.S. territories have recognized local or indigenous languages in addition to English, including Hawaii (Hawaiian), Alaska (twenty Native languages), South Dakota (Sioux), American Samoa (Samoan), Puerto Rico (Spanish), Guam (Chamorro), and the Northern Mariana Islands (Carolinian and Chamorro). In Puerto Rico, Spanish is more widely spoken than English.According to the American Community Survey, in 2010 some 229 million people (out of the total U.S. population of 308 million) spoke only English at home. More than 37 million spoke Spanish at home, making it the second most commonly used language in the United States. Other languages spoken at home by one million people or more include Chinese (2.8 million), Tagalog (1.6 million), Vietnamese (1.4 million), French (1.3 million), Korean (1.1 million), and German (1 million).The most widely taught foreign languages in the United States, in terms of enrollment numbers from kindergarten through university undergraduate education, are Spanish (around 7.2 million students), French (1.5 million), and German (500,000). Other commonly taught languages include Latin, Japanese, American Sign Language, Italian, and Chinese. 18% of all Americans claim to speak both English and another language.


Religion

The First Amendment of the U.S. Constitution guarantees the free exercise of religion and forbids Congress from passing laws respecting its establishment.
The United States has the world's largest Christian population. In a 2014 survey, 70.6% of adults in the United States identified themselves as Christians; Protestants accounted for 46.5%, while Roman Catholics, at 20.8%, formed the largest single Christian group. In 2014, 5.9% of the U.S. adult population claimed a non-Christian religion. These include Judaism (1.9%), Islam (0.9%), Hinduism (0.7%), and Buddhism (0.7%). The survey also reported that 22.8% of Americans described themselves as agnostic, atheist or simply having no religionup from 8.2% in 1990.Protestantism is the largest Christian religious grouping in the United States, accounting for almost half of all Americans. Baptists collectively form the largest branch of Protestantism at 15.4%, and the Southern Baptist Convention is the largest individual Protestant denomination at 5.3% of the U.S. population. Apart from Baptists, other Protestant categories include nondenominational Protestants, Methodists, Pentecostals, unspecified Protestants, Lutherans, Presbyterians, Congregationalists, other Reformed, Episcopalians/Anglicans, Quakers, Adventists, Holiness, Christian fundamentalists, Anabaptists, Pietists, and multiple others.The Bible Belt is an informal term for a region in the Southern United States in which socially conservative evangelical Protestantism is a significant part of the culture and Christian church attendance across the denominations is generally higher than the nation's average. By contrast, religion plays the least important role in New England and in the Western United States.


Health

The Centers for Disease Control and Prevention (CDC) reported that the United States had an average life expectancy at birth of 78.8 years in 2019 (76.3 years for men and 81.4 years for women), up 0.1 year from 2018. This was the second year that overall U.S. life expectancy rose slightly after three years of overall declines that followed decades of continuous improvement. The recent decline, primarily among the age group 25 to 64, was largely due to record highs in the drug overdose and suicide rates; the country still has one of the highest suicide rates among wealthy countries. From 1999 to 2019, more than 770,000 Americans died from drug overdoses. Life expectancy was highest among Asians and Hispanics and lowest among blacks.Increasing obesity in the United States and improvements in health and longevity outside the U.S. contributed to lowering the country's rank in life expectancy from 11th in the world in 1987 to 42nd in 2007. In 2017, the United States had the lowest life expectancy among Japan, Canada, Australia, the United Kingdom, and seven nations in western Europe. Obesity rates have more than doubled in the last 30 years and are the highest in the industrialized world. Approximately one-third of the adult population is obese and an additional third is overweight. Obesity-related type 2 diabetes is considered epidemic by health care professionals.In 2010, coronary artery disease, lung cancer, stroke, chronic obstructive pulmonary diseases, and traffic accidents caused the most years of life lost in the U.S. Low back pain, depression, musculoskeletal disorders, neck pain, and anxiety caused the most years lost to disability. The most harmful risk factors were poor diet, tobacco smoking, obesity, high blood pressure, high blood sugar, physical inactivity, and alcohol use. Alzheimer's disease, drug abuse, kidney disease, cancer, and falls caused the most additional years of life lost over their age-adjusted 1990 per-capita rates. U.S. teenage pregnancy and abortion rates are substantially higher than in other Western nations, especially among blacks and Hispanics.Government-funded health care coverage for the poor (Medicaid, established in 1965) and for those age 65 and older (Medicare, begun in 1966) is available to Americans who meet the programs' income or age qualifications. Nonetheless, the United States remains the only developed nation without a system of universal health care. In 2017, 12.2% of the population did not carry health insurance. The subject of uninsured and underinsured Americans is a major political issue. The Affordable Care Act (ACA), passed in early 2010 and informally known as "ObamaCare", roughly halved the uninsured share of the population. The bill and its ultimate effect are still issues of controversy in the United States. The U.S. health care system far outspends that of any other nation, measured both in per capita spending and as a percentage of GDP. However, the U.S. is a global leader in medical innovation.


Education

American public education is operated by state and local governments and regulated by the United States Department of Education through restrictions on federal grants. In most states, children are required to attend school from the age of six or seven (generally, kindergarten or first grade) until they turn 18 (generally bringing them through twelfth grade, the end of high school); some states allow students to leave school at 16 or 17.About 12% of children are enrolled in parochial or nonsectarian private schools. Just over 2% of children are homeschooled. The U.S. spends more on education per student than any nation in the world, spending an average of $12,794 per year on public elementary and secondary school students in the 20162017 school year. Some 80% of U.S. college students attend public universities.Of Americans 25 and older, 84.6% graduated from high school, 52.6% attended some college, 27.2% earned a bachelor's degree, and 9.6% earned graduate degrees. The basic literacy rate is approximately 99%. The United Nations assigns the United States an Education Index of 0.97, tying it for 12th in the world.The United States has many private and public institutions of higher education. The majority of the world's top universities, as listed by various ranking organizations, are in the U.S. There are also local community colleges with generally more open admission policies, shorter academic programs, and lower tuition.
In 2018, U21, a network of research-intensive universities, ranked the United States first in the world for breadth and quality of higher education, and 15th when GDP was a factor. As for public expenditures on higher education, the U.S. trails some other OECD (Organization for Cooperation and Development) nations but spends more per student than the OECD average, and more than all nations in combined public and private spending. As of 2018, student loan debt exceeded 1.5 trillion dollars.


Government and politics

The United States is a federal republic of 50 states, a federal district, five territories and several uninhabited island possessions. It is the world's oldest surviving federation. It is a federal republic and a representative democracy "in which majority rule is tempered by minority rights protected by law." The U.S. ranked 25th on the Democracy Index in 2018. On Transparency International's 2019 Corruption Perceptions Index, its public sector position deteriorated from a score of 76 in 2015 to 69 in 2019.In the American federalist system, citizens are usually subject to three levels of government: federal, state, and local. The local government's duties are commonly split between county and municipal governments. In almost all cases, executive and legislative officials are elected by a plurality vote of citizens by district.
The government is regulated by a system of checks and balances defined by the U.S. Constitution, which serves as the country's supreme legal document. The original text of the Constitution establishes the structure and responsibilities of the federal government and its relationship with the individual states. Article One protects the right to the writ of habeas corpus. The Constitution has been amended 27 times; the first ten amendments, which make up the Bill of Rights, and the Fourteenth Amendment form the central basis of Americans' individual rights. All laws and governmental procedures are subject to judicial review and any law ruled by the courts to be in violation of the Constitution is voided. The principle of judicial review, not explicitly mentioned in the Constitution, was established by the Supreme Court in Marbury v. Madison (1803) in a decision handed down by Chief Justice John Marshall.The federal government comprises three branches:

Legislative: The bicameral Congress, made up of the Senate and the House of Representatives, makes federal law, declares war, approves treaties, has the power of the purse, and has the power of impeachment, by which it can remove sitting members of the government.
Executive: The president is the commander-in-chief of the military, can veto legislative bills before they become law (subject to congressional override), and appoints the members of the Cabinet (subject to Senate approval) and other officers, who administer and enforce federal laws and policies.
Judicial: The Supreme Court and lower federal courts, whose judges are appointed by the president with Senate approval, interpret laws and overturn those they find unconstitutional.The House of Representatives has 435 voting members, each representing a congressional district for a two-year term. House seats are apportioned among the states by population. Each state then draws single-member districts to conform with the census apportionment. The District of Columbia and the five major U.S. territories each have one member of Congressthese members are not allowed to vote.The Senate has 100 members with each state having two senators, elected at-large to six-year terms; one-third of Senate seats are up for election every two years. The District of Columbia and the five major U.S. territories do not have senators. The president serves a four-year term and may be elected to the office no more than twice. The president is not elected by direct vote, but by an indirect electoral college system in which the determining votes are apportioned to the states and the District of Columbia. The Supreme Court, led by the chief justice of the United States, has nine members, who serve for life.


Political divisions

The 50 states are the principal political divisions in the country. Each state holds jurisdiction over a defined geographic territory, where it shares sovereignty with the federal government. They are subdivided into counties or county equivalents and further divided into municipalities. The District of Columbia is a federal district that contains the capital of the United States, Washington, D.C. The states and the District of Columbia choose the president of the United States. Each state has presidential electors equal to the number of their representatives and senators in Congress; the District of Columbia has three because of the 23rd Amendment. Territories of the United States such as Puerto Rico do not have presidential electors, and so people in those territories cannot vote for the president.The United States also observes tribal sovereignty of the American Indian nations to a limited degree, as it does with the states' sovereignty. American Indians are U.S. citizens and tribal lands are subject to the jurisdiction of the U.S. Congress and the federal courts. Like the states they have a great deal of autonomy, but also like the states, tribes are not allowed to make war, engage in their own foreign relations, or print and issue currency.Citizenship is granted at birth in all states, the District of Columbia, and all major U.S. territories except American Samoa.


Parties and elections

The United States has operated under a two-party system for most of its history. For elective offices at most levels, state-administered primary elections choose the major party nominees for subsequent general elections. Since the general election of 1856, the major parties have been the Democratic Party, founded in 1824, and the Republican Party, founded in 1854. Since the Civil War, only one third-party presidential candidateformer president Theodore Roosevelt, running as a Progressive in 1912has won as much as 20% of the popular vote. The president and vice president are elected by the Electoral College.In American political culture, the center-right Republican Party is considered "conservative" and the center-left Democratic Party is considered "liberal". The states of the Northeast and West Coast and some of the Great Lakes states, known as "blue states", are relatively liberal. The "red states" of the South and parts of the Great Plains and Rocky Mountains are relatively conservative.
Democratic Joe Biden, the winner of the 2020 presidential election and former vice president, is serving as the 46th president of the United States. Leadership in the Senate includes vice president Kamala Harris, president pro tempore Patrick Leahy, Majority Leader Chuck Schumer, and Minority Leader Mitch McConnell. Leadership in the House includes Speaker of the House Nancy Pelosi, Majority Leader Steny Hoyer, and Minority Leader Kevin McCarthy.In the 117th United States Congress, the House of Representatives and the Senate are controlled by the Democratic Party. The Senate consists of 50 Republicans and 48 Democrats with two Independents who caucus with the Democrats; the House consists of 222 Democrats and 211 Republicans. Of state governors, there are 27 Republicans and 23 Democrats. Among the D.C. mayor and the five territorial governors, there are three Democrats, one Republican, and one New Progressive.


Foreign relations

The United States has an established structure of foreign relations. It is a permanent member of the United Nations Security Council. New York City is home to the United Nations Headquarters. Almost all countries have embassies in Washington, D.C., and many have consulates around the country. Likewise, nearly all nations host American diplomatic missions. However, Iran, North Korea, Bhutan, and the Republic of China (Taiwan) do not have formal diplomatic relations with the United States (although the U.S. still maintains unofficial relations with Bhutan and Taiwan). It is a member of the G7, G20, and OECD.
The United States has a "Special Relationship" with the United Kingdom and strong ties with India, Canada, Australia, New Zealand, the Philippines, Japan, South Korea, Israel, and several European Union countries, including France, Italy, Germany, Spain and Poland. It works closely with fellow NATO members on military and security issues and with its neighbors through the Organization of American States and free trade agreements such as the trilateral North American Free Trade Agreement with Canada and Mexico. Colombia is traditionally considered by the United States as its most loyal ally in South America.The U.S. exercises full international defense authority and responsibility for Micronesia, the Marshall Islands and Palau through the Compact of Free Association.


Government finance

Taxation in the United States is progressive, and is levied at the federal, state, and local government levels. This includes taxes on income, payroll, property, sales, imports, estates, and gifts, as well as various fees. Taxation in the United States is based on citizenship, not residency. Both non-resident citizens and Green Card holders living abroad are taxed on their income irrespective of where they live or where their income is earned. The United States is one of the only countries in the world to do so.In 2010 taxes collected by federal, state and municipal governments amounted to 24.8% of GDP. Based on CBO estimates, under 2013 tax law the top 1% will be paying the highest average tax rates since 1979, while other income groups will remain at historic lows. For 2018, the effective tax rate for the wealthiest 400 households was 23%, compared to 24.2% for the bottom half of U.S. households.During fiscal year 2012, the federal government spent $3.54 trillion on a budget or cash basis. Major categories of fiscal year 2012 spending included: Medicare & Medicaid (23%), Social Security (22%), Defense Department (19%), non-defense discretionary (17%), other mandatory (13%) and interest (6%).The United States has the largest external debt in the world and the 34th largest government debt as a percentage of GDP in the world as of 2017; however, more recent estimates vary. The total national debt of the United States was $23.201 trillion, or 107% of GDP, in the fourth quarter of 2019. By 2012, total federal debt had surpassed 100% of U.S. GDP. The U.S. has a credit rating of AA+ from Standard & Poor's, AAA from Fitch, and AAA from Moody's.


Military

The president is the commander-in-chief of the United States Armed Forces and appoints its leaders, the secretary of defense and the Joint Chiefs of Staff. The Department of Defense administers five of the six service branches, which are made up of the Army, Marine Corps, Navy, Air Force, and Space Force. The Coast Guard, also a branch of the armed forces, is normally administered by the Department of Homeland Security in peacetime and can be transferred to the Department of the Navy in wartime. In 2019, all six branches of the U.S. Armed Forces reported 1.4 million personnel on active duty. The Reserves and National Guard brought the total number of troops to 2.3 million. The Department of Defense also employed about 700,000 civilians, not including contractors.

Military service in the United States is voluntary, although conscription may occur in wartime through the Selective Service System. From 1940 until 1973, conscription was mandatory even during peacetime. Today, American forces can be rapidly deployed by the Air Force's large fleet of transport aircraft, the Navy's 11 active aircraft carriers, and Marine expeditionary units at sea with the Navy, and Army's XVIII Airborne Corps and 75th Ranger Regiment deployed by Air Force transport aircraft. The Air Force can strike targets across the globe through its fleet of strategic bombers, maintains the air defense across the United States, and provides close air support to Army and Marine Corps ground forces.The Space Force operates the Global Positioning System, operates the Eastern and Western Ranges for all space launches, and operates the United States' Space Surveillance and Missile Warning networks. The military operates about 800 bases and facilities abroad, and maintains deployments greater than 100 active duty personnel in 25 foreign countries.The United States spent $649 billion on its military in 2019, 36% of global military spending. At 4.7% of GDP, the rate was the second-highest among the top 15 military spenders, after Saudi Arabia. Defense spending plays a major role in science and technology investment, with roughly half of U.S. federal research and development funded by the Department of Defense. Defense's share of the overall U.S. economy has generally declined in recent decades, from early Cold War peaks of 14.2% of GDP in 1953 and 69.5% of federal spending in 1954 to 4.7% of GDP and 18.8% of federal spending in 2011. In total number of personnel, the United States has the third-largest combined armed forces in the world, behind the Chinese People's Liberation Army and Indian Armed Forces.The country is one of the five recognized nuclear weapons states and one of nine countries to possess nuclear weapons. The United States possesses the second-largest stockpile of nuclear weapons in the world, behind the Russian Federation. More than 40% of the world's 14,000 nuclear weapons are held by the United States.


Law enforcement and crime

Law enforcement in the United States is primarily the responsibility of local police departments and sheriff's offices, with state police providing broader services. Federal agencies such as the Federal Bureau of Investigation (FBI) and the U.S. Marshals Service have specialized duties, including protecting civil rights, national security and enforcing U.S. federal courts' rulings and federal laws. State courts conduct most criminal trials while federal courts handle certain designated crimes as well as certain appeals from the state criminal courts.

A cross-sectional analysis of the World Health Organization Mortality Database from 2010 showed that United States homicide rates "were 7.0 times higher than in other high-income countries, driven by a gun homicide rate that was 25.2 times higher." In 2016, the U.S. murder rate was 5.4 per 100,000. 
The United States has the highest documented incarceration rate and largest prison population in the world. As of 2020, the Prison Policy Initiative reported that there were some 2.3 million people incarcerated. According to the Federal Bureau of Prisons, the majority of inmates held in federal prisons are convicted of drug offenses. The imprisonment rate for all prisoners sentenced to more than a year in state or federal facilities is 478 per 100,000 in 2013. About 9% of prisoners are held in privatized prisons, a practice beginning in the 1980s and a subject of contention.Although most nations have abolished capital punishment, it is sanctioned in the United States for certain federal and military crimes, and at the state level in 28 states, though three states have moratoriums on carrying out the penalty imposed by their governors. In 2019, the country had the sixth-highest number of executions in the world, following China, Iran, Saudi Arabia, Iraq, and Egypt. No executions took place from 1967 to 1977, owing in part to a U.S. Supreme Court ruling striking down the practice. Since the decision, however, there have been more than 1,500 executions. In recent years the number of executions and presence of capital punishment statute on whole has trended down nationally, with several states recently abolishing the penalty.


Economy

According to the International Monetary Fund, the U.S. GDP of $16.8 trillion constitutes 24% of the gross world product at market exchange rates and over 19% of the gross world product at purchasing power parity. The United States is the largest importer of goods and second-largest exporter, though exports per capita are relatively low. In 2010, the total U.S. trade deficit was $635 billion. Canada, China, Mexico, Japan, and Germany are its top trading partners.From 1983 to 2008, U.S. real compounded annual GDP growth was 3.3%, compared to a 2.3% weighted average for the rest of the G7. The country ranks ninth in the world in nominal GDP per capita and sixth in GDP per capita at PPP. The U.S. dollar is the world's primary reserve currency.In 2009, the private sector was estimated to constitute 86.4% of the economy. While its economy has reached a postindustrial level of development, the United States remains an industrial power. In August 2010, the American labor force consisted of 154.1 million people (50%). With 21.2 million people, government is the leading field of employment. The largest private employment sector is health care and social assistance, with 16.4 million people. It has a smaller welfare state and redistributes less income through government action than most European nations.The United States is the only advanced economy that does not guarantee its workers paid vacation and is one of a few countries in the world without paid family leave as a legal right. 74% of full-time American workers get paid sick leave, according to the Bureau of Labor Statistics, although only 24% of part-time workers get the same benefits. In 2009, the United States had the third-highest workforce productivity per person in the world, behind Luxembourg and Norway.


Science and technology

The United States has been a leader in technological innovation since the late 19th century and scientific research since the mid-20th century. Methods for producing interchangeable parts were developed by the U.S. War Department by the Federal Armories during the first half of the 19th century. This technology, along with the establishment of a machine tool industry, enabled the U.S. to have large-scale manufacturing of sewing machines, bicycles, and other items in the late 19th century and became known as the American system of manufacturing. Factory electrification in the early 20th century and introduction of the assembly line and other labor-saving techniques created the system of mass production. In the 21st century, approximately two-thirds of research and development funding comes from the private sector. The United States leads the world in scientific research papers and impact factor.In 1876, Alexander Graham Bell was awarded the first U.S. patent for the telephone. Thomas Edison's research laboratory, one of the first of its kind, developed the phonograph, the first long-lasting light bulb, and the first viable movie camera. The latter led to emergence of the worldwide entertainment industry. In the early 20th century, the automobile companies of Ransom E. Olds and Henry Ford popularized the assembly line. The Wright brothers, in 1903, made the first sustained and controlled heavier-than-air powered flight.The rise of fascism and Nazism in the 1920s and 30s led many European scientists, including Albert Einstein, Enrico Fermi, and John von Neumann, to immigrate to the United States. During World War II, the Manhattan Project developed nuclear weapons, ushering in the Atomic Age, while the Space Race produced rapid advances in rocketry, materials science, and aeronautics.The invention of the transistor in the 1950s, a key active component in practically all modern electronics, led to many technological developments and a significant expansion of the U.S. technology industry. This, in turn, led to the establishment of many new technology companies and regions around the country such as Silicon Valley in California. Advancements by American microprocessor companies such as Advanced Micro Devices (AMD) and Intel, along with both computer software and hardware companies such as Adobe Systems, Apple Inc., IBM, Microsoft, and Sun Microsystems, created and popularized the personal computer. The ARPANET was developed in the 1960s to meet Defense Department requirements, and became the first of a series of networks which evolved into the Internet.


Income, poverty and wealth

Accounting for 4.24% of the global population, Americans collectively possess 29.4% of the world's total wealth, the largest percentage of any country. Americans also make up roughly half of the world's population of millionaires. The Global Food Security Index ranked the U.S. number one for food affordability and overall food security in March 2013. Americans on average have more than twice as much living space per dwelling and per person as EU residents. For 2017 the United Nations Development Programme ranked the United States 13th among 189 countries in its Human Development Index (HDI) and 25th among 151 countries in its inequality-adjusted HDI (IHDI).

Wealth, like income and taxes, is highly concentrated; the richest 10% of the adult population possess 72% of the country's household wealth, while the bottom half possess only 2%. According to the Federal Reserve, the top 1% controlled 38.6% of the country's wealth in 2016. In 2017, Forbes found that just three individuals (Jeff Bezos, Warren Buffett and Bill Gates) held more money than the bottom half of the population. According to a 2018 study by the OECD, the United States has a larger percentage of low-income workers than almost any other developed nation, largely because of a weak collective bargaining system and lack of government support for at-risk workers. The top one percent of income-earners accounted for 52 percent of the income gains from 2009 to 2015, where income is defined as market income excluding government transfers.After years of stagnation, median household income reached a record high in 2016 following two consecutive years of record growth. Income inequality remains at record highs however, with the top fifth of earners taking home more than half of all overall income. The rise in the share of total annual income received by the top one percent, which has more than doubled from nine percent in 1976 to 20 percent in 2011, has significantly affected income inequality, leaving the United States with one of the widest income distributions among OECD nations. The extent and relevance of income inequality is a matter of debate.There were about 567,715 sheltered and unsheltered homeless persons in the U.S. in January 2019, with almost two-thirds staying in an emergency shelter or transitional housing program. In 2011, 16.7 million children lived in food-insecure households, about 35% more than 2007 levels, though only 845,000 U.S. children (1.1%) saw reduced food intake or disrupted eating patterns at some point during the year, and most cases were not chronic. As of June 2018, 40 million people, roughly 12.7% of the U.S. population, were living in poverty, including 13.3 million children. Of those impoverished, 18.5 million live in deep poverty (family income below one-half of the poverty threshold) and over five million live "in 'Third World' conditions". In 2017, the U.S. states or territories with the lowest and highest poverty rates were New Hampshire (7.6%) and American Samoa (65%), respectively. The economic impact and mass unemployment caused by the COVID-19 pandemic has raised fears of a mass eviction crisis, with an analysis by the Aspen Institute indicating that between 30 and 40 million people are at risk for eviction by the end of 2020.


Infrastructure


Transportation

Personal transportation is dominated by automobiles, which operate on a network of 4 million miles (6.4 million kilometers) of public roads. The United States has the world's second-largest automobile market, and has the highest vehicle ownership per capita in the world, with 816.4 vehicles per 1,000 Americans (2014). In 2017, there were 255,009,283 non-two wheel motor vehicles, or about 910 vehicles per 1,000 people.The civil airline industry is entirely privately owned and has been largely deregulated since 1978, while most major airports are publicly owned. The three largest airlines in the world by passengers carried are U.S.-based; American Airlines is number one after its 2013 acquisition by US Airways. Of the world's 50 busiest passenger airports, 16 are in the United States, including the busiest, HartsfieldJackson Atlanta International Airport.The United States has the largest rail transport network size of any country in the world with a system length of 125,828 miles, nearly all standard gauge. Amtrak is the main publicly owned passenger railroad service in the United States, providing service in 46 states and the District of Columbia.Transport is the largest single source of greenhouse gas emissions by the United States, which are the second highest by country, exceeded only by China's. The United States has historically been the world's largest producer of greenhouse gases, and greenhouse gas emissions per capita remain high.


Energy

The United States energy market is about 29,000 terawatt hours per year. In 2018, 37% of this energy came from petroleum, 31% from natural gas, and 13% from coal. The remainder was supplied by nuclear and renewable energy sources.


Culture

The United States is home to many cultures and a wide variety of ethnic groups, traditions, and values. Aside from the Native American, Native Hawaiian, and Native Alaskan populations, nearly all Americans or their ancestors immigrated within the past five centuries. Mainstream American culture is a Western culture largely derived from the traditions of European immigrants with influences from many other sources, such as traditions brought by slaves from Africa. More recent immigration from Asia and especially Latin America has added to a cultural mix that has been described as both a homogenizing melting pot, and a heterogeneous salad bowl in which immigrants and their descendants retain distinctive cultural characteristics.Americans have traditionally been characterized by a strong work ethic, competitiveness, and individualism, as well as a unifying belief in an "American creed" emphasizing liberty, equality, private property, democracy, rule of law, and a preference for limited government. Americans are extremely charitable by global standards: according to a 2006 British study, Americans gave 1.67% of GDP to charity, more than any other nation studied.The American Dream, or the perception that Americans enjoy high social mobility, plays a key role in attracting immigrants. Whether this perception is accurate has been a topic of debate. While mainstream culture holds that the United States is a classless society, scholars identify significant differences between the country's social classes, affecting socialization, language, and values. Americans tend to greatly value socioeconomic achievement, but being ordinary or average is also generally seen as a positive attribute.


Literature, philosophy, and visual art

In the 18th and early 19th centuries, American art and literature took most of its cues from Europe. Writers such as Washington Irving, Nathaniel Hawthorne, Edgar Allan Poe, and Henry David Thoreau established a distinctive American literary voice by the middle of the 19th century. Mark Twain and poet Walt Whitman were major figures in the century's second half; Emily Dickinson, virtually unknown during her lifetime, is now recognized as an essential American poet. A work seen as capturing fundamental aspects of the national experience and charactersuch as Herman Melville's Moby-Dick (1851), Twain's The Adventures of Huckleberry Finn (1885), F. Scott Fitzgerald's The Great Gatsby (1925) and Harper Lee's To Kill a Mockingbird (1960)may be dubbed the "Great American Novel."Thirteen U.S. citizens have won the Nobel Prize in Literature. William Faulkner, Ernest Hemingway and John Steinbeck are often named among the most influential writers of the 20th century. Popular literary genres such as the Western and hardboiled crime fiction developed in the United States. The Beat Generation writers opened up new literary approaches, as have postmodernist authors such as John Barth, Thomas Pynchon, and Don DeLillo.The transcendentalists, led by Thoreau and Ralph Waldo Emerson, established the first major American philosophical movement. After the Civil War, Charles Sanders Peirce and then William James and John Dewey were leaders in the development of pragmatism. In the 20th century, the work of W. V. O. Quine and Richard Rorty, and later Noam Chomsky, brought analytic philosophy to the fore of American philosophical academia. John Rawls and Robert Nozick also led a revival of political philosophy.
In the visual arts, the Hudson River School was a mid-19th-century movement in the tradition of European naturalism. The 1913 Armory Show in New York City, an exhibition of European modernist art, shocked the public and transformed the U.S. art scene. Georgia O'Keeffe, Marsden Hartley, and others experimented with new, individualistic styles. Major artistic movements such as the abstract expressionism of Jackson Pollock and Willem de Kooning and the pop art of Andy Warhol and Roy Lichtenstein developed largely in the United States. The tide of modernism and then postmodernism has brought fame to American architects such as Frank Lloyd Wright, Philip Johnson, and Frank Gehry. Americans have long been important in the modern artistic medium of photography, with major photographers including Alfred Stieglitz, Edward Steichen, Edward Weston, and Ansel Adams.


Food

Early settlers were introduced by Native Americans to such indigenous, non-European foods as turkey, sweet potatoes, corn, squash, and maple syrup. They and later immigrants combined these with foods they had known, such as wheat flour, beef, and milk to create a distinctive American cuisine.Homegrown foods are part of a shared national menu on one of America's most popular holidays, Thanksgiving, when some Americans make traditional foods to celebrate the occasion.The American fast food industry, the world's largest, pioneered the drive-through format in the 1940s. Characteristic dishes such as apple pie, fried chicken, pizza, hamburgers, and hot dogs derive from the recipes of various immigrants. French fries, Mexican dishes such as burritos and tacos, and pasta dishes freely adapted from Italian sources are widely consumed. Americans drink three times as much coffee as tea. Marketing by U.S. industries is largely responsible for making orange juice and milk ubiquitous breakfast beverages.


Music

One of America's early composers was a man named William Billings who, born in Boston, composed patriotic hymns in the 1770s. From the 1800s John Philip Sousa is regarded as one of America's greatest composers.Although little known at the time, Charles Ives's work of the 1910s established him as the first major U.S. composer in the classical tradition, while experimentalists such as Henry Cowell and John Cage created a distinctive American approach to classical composition. Aaron Copland and George Gershwin developed a new synthesis of popular and classical music.
The rhythmic and lyrical styles of African-American music have deeply influenced American music at large, distinguishing it from European and African traditions. Elements from folk idioms such as the blues and what is now known as old-time music were adopted and transformed into popular genres with global audiences. Jazz was developed by innovators such as Louis Armstrong and Duke Ellington early in the 20th century. Country music developed in the 1920s, and rhythm and blues in the 1940s.Elvis Presley and Chuck Berry were among the mid-1950s pioneers of rock and roll. Rock bands such as Metallica, the Eagles, and Aerosmith are among the highest grossing in worldwide sales. In the 1960s, Bob Dylan emerged from the folk revival to become one of America's most celebrated songwriters and James Brown led the development of funk.
More recent American creations include hip hop, salsa, techno, and house music. American pop stars such as Bing Crosby, Elvis Presley, Michael Jackson and Madonna have become global celebrities, as have contemporary musical artists such as Prince, Mariah Carey, Jennifer Lopez, Justin Timberlake, Britney Spears, Christina Aguilera, Beyonc, Bruno Mars, Katy Perry, Lady Gaga, Taylor Swift, and Ariana Grande.


Cinema

Hollywood, a northern district of Los Angeles, California, is one of the leaders in motion picture production. The world's first commercial motion picture exhibition was given in New York City in 1894, using Thomas Edison's Kinetoscope. Since the early 20th century, the U.S. film industry has largely been based in and around Hollywood, although in the 21st century an increasing number of films are not made there, and film companies have been subject to the forces of globalization.Director D. W. Griffith, the top American filmmaker during the silent film period, was central to the development of film grammar, and producer/entrepreneur Walt Disney was a leader in both animated film and movie merchandising. Directors such as John Ford redefined the image of the American Old West, and, like others such as John Huston, broadened the possibilities of cinema with location shooting. The industry enjoyed its golden years, in what is commonly referred to as the "Golden Age of Hollywood", from the early sound period until the early 1960s, with screen actors such as John Wayne and Marilyn Monroe becoming iconic figures. In the 1970s, "New Hollywood" or the "Hollywood Renaissance" was defined by grittier films influenced by French and Italian realist pictures of the post-war period. In more recent times, directors such as Steven Spielberg, George Lucas and James Cameron have gained renown for their blockbuster films, often characterized by high production costs and earnings.
Notable films topping the American Film Institute's AFI 100 list include Orson Welles's Citizen Kane (1941), which is frequently cited as the greatest film of all time, Casablanca (1942), The Godfather (1972), Gone with the Wind (1939), Lawrence of Arabia (1962), The Wizard of Oz (1939), The Graduate (1967), On the Waterfront (1954), Schindler's List (1993), Singin' in the Rain (1952), It's a Wonderful Life (1946) and Sunset Boulevard (1950). The Academy Awards, popularly known as the Oscars, have been held annually by the Academy of Motion Picture Arts and Sciences since 1929, and the Golden Globe Awards have been held annually since January 1944.


Sports

American football is by several measures the most popular spectator sport; the National Football League (NFL) has the highest average attendance of any sports league in the world, and the Super Bowl is watched by tens of millions globally. Baseball has been regarded as the U.S. national sport since the late 19th century, with Major League Baseball (MLB) being the top league. Basketball and ice hockey are the country's next two leading professional team sports, with the top leagues being the National Basketball Association (NBA) and the National Hockey League (NHL). College football and basketball attract large audiences. In soccer (a sport that has gained a footing in the United States since the mid-1990s), the country hosted the 1994 FIFA World Cup, the men's national soccer team qualified for ten World Cups and the women's team has won the FIFA Women's World Cup four times; Major League Soccer is the sport's highest league in the United States (featuring 23 American and three Canadian teams). The market for professional sports in the United States is roughly $69 billion, roughly 50% larger than that of all of Europe, the Middle East, and Africa combined.Eight Olympic Games have taken place in the United States. The 1904 Summer Olympics in St. Louis, Missouri, were the first-ever Olympic Games held outside of Europe. As of 2017, the United States has won 2,522 medals at the Summer Olympic Games, more than any other country, and 305 in the Winter Olympic Games, the second most behind Norway.
While most major U.S. sports such as baseball and American football have evolved out of European practices, basketball, volleyball, skateboarding, and snowboarding are American inventions, some of which have become popular worldwide. Lacrosse and surfing arose from Native American and Native Hawaiian activities that predate Western contact. The most-watched individual sports are golf and auto racing, particularly NASCAR and IndyCar.


Mass media

The four major broadcasters in the U.S. are the National Broadcasting Company (NBC), Columbia Broadcasting System (CBS), American Broadcasting Company (ABC), and Fox Broadcasting Company (FOX). The four major broadcast television networks are all commercial entities. Cable television offers hundreds of channels catering to a variety of niches. Americans listen to radio programming, also largely commercial, on average just over two-and-a-half hours a day.In 1998, the number of U.S. commercial radio stations had grown to 4,793 AM stations and 5,662 FM stations. In addition, there are 1,460 public radio stations. Most of these stations are run by universities and public authorities for educational purposes and are financed by public or private funds, subscriptions, and corporate underwriting. Much public-radio broadcasting is supplied by NPR. NPR was incorporated in February 1970 under the Public Broadcasting Act of 1967; its television counterpart, PBS, was created by the same legislation. As of September 30, 2014, there are 15,433 licensed full-power radio stations in the U.S. according to the U.S. Federal Communications Commission (FCC).Well-known newspapers include The Wall Street Journal, The New York Times, and USA Today. Although the cost of publishing has increased over the years, the price of newspapers has generally remained low, forcing newspapers to rely more on advertising revenue and on articles provided by a major wire service, such as the Associated Press or Reuters, for their national and world coverage. With very few exceptions, all the newspapers in the U.S. are privately owned, either by large chains such as Gannett or McClatchy, which own dozens or even hundreds of newspapers; by small chains that own a handful of papers; or in a situation that is increasingly rare, by individuals or families. Major cities often have "alternative weeklies" to complement the mainstream daily papers, such as New York City's The Village Voice or Los Angeles' LA Weekly. Major cities may also support a local business journal, trade papers relating to local industries, and papers for local ethnic and social groups. Aside from web portals and search engines, the most popular websites are Facebook, YouTube, Wikipedia, Yahoo!, eBay, Amazon, and Twitter.More than 800 publications are produced in Spanish, the second most commonly used language in the United States behind English.


Bison are large, even-toed ungulates in the genus Bison within the subfamily Bovinae.
Two extant and six extinct species are recognised. Of the six extinct species, five became extinct in the Quaternary extinction event. Bison palaeosinensis evolved in the Early Pleistocene in South Asia, and was the evolutionary ancestor of B. priscus (steppe bison), which was the ancestor of all other Bison species. From 2 million years ago to 6,000 BC, steppe bison ranged across the mammoth steppe, inhabiting Europe and northern Asia with B. schoetensacki (woodland bison), and North America with B. antiquus, B. latifrons, and B. occidentalis. The last species to go extinct, B. occidentalis, was succeeded at 3,000 BC by B. bison.
Of the two surviving species, the American bison, B. bison, found only in North America, is the more numerous. Although commonly known as a buffalo in the United States and Canada, it is only distantly related to the true buffalo. The North American species is composed of two subspecies, the Plains bison, B. b. bison, and the wood bison, B. b. athabascae, which is the namesake of Wood Buffalo National Park in Canada. A third subspecies, the eastern bison (B. b. pennsylvanicus) is no longer considered a valid taxon, being a junior synonym of B. b. bison. References to "woods bison" or "wood bison" from the eastern United States confusingly refer to this subspecies, not B. b. athabascae, which was not found in the region. The European bison, B. bonasus, or wisent, is found in Europe and the Caucasus, reintroduced after being extinct in the wild.
While all bison species are classified in their own genus, they are sometimes bred with domestic cattle (genus Bos) and produce sometimes fertile offspring called beefalo or zubron.


Description

The American bison and the European bison (wisent) are the largest surviving terrestrial animals in North America and Europe. They are typical artiodactyl (cloven hooved) ungulates, and are similar in appearance to other bovines such as cattle and true buffalo. They are broad and muscular with shaggy coats of long hair. Adults grow up to 2 metres (6 feet 7 inches) in height and 3.5 m (11 ft 6 in) in length for American bison and up to 2.1 m (6 ft 11 in) in height and 2.9 m (9 ft 6 in) in length for European bison. American bison can weigh from around 400 to 1,270 kilograms (880 to 2,800 pounds) and European bison can weigh from 800 to 1,000 kg (1,800 to 2,200 lb). European bison tend to be taller than American bison.
Bison are nomadic grazers and travel in herds. The bulls leave the herds of females at two or three years of age, and join a herd of males, which are generally smaller than female herds. Mature bulls rarely travel alone. Towards the end of the summer, for the reproductive season, the sexes necessarily commingle.American bison are known for living in the Great Plains, but formerly had a much larger range, including much of the eastern United States and parts of Mexico. Both species were hunted close to extinction during the 19th and 20th centuries, but have since rebounded. The wisent in part owes its survival to the Chernobyl disaster, as the Chernobyl Exclusion Zone has become a kind of wildlife preserve for wisent and other rare megafauna such as the Przewalski's horse, though poaching has become a threat in recent years. The American Plains bison is no longer listed as endangered, but this does not mean the species is secure. Genetically pure B. b. bison currently number only about 20,000, separated into fragmented herdsall of which require active conservation measures. The wood bison is on the endangered species list in Canada and is listed as threatened in the United States, though numerous attempts have been made by beefalo ranchers to have it entirely removed from the Endangered Species List.

Although superficially similar, physical and behavioural differences exist between the American and European bison. The American species has 15 ribs, while the European bison has 14. The American bison has four lumbar vertebrae, while the European has five. (The difference in this case is that what would be the first lumbar vertebra has ribs attached to it in American bison and is thus counted as the 15th thoracic vertebra, compared to 14 thoracic vertebrae in wisent.) Adult American bison are less slim in build and have shorter legs. American bison tend to graze more, and browse less than their European relatives. Their anatomies reflect this behavioural difference; the American bison's head hangs lower than the European's. The body of the American bison is typically hairier, though its tail has less hair than that of the European bison. The horns of the European bison point through the plane of their faces, making them more adept at fighting through the interlocking of horns in the same manner as domestic cattle, unlike the American bison, which favours butting. American bison are more easily tamed than their European cousins, and breed with domestic cattle more readily.


Evolution and genetic history
The bovine tribe (Bovini) split about 5 to 10 million years ago into the buffalos (Bubalus and Syncerus) and a group leading to bison and taurine cattle. Thereafter, the family lineage of bison and taurine cattle does not appear to be a straightforward "tree" structure as is often depicted in much evolution, because evidence of interbreeding and crossbreeding is seen between different species and members within this family, even many millions of years after their ancestors separated into different species. This crossbreeding was not sufficient to conflate the different species back together, but it has resulted in unexpected relationships between many members of this group, such as yak being related to American bison, when such relationships would otherwise not be apparent.
A 2003 study of mitochondrial DNA indicated four distinct maternal lineages in tribe Bovini:

Taurine cattle and zebu
Wisent
American bison and yak and
Banteng, gaur, and gayalHowever, Y chromosome analysis associated wisent and American bison. An earlier study using amplified fragment length polymorphism fingerprinting showed a close association of wisent with American bison, and probably with the yak, but noted that the interbreeding of Bovini species made determining relationships problematic.The genus Bison diverged from the lineage that led to cattle (Bos primigenius) at the Plio-Pleistocene boundary in South Asia. Two extant and six extinct species are recognised. Of the six extinct species, five went extinct in the Quaternary extinction event. Three were North American endemics: Bison antiquus, B. latifrons, and B. occidentalis. The fourth, B. priscus (steppe bison), ranged across steppe environments from Western Europe, through Central Asia, East Asia including Japan, and onto North America. The fifth, B. schoetensacki.  (woodland bison), inhabited Eurasian forests, extending from western Europe to the south of Siberia.

The sixth, B. palaeosinensis, evolving in the Early Pleistocene in South Asia, is presumed to have been the evolutionary ancestor of B. priscus and all successive Bison lineages. The steppe bison (B. priscus) evolved from Bison palaeosinensis in the Early Pleistocene. B. priscus is seen clearly in the fossil record around 2 million years ago. The steppe bison spread across Eurasia, and all proceeding contemporary and successive species are believed to have derived from the steppe bison. Going extinct around 6,000 BCE in Siberia and around 5,400 BCE in Alaska, outlasted only by B. occidentalis, B. bonasus and B. bison, the steppe bison was the predominant bison pictured in the ancient cave paintings of Spain and Southern France.
The modern European bison is likely to have arisen from the steppe bison. There is no direct fossil evidence of successive species between the steppe bison and the European bison, though there are three possible lines of ancestry pertaining to the European wisent. Past research has suggested that the European bison is descended from bison that had migrated from Asia to North America, and then back to Europe, where they crossbred with existing steppe bison. However, more recent phylogenetic research points to an origin either from the phenotypically and genetically similar Pleistocene woodland bison (B. schoetensacki) or as the result of an interbreeding event between the steppe bison and the aurochs (Bos primigenius), the ancestor of domesticated cattle, around 120,000 years ago. The possible hybrid is referred to in vernacular as the 'Higgs bison' as a hat-tip to the discovery process of the Higgs boson.At one point, some steppe bison crossbred with the ancestors of the modern yak. After that crossbreeding, a population of steppe bison crossed the Bering Land Bridge to North America. The steppe bison spread through the northern parts of North America and lived in Eurasia until around 11,000 years ago and North America until 4,000 to 8,000 years ago.The Pleistocene woodland bison (B. schoetensacki) evolved in the Middle Pleistocene from B. priscus, and tended to inhabit the dry conifer forests and woodland which lined the mammoth steppe, occupying a range from western Europe to the south of Siberia. Although their fossil records are far rarer than their antecedent, they are thought to have existed until at least 36,000 BCE.Bison latifrons (the "giant" or "longhorn" bison) is thought to have evolved in midcontinent North America from B. priscus, after the steppe bison crossed into North America. Giant bison (B. latifrons) appeared in the fossil record about 120,000 years ago. B. latifrons was one of many species of North American megafauna that became extinct during the transition from the Pleistocene to the Holocene epoch (an event referred to as the Quaternary extinction event). It is thought to have disappeared some 21,00030,000 years ago, during the late Wisconsin glaciation.B. latifrons co-existed with the slightly smaller B. antiquus for over 100,000 years. Their predecessor, the steppe bison appeared in the North American fossil record around 190,000 years ago. B. latifrons is believed to have been a more woodland-dwelling, non-herding species, while B. antiquus was a herding grassland-dweller, very much like its descendant B. bison. B. antiquus gave rise to both B. occidentalis, and later B. bison, the modern American bison, some 5,000 to 10,000 years ago. B. antiquus was the most common megafaunal species on the North American continent during much of the Late Pleistocene and is the most commonly found large animal found at the La Brea Tar Pits.In 2016, DNA extracted from Bison priscus fossil remains beneath a 130,000-year-old volcanic ashfall in the Yukon suggested recent arrival of the species. That genetic material indicated that all American bison had a common ancestor 135,000 to 195,000 years ago, during which period the Bering Land Bridge was exposed; this hypothesis precludes an earlier arrival. The researchers sequenced mitochondrial genomes from both that specimen and from the remains of a recently discovered, estimated 120,000-year-old giant, long-horned, B. latifrons from Snowmass, Colorado. The genetic information also indicated that a second, Pleistocene migration of bison over the land bridge occurred 21,000 to 45,000 years ago.

During the population bottleneck, after the great slaughter of American bison during the 19th century, the number of bison remaining alive in North America declined to as low as 541. During that period, a handful of ranchers gathered remnants of the existing herds to save the species from extinction. These ranchers bred some of the bison with cattle in an effort to produce "cattleo" (today called "beefalo") Accidental crossings were also known to occur. Generally, male domestic bulls were crossed with buffalo cows, producing offspring of which only the females were fertile. The crossbred animals did not demonstrate any form of hybrid vigor, so the practice was abandoned. Wisent-American bison hybrids were briefly experimented with in Germany (and found to be fully fertile) and a herd of such animals is maintained in Russia. A herd of cattle-wisent crossbreeds (zubron) is maintained in Poland. First-generation crosses do not occur naturally, requiring caesarean delivery. First-generation males are infertile. The U.S. National Bison Association has adopted a code of ethics that prohibits its members from deliberately crossbreeding bison with any other species. In the United States, many ranchers are now using DNA testing to cull the residual cattle genetics from their bison herds. The proportion of cattle DNA that has been measured in introgressed individuals and bison herds today is typically quite low, ranging from 0.56 to 1.8%.There are also remnant purebred American bison herds on public lands in North America. Herds of importance are found in Yellowstone National Park, Wind Cave National Park in South Dakota, Blue Mounds State Park in Minnesota, Elk Island National Park in Alberta, and Grasslands National Park in Saskatchewan. In 2015, a purebred herd of 350 individuals was identified on public lands in the Henry Mountains of southern Utah via genetic testing of mitochondrial and nuclear DNA. This study, published in 2015, also showed the Henry Mountains bison herd to be free of brucellosis, a bacterial disease that was imported with non-native domestic cattle to North America.


Behavior

Wallowing is a common behavior of bison. A bison wallow is a shallow depression in the soil, either wet or dry. Bison roll in these depressions, covering themselves with mud or dust. Possible explanations suggested for wallowing behavior include grooming behavior associated with moulting, male-male interaction (typically rutting behavior), social behavior for group cohesion, play behavior, relief from skin irritation due to biting insects, reduction of ectoparasite load (ticks and lice), and thermoregulation. In the process of wallowing, bison may become infected by the fatal disease anthrax, which may occur naturally in the soil.Bison temperament is often unpredictable. They usually appear peaceful, unconcerned, even lazy, yet they may attack anything, often without warning or apparent reason. They can move at speeds up to 56 km/h (35 mph) and cover long distances at a lumbering gallop.Their most obvious weapons are the horns borne by both males and females, but their massive heads can be used as battering rams, effectively using the momentum produced by what is a typical weight of 900 to 1,200 kilograms (2,000 to 2,700 lb) moving at 50 km/h (30 mph). The hind legs can also be used to kill or maim with devastating effect.  In the words of early naturalists, they were dangerous, savage animals that feared no other animal and in prime condition could best any foe (except for wolves and brown bears).
The rutting, or mating, season lasts from June through September, with peak activity in July and August. At this time, the older bulls rejoin the herd, and fights often take place between bulls. The herd exhibits much restlessness during breeding season. The animals are belligerent, unpredictable, and most dangerous.


Habitat

American bison live in river valleys and on prairies and plains. Typical habitat is open or semiopen grasslands, as well as sagebrush, semiarid lands, and scrublands. Some lightly wooded areas are also known historically to have supported bison. They also graze in hilly or mountainous areas where the slopes are not steep. Though not particularly known as high-altitude animals, bison in the Yellowstone Park bison herd are frequently found at elevations above 8,000 feet and the Henry Mountains bison herd is found on the plains around the Henry Mountains, Utah, as well as in mountain valleys of the Henry Mountains to an altitude of 10,000 feet.
European bison most commonly live in lightly wooded to fully wooded areas as well as areas with increased shrubs and bushes. European bison can sometimes be found living on grasslands and plains as well.


Restrictions
Throughout most of their historical range, landowners have sought restrictions on free-ranging bison. Herds on private land are required to be fenced in. In the state of Montana, free-ranging bison on public lands may be shot, due to concerns about transmission of disease to cattle and damage to public property. In 2013, Montana legislative measures concerning the bison were proposed and passed the legislature, but opposed by Native American tribes as they impinged on sovereign tribal rights. Three such bills were vetoed by Steve Bullock, the governor of Montana. The bison's circumstances remain an issue of contention between Native American tribes and private landowners.


Diet

Bison are ruminants, which gives them the ability to ferment plants in a specialized stomach prior to digesting them. Bison were once thought to almost exclusively consume grasses and sedges, but are now known to consume a wide-variety of plants including woody plants and herbaceous eudicots. Over the course of the year, bison shift which plants they select in their diet based on which plants have the highest protein or energy concentrations at a given time and will reliably consume the same species of plants across years. Protein concentrations of the plants they eat tend to be highest in the spring and decline thereafter, reaching their lowest in the winter. In Yellowstone National Park, bison browsed willows and cottonwoods, not only in the winter when few other plants are available, but also in the summer. Bison are thought to migrate to optimize their diet, and will concentrate their feeding on recently burned areas due to the higher quality forage that regrows after the burn. Wisent tend to browse on shrubs and low-hanging trees more often than do the American bison, which prefer grass to shrubbery and trees.


Reproduction
Female bison typically do not reproduce until three years of age and can reproduce to at least 19 years of age. Female bison can produce calves annually as long as their nutrition is sufficient, but will not give birth to a calf after years where weight gain was too low. A mother's probability of reproduction the following year is strongly dependent on the mother's mass and age. Heavier female bison produce heavier calves (weighed in the fall at weaning) than light mothers, while the weight of calves is lower for older mothers (after age 8).


Predators

Owing to their size, bison have few predators. Five notable exceptions are humans, grey wolves, cougars, grizzly bears, and coyotes. Wolves generally take down a bison while in a pack, but cases of a single wolf killing bison have been reported. Grizzly bears also consume bison, often by driving off the pack and consuming the wolves' kill. Grizzly bears and coyotes also prey on bison calves. Historically and prehistorically, lions, cave lions, tigers, dire wolves, Smilodon, Homotherium, cave hyenas, and Neanderthals had posed threats to bison.


Infections and illness
For the American bison, the main cause of illness is malignant catarrhal fever, though brucellosis is a serious concern in the Yellowstone Park bison herd. Bison in the Antelope Island bison herd are regularly inoculated against brucellosis, parasites, Clostridium infection, infectious bovine rhinotracheitis, and bovine vibriosis.The major concerns for illness in European bison are foot-and-mouth disease and balanoposthitis, which affects the male sex organs; a number of parasitic diseases have also been cited as threats. The inbreeding of the species caused by the small population plays a role in a number of genetic defects and immunity to diseases, which in turn poses greater risks to the population.


Name
The term "buffalo" is sometimes considered to be a misnomer for this animal, as it is only distantly related to either of the two "true buffalo", the Asian water buffalo and the African buffalo. Samuel de Champlain applied the term buffalo (buffles in French) to the bison in 1616 (published 1619), after seeing skins and a drawing shown to him by members of the Nipissing First Nation, who said they travelled 40 days (from east of Lake Huron) to trade with another nation who hunted the animals. Though "bison" might be considered more scientifically correct, "buffalo" is also considered  correct as a result of standard usage in American English, and is listed in many dictionaries as an acceptable name for American buffalo or bison. Buffalo has a much longer history than bison, which was first recorded in 1774.


Human impact

Bison was a significant resource for indigenous peoples of North America for food and raw materials until near extinction in the late 19th century. In fact, for the indigenous peoples of the Plains, it was their principal food source. Native Americans highly valued their relationship with the bison, saw them as sacred, and treated them in such a respectful way as to ensure their ongoing longevity and abundance. In his biography, Lakota teacher and elder John Fire Lame Deer describes the relationship as such:
The buffalo gave us everything we needed. Without it we were nothing. Our tipis were made of his skin. His hide was our bed, our blanket, our winter coat. It was our drum, throbbing through the night, alive, holy. Out of his skin we made our water bags. His flesh strengthened us, became flesh of our flesh. Not the smallest part of it was wasted. His stomach, a red-hot stone dropped into it, became our soup kettle. His horns were our spoons, the bones our knives, our women's awls and needles. Out of his sinews we made our bowstrings and thread. His ribs were fashioned into sleds for our children, his hoofs became rattles. His mighty skull, with the pipe leaning against it, was our sacred altar. The name of the greatest of all Sioux was Tatanka IyotakeSitting Bull. When you killed off the buffalo you also killed the Indianthe real, natural, "wild" Indian.

Humans, notably European settlers, were almost exclusively accountable for the near-extinction of the American bison in the 1800s. At the beginning of the century, tens of millions of bison roamed North America. Pioneers and settlers slaughtered an estimated 50 million bison during the 19th century, although the causes of decline and the numbers killed are disputed and debated. Railroads were advertising "hunting by rail", where trains encountered large herds alongside or crossing the tracks. Men aboard fired from the train's roof or windows, leaving countless animals to rot where they died. This overhunting was in part motivated by the U.S. government's desire to limit the range and power of indigenous plains Indians whose diets and cultures depended on the buffalo herds. The overhunting of the bison reduced their population to hundreds. Attempts to revive the American bison have been highly successful; farming has increased their population to nearly 150,000. The American bison is, therefore, no longer considered an endangered species, however, most of these animals are actually hybrids with domestic cattle and only two populations in Yellowstone National Park, USA and Elk Island National Park, Canada are genetically pure bison.As of July 2015, an estimated 4,900 bison lived in Yellowstone National Park, the largest U.S. bison population on public land. During 19831985 visitors experienced 33 bison-related injuries (range = 1013/year), so the park implemented education campaigns. After years of success, five injuries associated with bison encounters occurred in 2015, because visitors did not maintain the required distance of 75 ft (23 m) from bison while hiking or taking pictures.


Nutrition
Bison is an excellent source of complete protein and a rich source (20% or more of the Daily Value, DV) of multiple vitamins, including riboflavin, niacin, vitamin B6, and vitamin B12, and is also a rich source of minerals, including iron, phosphorus, and zinc. Additionally, bison is a good source (10% or more of the DV) of thiamine.


Livestock
The earliest plausible accounts of captive bison are those of the zoo at Tenochtitlan, the Aztec capital, which held an animal the Spaniards called "the Mexican bull". In 1552, Francisco Lopez de Gomara described Plains Indians herding and leading bison like cattle in his controversial book, Historia general de las Indias. Gomara, having never visited the Americas himself, likely misinterpreted early ethnographic accounts as the more familiar pastoralist relationship of the Old World. Today, bison are increasingly raised for meat, hides, wool, and dairy products. The majority of bison in the world are raised for human consumption or fur clothing. Bison meat is generally considered to taste very similar to beef, but is lower in fat and cholesterol, yet higher in protein than beef, which has led to the development of beefalo, a fertile hybrid of bison and domestic cattle. A market even exists for kosher bison meat; these bison are slaughtered at one of the few kosher mammal slaughterhouses in the U.S. and Canada, and the meat is then distributed worldwide.In America, the commercial industry for bison has been slow to develop despite individuals, such as Ted Turner, who have long marketed bison meat. In the 1990s, Turner found limited success with restaurants for high-quality cuts of meat, which include bison steaks and tenderloin. Lower-quality cuts suitable for hamburger and hot dogs have been described as "almost nonexistent". This created a marketing problem for commercial farming because the majority of usable meat, about 400 pounds for each bison, is suitable for these products. In 2003, the United States Department of Agriculture purchased $10 million worth of frozen overstock to save the industry, which would later recover through better use of consumer marketing. Restaurants have played a role in popularizing bison meat, like Ted's Montana Grill, which added bison to their menus. Ruby Tuesday first offered bison on their menus in 2005.In Canada, commercial bison farming began in the mid-1980s, concerning an unknown number of animals then. The first census of the bison occurred in 1996, which recorded 45,235 bison on 745 farms, and grew to 195,728 bison on 1,898 farms for the 2006 census.Several pet food companies use bison as a red meat alternative in dog foods. The companies producing these formulas include Natural Balance Pet Foods, Freshpet, the Blue Buffalo Company, Solid Gold, Canidae, and Taste of the Wild (made by Diamond Pet Foods, Inc., owned by Schell and Kampeter, Inc.).


A banana is an elongated, edible fruit  botanically a berry  produced by several kinds of large herbaceous flowering plants in the genus Musa. In some countries, bananas used for cooking may be called "plantains", distinguishing them from dessert bananas. The fruit is variable in size, color, and firmness, but is usually elongated and curved, with soft flesh rich in starch covered with a rind, which may be green, yellow, red, purple, or brown when ripe. The fruits grow in clusters hanging from the top of the plant. Almost all modern edible seedless (parthenocarp) bananas come from two wild species  Musa acuminata and Musa balbisiana. The scientific names of most cultivated bananas are Musa acuminata, Musa balbisiana, and Musa  paradisiaca for the hybrid Musa acuminata  M. balbisiana, depending on their genomic constitution. The old scientific name for this hybrid, Musa sapientum, is no longer used.
Musa species are native to tropical Indomalaya and Australia, and are likely to have been first domesticated in Papua New Guinea. They are grown in 135 countries, primarily for their fruit, and to a lesser extent to make fiber, banana wine, and banana beer and as ornamental plants. The world's largest producers of bananas in 2017 were India and China, which together accounted for approximately 38% of total production.Worldwide, there is no sharp distinction between "bananas" and "plantains". Especially in the Americas and Europe, "banana" usually refers to soft, sweet, dessert bananas, particularly those of the Cavendish group, which are the main exports from banana-growing countries. By contrast, Musa cultivars with firmer, starchier fruit are called "plantains". In other regions, such as Southeast Asia, many more kinds of banana are grown and eaten, so the binary distinction is not useful and is not made in local languages.
The term "banana" is also used as the common name for the plants that produce the fruit. This can extend to other members of the genus Musa, such as the scarlet banana (Musa coccinea), the pink banana (Musa velutina), and the Fe'i bananas. It can also refer to members of the genus Ensete, such as the snow banana (Ensete glaucum) and the economically important false banana (Ensete ventricosum). Both genera are in the banana family, Musaceae.


Description








The banana plant is the largest herbaceous flowering plant. All the above-ground parts of a banana plant grow from a structure usually called a "corm". Plants are normally tall and fairly sturdy, and are often mistaken for trees, but what appears to be a trunk is actually a "false stem" or pseudostem. Bananas grow in a wide variety of soils, as long as the soil is at least 60 centimetres (2.0 ft) deep, has good drainage and is not compacted. The leaves of banana plants are composed of a "stalk" (petiole) and a blade (lamina). The base of the petiole widens to form a sheath; the tightly packed sheaths make up the pseudostem, which is all that supports the plant. The edges of the sheath meet when it is first produced, making it tubular. As new growth occurs in the centre of the pseudostem the edges are forced apart. Cultivated banana plants vary in height depending on the variety and growing conditions. Most are around 5 m (16 ft) tall, with a range from 'Dwarf Cavendish' plants at around 3 m (10 ft) to 'Gros Michel' at 7 m (23 ft) or more. Leaves are spirally arranged and may grow 2.7 metres (8.9 ft) long and 60 cm (2.0 ft) wide. They are easily torn by the wind, resulting in the familiar frond look.When a banana plant is mature, the corm stops producing new leaves and begins to form a flower spike or inflorescence. A stem develops which grows up inside the pseudostem, carrying the immature inflorescence until eventually it emerges at the top. Each pseudostem normally produces a single inflorescence, also known as the "banana heart". (More are sometimes produced; an exceptional plant in the Philippines produced five.) After fruiting, the pseudostem dies, but offshoots will normally have developed from the base, so that the plant as a whole is perennial. In the plantation system of cultivation, only one of the offshoots will be allowed to develop in order to maintain spacing. The inflorescence contains many bracts (sometimes incorrectly referred to as petals) between rows of flowers. The female flowers (which can develop into fruit) appear in rows further up the stem (closer to the leaves) from the rows of male flowers. The ovary is inferior, meaning that the tiny petals and other flower parts appear at the tip of the ovary.The banana fruits develop from the banana heart, in a large hanging cluster, made up of tiers (called "hands"), with up to 20 fruit to a tier. The hanging cluster is known as a bunch, comprising 320 tiers, or commercially as a "banana stem", and can weigh 3050 kilograms (66110 lb). Individual banana fruits (commonly known as a banana or "finger") average 125 grams (4 12 oz), of which approximately 75% is water and 25% dry matter (nutrient table, lower right).
The fruit has been described as a "leathery berry". There is a protective outer layer (a peel or skin) with numerous long, thin strings (the phloem bundles), which run lengthwise between the skin and the edible inner portion. The inner part of the common yellow dessert variety can be split lengthwise into three sections that correspond to the inner portions of the three carpels by manually deforming the unopened fruit. In cultivated varieties, the seeds are diminished nearly to non-existence; their remnants are tiny black specks in the interior of the fruit.


Banana equivalent radiation dose
As with all living things on earth, potassium-containing bananas emit radioactivity at very low levels occurring naturally from potassium-40 (40K or K-40), which is one of several isotopes of potassium. The banana equivalent dose of radiation was developed in 1995 as a simple teaching-tool to educate the public about the natural, small amount of K-40 radiation occurring in every human and in common foods. The K-40 in a banana emits about 15 becquerels or 0.1 micro-sieverts (units of radioactivity exposure), an amount that does not add to the total body radiation dose when a banana is consumed.
This is because the radiation exposure from consuming one banana is only 1% of the average daily exposure to radiation, 50 times less than a typical dental x-ray and 400 times less than taking a commercial flight across the United States.


Etymology
The word banana is thought to be of West African origin, possibly from the Wolof word banaana, and passed into English via Spanish or Portuguese.


Taxonomy

The genus Musa was created by Carl Linnaeus in 1753. The name may be derived from Antonius Musa, physician to the Emperor Augustus, or Linnaeus may have adapted the Arabic word for banana, mauz. The old biological name Musa sapientum = "Muse of the wise" arose because of homophony in Latin with the classical Muses.
Musa is in the family Musaceae. The APG III system assigns Musaceae to the order Zingiberales, part of the commelinid clade of the monocotyledonous flowering plants. Some 70 species of Musa were recognized by the World Checklist of Selected Plant Families as of January 2013; several produce edible fruit, while others are cultivated as ornamentals.The classification of cultivated bananas has long been a problematic issue for taxonomists. Linnaeus originally placed bananas into two species based only on their uses as food: Musa sapientum for dessert bananas and Musa paradisiaca for plantains. More species names were added, but this approach proved to be inadequate for the number of cultivars in the primary center of diversity of the genus, Southeast Asia. Many of these cultivars were given names that were later discovered to be synonyms.In a series of papers published from 1947 onwards, Ernest Cheesman showed that Linnaeus's Musa sapientum and Musa paradisiaca were cultivars and descendants of two wild seed-producing species, Musa acuminata and Musa balbisiana, both first described by Luigi Aloysius Colla. Cheesman recommended the abolition of Linnaeus's species in favor of reclassifying bananas according to three morphologically distinct groups of cultivars  those primarily exhibiting the botanical characteristics of Musa balbisiana, those primarily exhibiting the botanical characteristics of Musa acuminata, and those with characteristics of both. Researchers Norman Simmonds and Ken Shepherd proposed a genome-based nomenclature system in 1955. This system eliminated almost all the difficulties and inconsistencies of the earlier classification of bananas based on assigning scientific names to cultivated varieties. Despite this, the original names are still recognized by some authorities, leading to confusion.The accepted scientific names for most groups of cultivated bananas are Musa acuminata Colla and Musa balbisiana Colla for the ancestral species, and Musa  paradisiaca L. for the hybrid M. acuminata  M. balbisiana.Synonyms of M.  paradisiaca include

many subspecific and varietal names of M.  paradisiaca, including M. p. subsp. sapientum (L.) Kuntze
Musa  dacca Horan.
Musa  sapidisiaca K.C.Jacob, nom. superfl.
Musa  sapientum L., and many of its varietal names, including M.  sapientum var. paradisiaca (L.) Baker, nom. illeg.Generally, modern classifications of banana cultivars follow Simmonds and Shepherd's system. Cultivars are placed in groups based on the number of chromosomes they have and which species they are derived from. Thus the Latundan banana is placed in the AAB Group, showing that it is a triploid derived from both M. acuminata (A) and M. balbisiana (B). For a list of the cultivars classified under this system, see "List of banana cultivars".
In 2012, a team of scientists announced they had achieved a draft sequence of the genome of Musa acuminata.


Bananas and plantains
In regions such as North America and Europe, Musa fruits offered for sale can be divided into "bananas" and "plantains", based on their intended use as food. Thus the banana producer and distributor Chiquita produces publicity material for the American market which says that "a plantain is not a banana". The stated differences are that plantains are more starchy and less sweet; they are eaten cooked rather than raw; they have thicker skin, which may be green, yellow or black; and they can be used at any stage of ripeness. Linnaeus made the same distinction between plantains and bananas when first naming two "species" of Musa. Members of the "plantain subgroup" of banana cultivars, most important as food in West Africa and Latin America, correspond to the Chiquita description, having long pointed fruit. They are described by Ploetz et al. as "true" plantains, distinct from other cooking bananas. The cooking bananas of East Africa belong to a different group, the East African Highland bananas, so would not qualify as "true" plantains on this definition.

An alternative approach divides bananas into dessert bananas and cooking bananas, with plantains being one of the subgroups of cooking bananas. Triploid cultivars derived solely from M. acuminata are examples of "dessert bananas", whereas triploid cultivars derived from the hybrid between M. acuminata and M. balbinosa (in particular the plantain subgroup of the AAB Group) are "plantains". Small farmers in Colombia grow a much wider range of cultivars than large commercial plantations. A study of these cultivars showed that they could be placed into at least three groups based on their characteristics: dessert bananas, non-plantain cooking bananas, and plantains, although there were overlaps between dessert and cooking bananas.In Southeast Asia  the center of diversity for bananas, both wild and cultivated  the distinction between "bananas" and "plantains" does not work, according to Valmayor et al. Many bananas are used both raw and cooked. There are starchy cooking bananas which are smaller than those eaten raw. The range of colors, sizes and shapes is far wider than in those grown or sold in Africa, Europe or the Americas. Southeast Asian languages do not make the distinction between "bananas" and "plantains" that is made in English (and Spanish). Thus both Cavendish cultivars, the classic yellow dessert bananas, and Saba cultivars, used mainly for cooking, are called pisang in Malaysia and Indonesia, kluai in Thailand and chuoi in Vietnam. Fe'i bananas, grown and eaten in the islands of the Pacific, are derived from entirely different wild species than traditional bananas and plantains. Most Fe'i bananas are cooked, but Karat bananas, which are short and squat with bright red skins, very different from the usual yellow dessert bananas, are eaten raw.In summary, in commerce in Europe and the Americas (although not in small-scale cultivation), it is possible to distinguish between "bananas", which are eaten raw, and "plantains", which are cooked. In other regions of the world, particularly India, Southeast Asia and the islands of the Pacific, there are many more kinds of banana and the two-fold distinction is not useful and not made in local languages. Plantains are one of many kinds of cooking bananas, which are not always distinct from dessert bananas.


Historical cultivation


Early cultivation

An article on Banana tree cultivation is brought down in Ibn al-'Awwam's 12th-century agricultural work, Book on Agriculture.The earliest domestication of bananas (Musa spp.) were initially from naturally occurring parthenocarpic (seedless) individuals of Musa acuminata banksii in New Guinea. These were cultivated by Papuans before the arrival of Austronesian-speakers. Numerous phytoliths of bananas have been recovered from the Kuk Swamp archaeological site and dated to around 10,000 to 6,500 BP. From New Guinea, cultivated bananas spread westward into Island Southeast Asia through proximity (not migrations). They hybridized with other (possibly independently domesticated) subspecies of Musa acuminata as well as Musa balbisiana in the Philippines, northern New Guinea, and possibly Halmahera. These hybridization events produced the triploid cultivars of bananas commonly grown today. From Island Southeast Asia, they became part of the staple crops of Austronesian peoples and were spread during their voyages and ancient maritime trading routes into Oceania, East Africa, South Asia, and Indochina.

These ancient introductions resulted in the banana subgroup now known as the "true" plantains, which include the East African Highland bananas and the Pacific plantains (the Iholena and Maoli-Popo'ulu subgroups). East African Highland bananas originated from banana populations introduced to Madagascar probably from the region between Java, Borneo, and New Guinea; while Pacific plantains were introduced to the Pacific Islands from either eastern New Guinea or the Bismarck Archipelago.Phytolith discoveries in Cameroon dating to the first millennium BCE triggered an as yet unresolved debate about the date of first cultivation in Africa. There is linguistic evidence that bananas were known in Madagascar around that time. The earliest prior evidence indicates that cultivation dates to no earlier than late 6th century CE. It is likely, however, that bananas were brought at least to Madagascar if not to the East African coast during the phase of Malagasy colonization of the island from South East Asia c. 400 CE.A second wave of introductions later spread bananas to other parts of tropical Asia, particularly Indochina and the Indian Subcontinent. However, there is evidence that bananas were known to the Indus Valley Civilisation from phytoliths recovered from the Kot Diji archaeological site in Pakistan (although they are absent in other contemporary sites in South Asia). This may be a possible indication of very early dispersal of bananas by Austronesian traders by sea from as early as 2000 BCE. But this is still putative, as they may have come from local wild Musa species used for fiber or as ornamentals, not food.Southeast Asia remains the region of primary diversity of the banana. Areas of secondary diversity are found in Africa, indicating a long history of banana cultivation in these regions.

The banana may also have been present in isolated locations elsewhere in the Middle East on the eve of Islam. The spread of Islam was followed by far-reaching diffusion. There are numerous references to it in Islamic texts (such as poems and hadiths) beginning in the 9th century. By the 10th century the banana appears in texts from Palestine and Egypt. From there it diffused into North Africa and Muslim Iberia. During the medieval ages, bananas from Granada were considered among the best in the Arab world. In 650, Islamic conquerors brought the banana to Palestine. Today, banana consumption increases significantly in Islamic countries during Ramadan, the month of daylight fasting.Bananas were certainly grown in the Christian Kingdom of Cyprus by the late medieval period. Writing in 1458, the Italian traveller and writer Gabriele Capodilista wrote favourably of the extensive farm produce of the estates at Episkopi, near modern-day Limassol, including the region's banana plantations.

Bananas were introduced to the Americas by Portuguese sailors who brought the fruits from West Africa in the 16th century.Many wild banana species as well as cultivars exist in extraordinary diversity in India, China, and Southeast Asia.

There are fuzzy bananas whose skins are bubblegum pink; green-and-white striped bananas with pulp the color of orange sherbet; bananas that, when cooked, taste like strawberries. The Double Mahoi plant can produce two bunches at once. The Chinese name of the aromatic Go San Heong banana means 'You can smell it from the next mountain.' The fingers on one banana plant grow fused; another produces bunches of a thousand fingers, each only an inch long.


Plantation cultivation in the Caribbean, Central and South America

In the 15th and 16th centuries, Portuguese colonists started banana plantations in the Atlantic Islands, Brazil, and western Africa. North Americans began consuming bananas on a small scale at very high prices shortly after the Civil War, though it was only in the 1880s that the food became more widespread. As late as the Victorian Era, bananas were not widely known in Europe, although they were available. Jules Verne introduces bananas to his readers with detailed descriptions in Around the World in Eighty Days (1872).
The earliest modern plantations originated in Jamaica and the related Western Caribbean Zone, including most of Central America. It involved the combination of modern transportation networks of steamships and railroads with the development of refrigeration that allowed more time between harvesting and ripening. North American shippers like Lorenzo Dow Baker and Andrew Preston, the founders of the Boston Fruit Company started this process in the 1870s, but railroad builders like Minor C. Keith also participated, eventually culminating in the multi-national giant corporations like today's Chiquita Brands International and Dole. These companies were monopolistic, vertically integrated (meaning they controlled growing, processing, shipping and marketing) and usually used political manipulation to build enclave economies (economies that were internally self-sufficient, virtually tax exempt, and export-oriented that contribute very little to the host economy). Their political maneuvers, which gave rise to the term Banana republic for states like Honduras and Guatemala, included working with local elites and their rivalries to influence politics or playing the international interests of the United States, especially during the Cold War, to keep the political climate favorable to their interests.


Peasant cultivation for export in the Caribbean

The vast majority of the world's bananas today are cultivated for family consumption or for sale on local markets. India is the world leader in this sort of production, but many other Asian and African countries where climate and soil conditions allow cultivation also host large populations of banana growers who sell at least some of their crop.Peasant sector banana growers produce for the world market in the Caribbean, however. The Windward Islands are notable for the growing, largely of Cavendish bananas, for an international market, generally in Europe but also in North America. In the Caribbean, and especially in Dominica where this sort of cultivation is widespread, holdings are in the 12 acre range. In many cases the farmer earns additional money from other crops, from engaging in labor outside the farm, and from a share of the earnings of relatives living overseas.Banana crops are vulnerable to destruction by high winds, such as tropical storms or cyclones.


Modern cultivation
All widely cultivated bananas today descend from the two wild bananas Musa acuminata and Musa balbisiana. While the original wild bananas contained large seeds, diploid or polyploid cultivars (some being hybrids) with tiny seeds or triploid hybrids without seeds are preferred for human raw fruit consumption. These are propagated asexually from offshoots. The plant is allowed to produce two shoots at a time; a larger one for immediate fruiting and a smaller "sucker" or "follower" to produce fruit in 68 months.
As a non-seasonal crop, bananas are available fresh year-round.


Cavendish

In global commerce in 2009, by far the most important cultivars belonged to the triploid AAA group of Musa acuminata, commonly referred to as Cavendish group bananas. They accounted for the majority of banana exports, despite only coming into existence in 1836. The cultivars Dwarf Cavendish and Grand Nain (Chiquita Banana) gained popularity in the 1950s after the previous mass-produced cultivar, Gros Michel (also an AAA group cultivar), became commercially unviable due to Panama disease, caused by the fungus Fusarium oxysporum which attacks the roots of the banana plant. Cavendish cultivars are resistant to the Panama disease, but in 2013 there were fears that the black sigatoka fungus would in turn make Cavendish bananas unviable.Even though it is no longer viable for large scale cultivation, Gros Michel is not extinct and is still grown in areas where Panama disease is not found. Likewise, Dwarf Cavendish and Grand Nain are in no danger of extinction, but they may leave supermarket shelves if disease makes it impossible to supply the global market. It is unclear if any existing cultivar can replace Cavendish bananas, so various hybridisation and genetic engineering programs are attempting to create a disease-resistant, mass-market banana.  One such strain that has emerged is the Taiwanese Cavendish, also known as the Formosana.


Ripening
Export bananas are picked green, and ripen in special rooms upon arrival in the destination country. These rooms are air-tight and filled with ethylene gas to induce ripening. The vivid yellow color consumers normally associate with supermarket bananas is, in fact, caused by the artificial ripening process. Flavor and texture are also affected by ripening temperature. Bananas are refrigerated to between 13.5 and 15 C (56.3 and 59.0 F) during transport. At lower temperatures, ripening permanently stalls, and the bananas turn gray as cell walls break down. The skin of ripe bananas quickly blackens in the 4 C (39 F) environment of a domestic refrigerator, although the fruit inside remains unaffected.

Bananas can be ordered by the retailer "ungassed" (i.e. not treated with ethylene), and may show up at the supermarket fully green. Guineos verdes (green bananas) that have not been gassed will never fully ripen before becoming rotten. Instead of fresh eating, these bananas can be used for cooking, as seen in Jamaican cuisine.A 2008 study reported that ripe bananas fluoresce when exposed to ultraviolet light. This property is attributed to the degradation of chlorophyll leading to the accumulation of a fluorescent product in the skin of the fruit. The chlorophyll breakdown product is stabilized by a propionate ester group. Banana-plant leaves also fluoresce in the same way. Green (under-ripe) bananas do not fluoresce. That paper suggested that this fluorescence could be put to use "for optical in vivo monitoring of ripening and over-ripening of bananas and other fruit."


Storage and transport
Bananas must be transported over long distances from the tropics to world markets. To obtain maximum shelf life, harvest comes before the fruit is mature. The fruit requires careful handling, rapid transport to ports, cooling, and refrigerated shipping. The goal is to prevent the bananas from producing their natural ripening agent, ethylene. This technology allows storage and transport for 34 weeks at 13 C (55 F). On arrival, bananas are held at about 17 C (63 F) and treated with a low concentration of ethylene. After a few days, the fruit begins to ripen and is distributed for final sale. Ripe bananas can be held for a few days at home. If bananas are too green, they can be put in a brown paper bag with an apple or tomato overnight to speed up the ripening process.Carbon dioxide (which bananas produce) and ethylene absorbents extend fruit life even at high temperatures. This effect can be exploited by packing banana in a polyethylene bag and including an ethylene absorbent, e.g., potassium permanganate, on an inert carrier. The bag is then sealed with a band or string. This treatment has been shown to more than double lifespans up to 34 weeks without the need for refrigeration.


Sustainability
The excessive use of fertilizers often left in abandoned plantations contributes greatly to eutrophication in local streams and lakes, and harms aquatic life after algal blooms deprive fish of oxygen. It has been theorized that destruction of 60% of coral reefs along the coasts of Costa Rica is partially from sediments from banana plantations. Another issue is the deforestation associated with expanding banana production. As monocultures rapidly deplete soil nutrients plantations expand to areas with rich soils and cut down forests, which also affects soil erosion and degradation, and increases frequency of flooding. The World Wildlife Fund (WWF) stated that banana production produced more waste than any other agricultural sector, mostly from discarded banana plants, bags used to cover the bananas, strings to tie them, and containers for transport.Voluntary sustainability standards such as Rainforest Alliance and Fairtrade are increasingly being used to address some of these issues. Bananas production certified by such sustainability standards experienced a 43% compound annual growth rate from 2008 to 2016, to represent 36% of banana exports.


Production and export

In 2017, world production of bananas and plantains combined was 153 million tonnes, led by India and China with a combined total of 27% of global production. Other major producers were the Philippines, Colombia, Indonesia, Ecuador, and  Brazil.
As reported for 2013, total world exports were 20 million tonnes of bananas and 859,000 tonnes of plantains. Ecuador and the Philippines were the leading exporters with 5.4 and 3.3 million tonnes, respectively, and the Dominican Republic was the leading exporter of plantains with 210,350 tonnes.


Developing countries
Bananas and plantains constitute a major staple food crop for millions of people in developing countries. In many tropical countries, green (unripe) bananas used for cooking represent the main cultivars. Most producers are small-scale farmers either for home consumption or local markets. Because bananas and plantains produce fruit year-round, they provide a valuable food source during the hunger season (when the food from one annual/semi-annual harvest has been consumed, and the next is still to come). Bananas and plantains are important for global food security.


Pests, diseases, and natural disasters

While in no danger of outright extinction, the most common edible banana cultivar Cavendish (extremely popular in Europe and the Americas) could become unviable for large-scale cultivation in the next 1020 years. Its predecessor 'Gros Michel', discovered in the 1820s, suffered this fate. Like almost all bananas, Cavendish lacks genetic diversity, which makes it vulnerable to diseases, threatening both commercial cultivation and small-scale subsistence farming. Some commentators remarked that those variants which could replace what much of the world considers a "typical banana" are so different that most people would not consider them the same fruit, and blame the decline of the banana on monogenetic cultivation driven by short-term commercial motives.


Panama disease
Panama disease is caused by a fusarium soil fungus (Race 1), which enters the plants through the roots and travels with water into the trunk and leaves, producing gels and gums that cut off the flow of water and nutrients, causing the plant to wilt, and exposing the rest of the plant to lethal amounts of sunlight. Prior to 1960, almost all commercial banana production centered on "Gros Michel", which was highly susceptible. Cavendish was chosen as the replacement for Gros Michel because, among resistant cultivars, it produces the highest quality fruit. However, more care is required for shipping the Cavendish, and its quality compared to Gros Michel is debated.According to current sources, a deadly form of Panama disease is infecting Cavendish. All plants are genetically identical, which prevents evolution of disease resistance. Researchers are examining hundreds of wild varieties for resistance.


Tropical race 4
Tropical race 4 (TR4), a reinvigorated strain of Panama disease, was first discovered in 1993. This virulent form of fusarium wilt destroyed Cavendish in several southeast Asian countries and spread to Australia and India. As the soil-based fungi can easily be carried on boots, clothing, or tools, the wilt spread to the Americas despite years of preventive efforts. Cavendish is highly susceptible to TR4, and over time, Cavendish is endangered for commercial production by this disease. The only known defense to TR4 is genetic resistance. This is conferred either by RGA2, a gene isolated from a TR4-resistant diploid banana, or by the nematode-derived Ced9. Experts state the need to enrich banana biodiversity by producing diverse new banana varieties, not just having a focus on the Cavendish.


Black sigatoka
Black sigatoka is a fungal leaf spot disease first observed in Fiji in 1963 or 1964. Black Sigatoka (also known as black leaf streak) has spread to banana plantations throughout the tropics from infected banana leaves that were used as packing material. It affects all main cultivars of bananas and plantains (including the Cavendish cultivars), impeding photosynthesis by blackening parts of the leaves, eventually killing the entire leaf. Starved for energy, fruit production falls by 50% or more, and the bananas that do grow ripen prematurely, making them unsuitable for export. The fungus has shown ever-increasing resistance to treatment, with the current expense for treating 1 hectare (2.5 acres) exceeding $1,000 per year. In addition to the expense, there is the question of how long intensive spraying can be environmentally justified.


Banana bunchy top virus
Banana bunchy top virus (BBTV) is a plant virus of the genus Babuvirus, family Nanonviridae affecting Musa spp. (including banana, abaca, plantain and ornamental bananas) and Ensete spp. in the family Musaceae. Banana bunchy top disease (BBTD) symptoms include dark green streaks of variable length in leaf veins, midribs and petioles. Leaves become short and stunted as the disease progresses, becoming 'bunched' at the apex of the plant. Infected plants may produce no fruit or the bunch may not emerge from the pseudostem. The virus is transmitted by the banana aphid Pentalonia nigronervosa and is widespread in SE Asia, Asia, the Philippines, Taiwan, Oceania and parts of Africa. There is no cure for BBTD, but it can be effectively controlled by the eradication of diseased plants and the use of virus-free planting material. No resistant cultivars have been found, but varietal differences in susceptibility have been reported. The commercially important Cavendish subgroup is severely affected.


Banana bacterial wilt
Banana bacterial wilt (BBW) is a bacterial disease caused by Xanthomonas campestris pv. musacearum. After being originally identified on a close relative of bananas, Ensete ventricosum, in Ethiopia in the 1960s, BBW occurred in Uganda in 2001 affecting all banana cultivars. Since then BBW has been diagnosed in Central and East Africa including the banana growing regions of Rwanda, the Democratic Republic of the Congo, Tanzania, Kenya, Burundi, and Uganda.


Conservation
Given the narrow range of genetic diversity present in bananas and the many threats via biotic (pests and diseases) and abiotic (such as drought) stress, conservation of the full spectrum of banana genetic resources is ongoing. Banana germplasm is conserved in many national and regional gene banks, and at the world's largest banana collection, the International Musa Germplasm Transit Centre (ITC), managed by Bioversity International and hosted at KU Leuven in Belgium. Musa cultivars are usually seedless, and options for their long-term conservation are constrained by the vegetative nature of the plant's reproductive system. Consequently, they are conserved by three main methods: in vivo (planted in field collections), in vitro (as plantlets in test tubes within a controlled environment), and by cryopreservation (meristems conserved in liquid nitrogen at 196 C). Genes from wild banana species are conserved as DNA and as cryopreserved pollen and banana seeds from wild species are also conserved, although less commonly, as they are difficult to regenerate. In addition, bananas and their crop wild relatives are conserved in situ (in wild natural habitats where they evolved and continue to do so). Diversity is also conserved in farmers' fields where continuous cultivation, adaptation and improvement of cultivars is often carried out by small-scale farmers growing traditional local cultivars.


Nutrition
Raw bananas (not including the peel) are 75% water, 23% carbohydrates, 1% protein, and contain negligible fat. A 100-gram reference serving supplies 89 Calories, 31% of the US recommended Daily Value (DV) of vitamin B6, and moderate amounts of vitamin C, manganese and dietary fiber, with no other micronutrients in significant content (see table).


Potassium
Although bananas are commonly thought to contain exceptional potassium content, their actual potassium content is not high per typical food serving, having only 8% of the US recommended Daily Value for potassium (considered a low level of the DV, see nutrition table), and their potassium-content ranking among fruits, vegetables, legumes, and many other foods is relatively moderate. Vegetables with higher potassium content than raw dessert bananas (358 mg per 100 g) include raw spinach (558 mg per 100 g), baked potatoes without skin (391 mg per 100 g), cooked soybeans (539 mg per 100 g), grilled portabella mushrooms (437 mg per 100 g), and processed tomato sauces (413439 mg per 100 g). Raw plantains contain 499 mg potassium per 100 g. Dehydrated dessert bananas or banana powder contain 1491 mg potassium per 100 g.


Allergen
Individuals with a latex allergy may experience a reaction to bananas.


Culture


Food and cooking


Fruit
Bananas are a staple starch for many tropical populations. Depending upon cultivar and ripeness, the flesh can vary in taste from starchy to sweet, and texture from firm to mushy. Both the skin and inner part can be eaten raw or cooked. The primary component of the aroma of fresh bananas is isoamyl acetate (also known as banana oil), which, along with several other compounds such as butyl acetate and isobutyl acetate, is a significant contributor to banana flavor.During the ripening process, bananas produce the gas ethylene, which acts as a plant hormone and indirectly affects the flavor. Among other things, ethylene stimulates the formation of amylase, an enzyme that breaks down starch into sugar, influencing the taste of bananas. The greener, less ripe bananas contain higher levels of starch and, consequently, have a "starchier" taste. On the other hand, yellow bananas taste sweeter due to higher sugar concentrations. Furthermore, ethylene signals the production of pectinase, an enzyme which breaks down the pectin between the cells of the banana, causing the banana to soften as it ripens.Bananas are eaten deep fried, baked in their skin in a split bamboo, or steamed in glutinous rice wrapped in a banana leaf. Bananas can be made into fruit preserves. Banana pancakes are popular among travelers in South Asia and Southeast Asia. This has elicited the expression Banana Pancake Trail for those places in Asia that cater to these travelers. Banana chips are a snack produced from sliced dehydrated or fried banana or plantain, which have a dark brown color and an intense banana taste. Dried bananas are also ground to make banana flour. Extracting juice is difficult, because when a banana is compressed, it simply turns to pulp. Bananas feature prominently in Philippine cuisine, being part of traditional dishes and desserts like maruya, turn, and halo-halo or saba con yelo. Most of these dishes use the Saba Banana or Cardaba banana cultivar. Bananas are also commonly used in cuisine in the South-Indian state of Kerala, where they are steamed (puzhungiyathu), made into curries, fried into chips, (upperi) or fried in batter (pazhampori). Pisang goreng, bananas fried with batter similar to the Filipino maruya or Kerala pazhampori, is a popular dessert in Malaysia, Singapore, and Indonesia. A similar dish is known in the United Kingdom and United States as banana fritters.
Plantains are used in various stews and curries or cooked, baked or mashed in much the same way as potatoes, such as the pazham pachadi dish prepared in Kerala.


Flower

Banana hearts are used as a vegetable in South Asian and Southeast Asian cuisine, either raw or steamed with dips or cooked in soups, curries and fried foods. The flavor resembles that of artichoke. As with artichokes, both the fleshy part of the bracts and the heart are edible.


Leaves

Banana leaves are large, flexible, and waterproof. They are often used as ecologically friendly disposable food containers or as "plates" in South Asia and several Southeast Asian countries. In Indonesian cuisine, banana leaf is employed in cooking methods like pepes and botok; banana leaf packages containing food ingredients and spices are cooked in steam or in boiled water, or are grilled on charcoal. When used so for steaming or grilling, the banana leaves protect the food ingredients from burning and add a subtle sweet flavor. In South India, it is customary to serve traditional food on a banana leaf. In Tamil Nadu (India), dried banana leaves are used as to pack food and to make cups to hold liquid food items.


Trunk

The tender core of the banana plant's trunk is also used in South Asian and Southeast Asian cuisine. Examples include the Burmese dish mohinga, and the Filipino dishes inubaran and kadyos, manok, kag ubad.


Fiber


Textiles
Banana fiber harvested from the pseudostems and leaves of the plant has been used for textiles in Asia since at least the 13th century. Both fruit-bearing and fibrous varieties of the banana plant have been used. In the Japanese system Kijka-bashfu, leaves and shoots are cut from the plant periodically to ensure softness. Harvested shoots are first boiled in lye to prepare fibers for yarn-making. These banana shoots produce fibers of varying degrees of softness, yielding yarns and textiles with differing qualities for specific uses. For example, the outermost fibers of the shoots are the coarsest, and are suitable for tablecloths, while the softest innermost fibers are desirable for kimono and kamishimo. This traditional Japanese cloth-making process requires many steps, all performed by hand.In India, a banana fiber separator machine has been developed, which takes the agricultural waste of local banana harvests and extracts strands of the fiber.


Paper

Banana fiber is used in the production of banana paper. Banana paper is made from two different parts: the bark of the banana plant, mainly used for artistic purposes, or from the fibers of the stem and non-usable fruits. The paper is either hand-made or by industrial process.


Cultural roles


Arts
The song "Yes! We Have No Bananas" was written by Frank Silver and Irving Cohn and originally released in 1923; for many decades, it was the best-selling sheet music in history. Since then the song has been rerecorded several times and has been particularly popular during banana shortages.
A person slipping on a banana peel has been a staple of physical comedy for generations. An American comedy recording from 1910 features a popular character of the time, "Uncle Josh", claiming to describe his own such incident:Now I don't think much of the man that throws a banana peelin' on the sidewalk, and I don't think much of the banana peel that throws a man on the sidewalk neither ... my foot hit the bananer peelin' and I went up in the air, and I come down ker-plunk, jist as I was pickin' myself up a little boy come runnin' across the street ... he says, "Oh mister, won't you please do that agin? My little brother didn't see you do it."The poet Bash is named after the Japanese word for a banana plant. The "bash" planted in his garden by a grateful student became a source of inspiration to his poetry, as well as a symbol of his life and home.
The cover artwork for the debut album of The Velvet Underground features a banana made by Andy Warhol. On the original vinyl LP version, the design allowed the listener to "peel" this banana to find a pink, peeled phallic banana on the inside.
Italian artist Maurizio Cattelan created a concept art piece titled Comedian involving taping a banana to a wall using silver duct tape. The piece was exhibited briefly at the Art Basel in Miami before being removed from the exhibition and eaten sans permission in another artistic stunt titled Hungry Artist by New York artist David Datuna.


Religion and popular beliefs

In India, bananas serve a prominent part in many festivals and occasions of Hindus. In South Indian weddings, particularly Tamil weddings, banana trees are tied in pairs to form an arch as a blessing to the couple for a long-lasting, useful life.In Thailand, it is believed that a certain type of banana plant may be inhabited by a spirit, Nang Tani, a type of ghost related to trees and similar plants that manifests itself as a young woman. Often people tie a length of colored satin cloth around the pseudostem of the banana plants.In Malay folklore, the ghost known as Pontianak is associated with banana plants (pokok pisang), and its spirit is said to reside in them during the day.


Racist symbol
There is a long racist history of describing people of African descent as being more like monkeys than humans, and due to the assumption in popular culture that monkeys like bananas, bananas have been used in symbolic acts of hate speech.Particularly in Europe, bananas have long been commonly thrown at black footballers by racist spectators. In April 2014, during a match at Villarreal's stadium, El Madrigal, Dani Alves was targeted by Villareal supporter David Campaya Lleo, who threw a banana at him. Alves picked up the banana, peeled it and took a bite, and the meme went viral on social media in support of him.  Racist taunts are an ongoing problem in football.   Bananas were hung from nooses around the campus of American University in May 2017 after the student body elected its first black woman student government president."Banana" is also a slur aimed at some asian people, that are said to be "yellow on the outside, white on the inside". Used primarily by East or Southeast Asians for other East/Southeast Asians or Asian Americans who are perceived as assimilated into mainstream American culture.


Unicode
The Unicode standard includes the emoji character U+1F34C  BANANA (HTML &#127820;).


Other uses

In internet culture, bananas are sometimes included in images as a reference for the size of other objects in the image. This use, often accompanied with the text "banana for scale", became an internet meme.
The large leaves may be used as umbrellas.
Banana peel may have capability to extract heavy metal contamination from river water, similar to other purification materials.  In 2007, banana peel powder was tested as a means of filtration for heavy metals and radionuclides occurring in water produced by the nuclear and fertilizer industries (cadmium contaminant is present in phosphates). When added and thoroughly mixed for 40 minutes, the powder can remove roughly 65% of heavy metals, and this can be repeated.
Waste bananas can be used to feed livestock.


An aurora (plural: auroras or aurorae), sometimes referred to as polar lights (aurora polaris), northern lights (aurora borealis), or southern lights (aurora australis), is a natural light display in the Earth's sky, predominantly seen in high-latitude regions (around the Arctic and Antarctic).
Auroras are the result of disturbances in the magnetosphere caused by solar wind. These disturbances are sometimes strong enough to alter the trajectories of charged particles in both solar wind and magnetospheric plasma. These particles, mainly electrons and protons, precipitate into the upper atmosphere (thermosphere/exosphere).
The resulting ionization and excitation of atmospheric constituents emit light of varying color and complexity. The form of the aurora, occurring within bands around both polar regions, is also dependent on the amount of acceleration imparted to the precipitating particles. Precipitating protons generally produce optical emissions as incident hydrogen atoms after gaining electrons from the atmosphere. Proton auroras are usually observed at lower latitudes.Most of the planets in our solar system, some natural satellites, brown dwarfs, and even comets also host auroras.


Etymology
The word "aurora" is derived from the name of the Roman goddess of the dawn, Aurora, who travelled from east to west announcing the coming of the sun. Ancient Greek poets used the name metaphorically to refer to dawn, often mentioning its play of colours across the otherwise dark sky (e.g., "rosy-fingered dawn"). 


Occurrence
Most auroras occur in a band known as the "auroral zone", which is typically 3 to 6 wide in latitude and between 10 and 20 from the geomagnetic poles at all local times (or longitudes), most clearly seen at night against a dark sky. A region that currently displays an aurora is called the "auroral oval", a band displaced by the solar wind towards the night side of the Earth. Early evidence for a geomagnetic connection comes from the statistics of auroral observations. Elias Loomis (1860), and later Hermann Fritz (1881) and Sophus Tromholt (1881) in more detail, established that the aurora appeared mainly in the auroral zone. Day-to-day positions of the auroral ovals are posted on the Internet.In northern latitudes, the effect is known as the aurora borealis or the northern lights. The former term was coined by Galileo in 1619, from the Roman goddess of the dawn and the Greek name for the north wind. The southern counterpart, the aurora australis or the southern lights, has features almost identical to the aurora borealis and changes simultaneously with changes in the northern auroral zone. The aurora australis is visible from high southern latitudes in Antarctica, Chile, Argentina, New Zealand, and Australia.
A geomagnetic storm causes the auroral ovals (north and south) to expand, bringing the aurora to lower latitudes. The instantaneous distribution of auroras ("auroral oval") is slightly different, being centered about 35 nightward of the magnetic pole, so that auroral arcs reach furthest toward the equator when the magnetic pole in question is in between the observer and the Sun. The aurora can be seen best at this time, which is called magnetic midnight.
Auroras seen within the auroral oval may be directly overhead, but from farther away, they illuminate the poleward horizon as a greenish glow, or sometimes a faint red, as if the Sun were rising from an unusual direction. Auroras also occur poleward of the auroral zone as either diffuse patches or arcs, which can be subvisual.

Auroras are occasionally seen in latitudes below the auroral zone, when a geomagnetic storm temporarily enlarges the auroral oval. Large geomagnetic storms are most common during the peak of the 11-year sunspot cycle or during the three years after the peak.
An electron spirals (gyrates) about a field line at an angle that is determined by its velocity vectors, parallel and perpendicular, respectively, to the local geomagnetic field vector B. This angle is known as the "pitch angle" of the particle. The distance, or radius, of the electron from the field line at any time is known as its Larmor radius. The pitch angle increases as the electron travels to a region of greater field strength nearer to the atmosphere. Thus, it is possible for some particles to return, or mirror, if the angle becomes 90 before entering the atmosphere to collide with the denser molecules there. Other particles that do not mirror enter the atmosphere and contribute to the auroral display over a range of altitudes.
Other types of auroras have been observed from space, e.g."poleward arcs" stretching sunward across the polar cap, the related "theta aurora", and "dayside arcs" near noon. These are relatively infrequent and poorly understood. Other interesting effects occur such as flickering aurora, "black aurora" and subvisual red arcs. In addition to all these, a weak glow (often deep red) observed around the two polar cusps, the field lines separating the ones that close through the Earth from those that are swept into the tail and close remotely.


Images

The altitudes where auroral emissions occur were revealed by Carl Strmer and his colleagues, who used cameras to triangulate more than 12,000 auroras. They discovered that most of the light is produced between 90 and 150 km above the ground, while extending at times to more than 1000 km.
Images of auroras are significantly more common today than in the past due to the increase in the use of digital cameras that have high enough sensitivities. Film and digital exposure to auroral displays is fraught with difficulties. Due to the different color spectra present, and the temporal changes occurring during the exposure, the results are somewhat unpredictable. Different layers of the film emulsion respond differently to lower light levels, and choice of a film can be very important. Longer exposures superimpose rapidly changing features, and often blanket the dynamic attribute of a display. Higher sensitivity creates issues with graininess.
David Malin pioneered multiple exposure using multiple filters for astronomical photography, recombining the images in the laboratory to recreate the visual display more accurately. For scientific research, proxies are often used, such as ultraviolet, and color-correction to simulate the appearance to humans. Predictive techniques are also used, to indicate the extent of the display, a highly useful tool for aurora hunters. Terrestrial features often find their way into aurora images, making them more accessible and more likely to be published by major websites. Excellent images are possible with standard film (using ISO ratings between 100 and 400) and a single-lens reflex camera with full aperture, a fast lens (f1.4 50 mm, for example), and exposures between 10 and 30 seconds, depending on the aurora's brightness.Early work on the imaging of the auroras was done in 1949 by the University of Saskatchewan using the SCR-270 radar.





Forms of auroras
According to Clark (2007), there are four main forms that can be seen from the ground, from least to most visible:

A mild glow, near the horizon. These can be close to the limit of visibility, but can be distinguished from moonlit clouds because stars can be seen undiminished through the glow.
Patches or surfaces that look like clouds.
Arcs curve across the sky.
Rays are light and dark stripes across arcs, reaching upwards by various amounts.
Coronas cover much of the sky and diverge from one point on it.Brekke (1994) also described some auroras as curtains. The similarity to curtains is often enhanced by folds within the arcs. Arcs can fragment or break up into separate, at times rapidly changing, often rayed features that may fill the whole sky. These are also known as discrete auroras, which are at times bright enough to read a newspaper by at night.These forms are consistent with auroras' being shaped by Earth's magnetic field. The appearances of arcs, rays, curtains, and coronas are determined by the shapes of the luminous parts of the atmosphere and a viewer's position.


Colors and wavelengths of auroral light
Red: At its highest altitudes, excited atomic oxygen emits at 630 nm (red); low concentration of atoms and lower sensitivity of eyes at this wavelength make this color visible only under more intense solar activity. The low number of oxygen atoms and their gradually diminishing concentration is responsible for the faint appearance of the top parts of the "curtains". Scarlet, crimson, and carmine are the most often-seen hues of red for the auroras.
Green: At lower altitudes, the more frequent collisions suppress the 630 nm (red) mode: rather the 557.7 nm emission (green) dominates. A fairly high concentration of atomic oxygen and higher eye sensitivity in green make green auroras the most common. The excited molecular nitrogen (atomic nitrogen being rare due to the high stability of the N2 molecule) plays a role here, as it can transfer energy by collision to an oxygen atom, which then radiates it away at the green wavelength. (Red and green can also mix together to produce pink or yellow hues.) The rapid decrease of concentration of atomic oxygen below about 100 km is responsible for the abrupt-looking end of the lower edges of the curtains. Both the 557.7 and 630.0 nm wavelengths correspond to forbidden transitions of atomic oxygen, a slow mechanism responsible for the graduality (0.7 s and 107 s respectively) of flaring and fading.
Blue: At yet lower altitudes, atomic oxygen is uncommon, and molecular nitrogen and ionized molecular nitrogen take over in producing visible light emission, radiating at a large number of wavelengths in both red and blue parts of the spectrum, with 428 nm (blue) being dominant. Blue and purple emissions, typically at the lower edges of the "curtains", show up at the highest levels of solar activity. The molecular nitrogen transitions are much faster than the atomic oxygen ones.
Ultraviolet: Ultraviolet radiation from auroras (within the optical window but not visible to virtually all humans) has been observed with the requisite equipment. Ultraviolet auroras have also been seen on Mars, Jupiter and Saturn.
Infrared: Infrared radiation, in wavelengths that are within the optical window, is also part of many auroras.
Yellow and pink are a mix of red and green or blue. Other shades of red, as well as orange, may be seen on rare occasions; yellow-green is moderately common. As red, green, and blue are the primary colors of additive synthesis of colors, in theory, practically any color might be possible, but the ones mentioned in this article comprise a virtually exhaustive list.


Changes with time
Auroras change with time. Over the night, they begin with glows and progress towards coronas, although they may not reach them. They tend to fade in the opposite order.At shorter time scales, auroras can change their appearances and intensity, sometimes so slowly as to be difficult to notice, and at other times rapidly down to the sub-second scale. The phenomenon of pulsating auroras is an example of intensity variations over short timescales, typically with periods of 220 seconds. This type of aurora is generally accompanied by decreasing peak emission heights of about 8 km for blue and green emissions and above average solar wind speeds (~ 500 km/s).


Other auroral radiation
In addition, the aurora and associated currents produce a strong radio emission around 150 kHz known as auroral kilometric radiation (AKR), discovered in 1972. Ionospheric absorption makes AKR only observable from space. X-ray emissions, originating from the particles associated with auroras, have also been detected.


Aurora noise
Aurora noise, similar to a hissing, or crackling noise, begins about 70 m (230 ft) above the Earth's surface and is caused by charged particles in an inversion layer of the atmosphere formed during a cold night. The charged particles discharge when particles from the Sun hit the inversion layer, creating the noise.


Atypical auroras


STEVE
In 2016, more than fifty citizen science observations described what was to them an unknown type of aurora which they named "STEVE," for "Strong Thermal Emission Velocity Enhancement." STEVE is not an aurora but is caused by a 25 km (16 mi) wide ribbon of hot plasma at an altitude of 450 km (280 mi), with a temperature of 6,000 K (5,730 C; 10,340 F) and flowing at a speed of 6 km/s (3.7 mi/s) (compared to 10 m/s (33 ft/s) outside the ribbon).


Picket-fence aurora
The processes that cause STEVE also are associated with a picket-fence aurora, although the latter can be seen without STEVE. It is an aurora because it is caused by precipitation of electrons in the atmosphere but it appears outside the auroral oval, closer to the equator than typical auroras. When the picket-fence aurora appears with STEVE, it is below.


Causes
A full understanding of the physical processes which lead to different types of auroras is still incomplete, but the basic cause involves the interaction of the solar wind with the Earth's magnetosphere. The varying intensity of the solar wind produces effects of different magnitudes but includes one or more of the following physical scenarios.

A quiescent solar wind flowing past the Earth's magnetosphere steadily interacts with it and can both inject solar wind particles directly onto the geomagnetic field lines that are 'open', as opposed to being 'closed' in the opposite hemisphere, and provide diffusion through the bow shock. It can also cause particles already trapped in the radiation belts to precipitate into the atmosphere. Once particles are lost to the atmosphere from the radiation belts, under quiet conditions, new ones replace them only slowly, and the loss-cone becomes depleted. In the magnetotail, however, particle trajectories seem constantly to reshuffle, probably when the particles cross the very weak magnetic field near the equator. As a result, the flow of electrons in that region is nearly the same in all directions ("isotropic") and assures a steady supply of leaking electrons. The leakage of electrons does not leave the tail positively charged, because each leaked electron lost to the atmosphere is replaced by a low energy electron drawn upward from the ionosphere. Such replacement of "hot" electrons by "cold" ones is in complete accord with the 2nd law of thermodynamics. The complete process, which also generates an electric ring current around the Earth, is uncertain.
Geomagnetic disturbance from an enhanced solar wind causes distortions of the magnetotail ("magnetic substorms"). These 'substorms' tend to occur after prolonged spells(hours) during which the interplanetary magnetic field has had an appreciable southward component. This leads to a higher rate of interconnection between its field lines and those of Earth. As a result, the solar wind moves magnetic flux (tubes of magnetic field lines, 'locked' together with their resident plasma) from the day side of Earth to the magnetotail, widening the obstacle it presents to the solar wind flow and constricting the tail on the night-side. Ultimately some tail plasma can separate ("magnetic reconnection"); some blobs ("plasmoids") are squeezed downstream and are carried away with the solar wind; others are squeezed toward Earth where their motion feeds strong outbursts of auroras, mainly around midnight ("unloading process"). A geomagnetic storm resulting from greater interaction adds many more particles to the plasma trapped around Earth, also producing enhancement of the "ring current". Occasionally the resulting modification of the Earth's magnetic field can be so strong that it produces auroras visible at middle latitudes, on field lines much closer to the equator than those of the auroral zone.

Acceleration of auroral charged particles invariably accompanies a magnetospheric disturbance that causes an aurora. This mechanism, which is believed to predominantly arise from strong electric fields along the magnetic field or wave-particle interactions, raises the velocity of a particle in the direction of the guiding magnetic field. The pitch angle is thereby decreased and increases the chance of it being precipitated into the atmosphere. Both electromagnetic and electrostatic waves, produced at the time of greater geomagnetic disturbances, make a significant contribution to the energizing processes that sustain an aurora. Particle acceleration provides a complex intermediate process for transferring energy from the solar wind indirectly into the atmosphere.
The details of these phenomena are not fully understood. However, it is clear that the prime source of auroral particles is the solar wind feeding the magnetosphere, the reservoir containing the radiation zones and temporarily magnetically-trapped particles confined by the geomagnetic field, coupled with particle acceleration processes.


Auroral particles
The immediate cause of the ionization and excitation of atmospheric constituents leading to auroral emissions was discovered in 1960, when a pioneering rocket flight from Fort Churchill in Canada revealed a flux of electrons entering the atmosphere from above. Since then an extensive collection of measurements has been acquired painstakingly and with steadily improving resolution since the 1960s by many research teams using rockets and satellites to traverse the auroral zone. The main findings have been that auroral arcs and other bright forms are due to electrons that have been accelerated during the final few 10,000 km or so of their plunge into the atmosphere. These electrons often, but not always, exhibit a peak in their energy distribution, and are preferentially aligned along the local direction of the magnetic field.
Electrons mainly responsible for diffuse and pulsating auroras have, in contrast, a smoothly falling energy distribution, and an angular (pitch-angle) distribution favouring directions perpendicular to the local magnetic field. Pulsations were discovered to originate at or close to the equatorial crossing point of auroral zone magnetic field lines. Protons are also associated with auroras, both discrete and diffuse.


Auroras and the atmosphere
Auroras result from emissions of photons in the Earth's upper atmosphere, above 80 km (50 mi), from ionized nitrogen atoms regaining an electron, and oxygen atoms and nitrogen based molecules returning from an excited state to ground state. They are ionized or excited by the collision of particles precipitated into the atmosphere. Both incoming electrons and protons may be involved. Excitation energy is lost within the atmosphere by the emission of a photon, or by collision with another atom or molecule:

oxygen emissions
green or orange-red, depending on the amount of energy absorbed.
nitrogen emissions
blue, purple or red; blue and purple if the molecule regains an electron after it has been ionized, red if returning to ground state from an excited state.Oxygen is unusual in terms of its return to ground state: it can take 0.7 seconds to emit the 557.7 nm green light and up to two minutes for the red 630.0 nm emission. Collisions with other atoms or molecules absorb the excitation energy and prevent emission, this process is called collisional quenching. Because the highest parts of the atmosphere contain a higher percentage of oxygen and lower particle densities, such collisions are rare enough to allow time for oxygen to emit red light. Collisions become more frequent progressing down into the atmosphere due to increasing density, so that red emissions do not have time to happen, and eventually, even green light emissions are prevented.
This is why there is a color differential with altitude; at high altitudes oxygen red dominates, then oxygen green and nitrogen blue/purple/red, then finally nitrogen blue/purple/red when collisions prevent oxygen from emitting anything. Green is the most common color. Then comes pink, a mixture of light green and red, followed by pure red, then yellow (a mixture of red and green), and finally, pure blue.


Auroras and the ionosphere
Bright auroras are generally associated with Birkeland currents (Schield et al., 1969; Zmuda and Armstrong, 1973), which flow down into the ionosphere on one side of the pole and out on the other. In between, some of the current connects directly through the ionospheric E layer (125 km); the rest ("region 2") detours, leaving again through field lines closer to the equator and closing through the "partial ring current" carried by magnetically trapped plasma. The ionosphere is an ohmic conductor, so some consider that such currents require a driving voltage, which an, as yet unspecified, dynamo mechanism can supply. Electric field probes in orbit above the polar cap suggest voltages of the order of 40,000 volts, rising up to more than 200,000 volts during intense magnetic storms. In another interpretation, the currents are the direct result of electron acceleration into the atmosphere by wave/particle interactions.
Ionospheric resistance has a complex nature, and leads to a secondary Hall current flow. By a strange twist of physics, the magnetic disturbance on the ground due to the main current almost cancels out, so most of the observed effect of auroras is due to a secondary current, the auroral electrojet. An auroral electrojet index (measured in nanotesla) is regularly derived from ground data and serves as a general measure of auroral activity. Kristian Birkeland deduced that the currents flowed in the eastwest directions along the auroral arc, and such currents, flowing from the dayside toward (approximately) midnight were later named "auroral electrojets" (see also Birkeland currents).


Interaction of the solar wind with Earth
The Earth is constantly immersed in the solar wind, a rarefied flow of magnetized hot plasma (a gas of free electrons and positive ions) emitted by the Sun in all directions, a result of the two-million-degree temperature of the Sun's outermost layer, the corona. The quiescent solar wind reaches Earth with a velocity typically around 400 km/s, a density of around 5 ions/cm3 and a magnetic field intensity of around 25 nT (for comparison, Earth's surface field is typically 30,00050,000 nT). During magnetic storms, in particular, flows can be several times faster; the interplanetary magnetic field (IMF) may also be much stronger. Joan Feynman deduced in the 1970s that the long-term averages of solar wind speed correlated with geomagnetic activity. Her work resulted from data collected by the Explorer 33 spacecraft.
The solar wind and magnetosphere consist of plasma (ionized gas), which conducts electricity. It is well known (since Michael Faraday's work around 1830) that when an electrical conductor is placed within a magnetic field while relative motion occurs in a direction that the conductor cuts across (or is cut by), rather than along, the lines of the magnetic field, an electric current is induced within the conductor. The strength of the current depends on a) the rate of relative motion, b) the strength of the magnetic field, c) the number of conductors ganged together and d) the distance between the conductor and the magnetic field, while the direction of flow is dependent upon the direction of relative motion. Dynamos make use of this basic process ("the dynamo effect"), any and all conductors, solid or otherwise are so affected, including plasmas and other fluids.
The IMF originates on the Sun, linked to the sunspots, and its field lines (lines of force) are dragged out by the solar wind. That alone would tend to line them up in the Sun-Earth direction, but the rotation of the Sun angles them at Earth by about 45 degrees forming a spiral in the ecliptic plane), known as the Parker spiral. The field lines passing Earth are therefore usually linked to those near the western edge ("limb") of the visible Sun at any time.
The solar wind and the magnetosphere, being two electrically conducting fluids in relative motion, should be able in principle to generate electric currents by dynamo action and impart energy from the flow of the solar wind. However, this process is hampered by the fact that plasmas conduct readily along magnetic field lines, but less readily perpendicular to them. Energy is more effectively transferred by the temporary magnetic connection between the field lines of the solar wind and those of the magnetosphere. Unsurprisingly this process is known as magnetic reconnection. As already mentioned, it happens most readily when the interplanetary field is directed southward, in a similar direction to the geomagnetic field in the inner regions of both the north magnetic pole and south magnetic pole.

Auroras are more frequent and brighter during the intense phase of the solar cycle when coronal mass ejections increase the intensity of the solar wind.


Magnetosphere
Earth's magnetosphere is shaped by the impact of the solar wind on the Earth's magnetic field. This forms an obstacle to the flow, diverting it, at an average distance of about 70,000 km (11 Earth radii or Re), producing a bow shock 12,000 km to 15,000 km (1.9 to 2.4 Re) further upstream. The width of the magnetosphere abreast of Earth, is typically 190,000 km (30 Re), and on the night side a long "magnetotail" of stretched field lines extends to great distances (> 200 Re).
The high latitude magnetosphere is filled with plasma as the solar wind passes the Earth. The flow of plasma into the magnetosphere increases with additional turbulence, density, and speed in the solar wind. This flow is favored by a southward component of the IMF, which can then directly connect to the high latitude geomagnetic field lines. The flow pattern of magnetospheric plasma is mainly from the magnetotail toward the Earth, around the Earth and back into the solar wind through the magnetopause on the day-side. In addition to moving perpendicular to the Earth's magnetic field, some magnetospheric plasma travels down along the Earth's magnetic field lines, gains additional energy and loses it to the atmosphere in the auroral zones. The cusps of the magnetosphere, separating geomagnetic field lines that close through the Earth from those that close remotely allow a small amount of solar wind to directly reach the top of the atmosphere, producing an auroral glow.
On 26 February 2008, THEMIS probes were able to determine, for the first time, the triggering event for the onset of magnetospheric substorms. Two of the five probes, positioned approximately one third the distance to the moon, measured events suggesting a magnetic reconnection event 96 seconds prior to auroral intensification.Geomagnetic storms that ignite auroras may occur more often during the months around the equinoxes. It is not well understood, but geomagnetic storms may vary with Earth's seasons. Two factors to consider are the tilt of both the solar and Earth's axis to the ecliptic plane. As the Earth orbits throughout a year, it experiences an interplanetary magnetic field (IMF) from different latitudes of the Sun, which is tilted at 8 degrees. Similarly, the 23-degree tilt of the Earth's axis about which the geomagnetic pole rotates with a diurnal variation changes the daily average angle that the geomagnetic field presents to the incident IMF throughout a year. These factors combined can lead to minor cyclical changes in the detailed way that the IMF links to the magnetosphere. In turn, this affects the average probability of opening a door through which energy from the solar wind can reach the Earth's inner magnetosphere and thereby enhance auroras. Recent evidence in 2021 has shown that individual separate substorms may in fact be correlated networked communities.


Auroral particle acceleration
The electrons responsible for the brightest forms of the aurora are well accounted for by their acceleration in the dynamic electric fields of plasma turbulence encountered during precipitation from the magnetosphere into the auroral atmosphere. In contrast, static electric fields are unable to transfer energy to the electrons due to their conservative nature. The electrons and ions that cause the diffuse aurora appear not to be accelerated during precipitation.
The increase in strength of magnetic field lines towards the Earth creates a 'magnetic mirror' that turns back many of the downward flowing electrons. The bright forms of auroras are produced when downward acceleration not only increases the energy of precipitating electrons but also reduces their pitch angles (angle between electron velocity and the local magnetic field vector). This greatly increases the rate of deposition of energy into the atmosphere, and thereby the rates of ionization, excitation and consequent emission of auroral light. Acceleration also increases the electron current flowing between the atmosphere and magnetosphere.
One early theory proposed for the acceleration of auroral electrons is based on an assumed static, or quasi-static, electric field creating a uni-directional potential drop. No mention is provided of either the necessary space-charge or equipotential distribution, and these remain to be specified for the notion of acceleration by double layers to be credible. Fundamentally, Poisson's equation indicates that there can be no configuration of charge resulting in a net potential drop. Inexplicably though, some authors still invoke quasi-static parallel electric fields as net accelerators of auroral electrons, citing interpretations of transient observations of fields and particles as supporting this theory as firm fact. In another example, there is little justification given for saying 'FAST observations demonstrate detailed quantitative agreement between the measured electric potentials and the ion beam energies...., leaving no doubt that parallel potential drops are a dominant source of auroral particle acceleration'.
Another theory is based on acceleration by Landau resonance in the turbulent electric fields of the acceleration region. This process is essentially the same as that employed in plasma fusion laboratories throughout the world, and appears well able to account in principle for most  if not all  detailed properties of the electrons responsible for the brightest forms of auroras, above, below and within the acceleration region.

Other mechanisms have also been proposed, in particular, Alfvn waves, wave modes involving the magnetic field first noted by Hannes Alfvn (1942), which have been observed in the laboratory and in space. The question is whether these waves might just be a different way of looking at the above process, however, because this approach does not point out a different energy source, and many plasma bulk phenomena can also be described in terms of Alfvn waves.
Other processes are also involved in the aurora, and much remains to be learned. Auroral electrons created by large geomagnetic storms often seem to have energies below 1 keV and are stopped higher up, near 200 km. Such low energies excite mainly the red line of oxygen so that often such auroras are red. On the other hand, positive ions also reach the ionosphere at such time, with energies of 2030 keV, suggesting they might be an "overflow" along magnetic field lines of the copious "ring current" ions accelerated at such times, by processes different from the ones described above.
Some O+ ions ("conics") also seem accelerated in different ways by plasma processes associated with the aurora. These ions are accelerated by plasma waves in directions mainly perpendicular to the field lines. They, therefore, start at their "mirror points" and can travel only upward. As they do so, the "mirror effect" transforms their directions of motion, from perpendicular to the field line to a cone around it, which gradually narrows down, becoming increasingly parallel at large distances where the field is much weaker.


Auroral events of historical significance
The discovery of a 1770 Japanese diary in 2017 depicting auroras above the ancient Japanese capital of Kyoto suggested that the storm may have been 7% larger than the Carrington event, which affected telegraph networks.The auroras that resulted from the "great geomagnetic storm" on both 28 August and 2 September 1859, however, are thought to be the most spectacular in recent recorded history. In a paper to the Royal Society on 21 November 1861, Balfour Stewart described both auroral events as documented by a self-recording magnetograph at the Kew Observatory and established the connection between the 2 September 1859 auroral storm and the Carrington-Hodgson flare event when he observed that "It is not impossible to suppose that in this case our luminary was taken in the act." The second auroral event, which occurred on 2 September 1859 as a result of the exceptionally intense Carrington-Hodgson white light solar flare on 1 September 1859, produced auroras, so widespread and extraordinarily bright, that they were seen and reported in published scientific measurements, ship logs, and newspapers throughout the United States, Europe, Japan, and Australia. It was reported by The New York Times that in Boston on Friday 2 September 1859 the aurora was "so brilliant that at about one o'clock ordinary print could be read by the light". One o'clock EST time on Friday 2 September, would have been 6:00 GMT and the self-recording magnetograph at the Kew Observatory was recording the geomagnetic storm, which was then one hour old, at its full intensity. Between 1859 and 1862, Elias Loomis published a series of nine papers on the Great Auroral Exhibition of 1859 in the American Journal of Science where he collected worldwide reports of the auroral event.That aurora is thought to have been produced by one of the most intense coronal mass ejections in history. It is also notable for the fact that it is the first time where the phenomena of auroral activity and electricity were unambiguously linked. This insight was made possible not only due to scientific magnetometer measurements of the era, but also as a result of a significant portion of the 125,000 miles (201,000 km) of telegraph lines then in service being significantly disrupted for many hours throughout the storm. Some telegraph lines, however, seem to have been of the appropriate length and orientation to produce a sufficient geomagnetically induced current from the electromagnetic field to allow for continued communication with the telegraph operator power supplies switched off. The following conversation occurred between two operators of the American Telegraph Line between Boston and Portland, Maine, on the night of 2 September 1859 and reported in the Boston Traveler:

Boston operator (to Portland operator): "Please cut off your battery [power source] entirely for fifteen minutes."Portland operator: "Will do so. It is now disconnected."Boston: "Mine is disconnected, and we are working with the auroral current. How do you receive my writing?"Portland: "Better than with our batteries on.  Current comes and goes gradually."Boston: "My current is very strong at times, and we can work better without the batteries, as the aurora seems to neutralize and augment our batteries alternately, making current too strong at times for our relay magnets. Suppose we work without batteries while we are affected by this trouble."Portland: "Very well. Shall I go ahead with business?"Boston: "Yes. Go ahead."

The conversation was carried on for around two hours using no battery power at all and working solely with the current induced by the aurora, and it was said that this was the first time on record that more than a word or two was transmitted in such manner. Such events led to the general conclusion that

The effect of the aurorae on the electric telegraph is generally to increase or diminish the electric current generated in working the wires. Sometimes it entirely neutralizes them, so that, in effect, no fluid [current] is discoverable in them. The aurora borealis seems to be composed of a mass of electric matter, resembling in every respect, that generated by the electric galvanic battery. The currents from it change coming on the wires, and then disappear the mass of the aurora rolls from the horizon to the zenith.


Historical theories, superstition and mythology
An aurora was described by the Greek explorer Pytheas in the 4th century BC. Seneca wrote about auroras in the first book of his Naturales Quaestiones, classifying them, for instance as pithaei ('barrel-like'); chasmata ('chasm'); pogoniae ('bearded'); cyparissae ('like cypress trees'), and describing their manifold colors. He wrote about whether they were above or below the clouds, and recalled that under Tiberius, an aurora formed above the port city of Ostia that was so intense and red that a cohort of the army, stationed nearby for fire duty, galloped to the rescue. It has been suggested that Pliny the Elder depicted the aurora borealis in his Natural History, when he refers to trabes, chasma, 'falling red flames' and 'daylight in the night'.The history of China has rich, and possibly the oldest, records of the aurora borealis. On an autumn around 2000 BC, according to a legend, a young woman named Fubao was sitting alone in the wilderness by a bay, when suddenly a "magical band of light" appeared like "moving clouds and flowing water", turning into a bright halo around the Big Dipper, which cascaded a pale silver brilliance, illuminating the earth and making shapes and shadows seem alive. Moved by this sight, Fubao became pregnant and gave birth to a son, the Emperor Xuanyuan, known legendarily as the initiator of Chinese culture and the ancestor of all Chinese people. In the Shanhaijing, a creature named 'Shilong' is described to be like a red dragon shining in the night sky with a body a thousand miles long. In ancient times, the Chinese did not have a fixed word for the aurora, so it was named according to the different shapes of the aurora, such as "Sky Dog ()", "Sword/Knife Star ()", "Chiyou banner ()", "Sky's Open Eyes ()", and "Stars like Rain ()".
In Japanese folklore, pheasants were considered messengers from heaven. However, researchers from Japan's Graduate University for Advanced Studies and National Institute of Polar Research claimed in March 2020 that red pheasant tails witnessed across the night sky over Japan in 620 A.D., might be a red aurora produced during a magnetic storm.

In the traditions of Aboriginal Australians, the Aurora Australis is commonly associated with fire. For example, the Gunditjmara people of western Victoria called auroras puae buae ('ashes'), while the Gunai people of eastern Victoria perceived auroras as bushfires in the spirit world. The Dieri people of South Australia say that an auroral display is kootchee, an evil spirit creating a large fire. Similarly, the Ngarrindjeri people of South Australia refer to auroras seen over Kangaroo Island as the campfires of spirits in the 'Land of the Dead'. Aboriginal people in southwest Queensland believe the auroras to be the fires of the Oola Pikka, ghostly spirits who spoke to the people through auroras. Sacred law forbade anyone except male elders from watching or interpreting the messages of ancestors they believed were transmitted through an aurora.Bulfinch's Mythology relates that in Norse mythology, the armour of the Valkyrior "sheds a strange flickering light, which flashes up over the northern skies, making what Men call the 'aurora borealis', or 'Northern Lights' ". There appears to be no evidence in Old Norse literature to substantiate this assertion. The first Old Norse account of norrljs is found in the Norwegian chronicle Konungs Skuggsj from AD 1230. The chronicler has heard about this phenomenon from compatriots returning from Greenland, and he gives three possible explanations: that the ocean was surrounded by vast fires; that the sun flares could reach around the world to its night side; or that glaciers could store energy so that they eventually became fluorescent.Walter William Bryant wrote in his book Kepler (1920) that Tycho Brahe "seems to have been something of a homopathist, for he recommends sulfur to cure infectious diseases 'brought on by the sulphurous vapours of the Aurora Borealis.'"
In 1778, Benjamin Franklin theorized in his paper Aurora Borealis, Suppositions and Conjectures towards forming an Hypothesis for its Explanation that an aurora was caused by a concentration of electrical charge in the polar regions intensified by the snow and moisture in the air: May not then the great quantity of electricity brought into the polar regions by the clouds, which are condens'd there, and fall in snow, which electricity would enter the earth, but cannot penetrate the ice; may it not, I say (as a bottle overcharged) break thro' that low atmosphere and run along in the vacuum over the air towards the equator, diverging as the degrees of longitude enlarge, strongly visible where densest, and becoming less visible as it more diverges; till it finds a passage to the earth in more temperate climates, or is mingled with the upper air?
Observations of the rhythmic movement of compass needles due to the influence of an aurora were confirmed in the Swedish city of Uppsala by Anders Celsius and Olof Hiorter. In 1741, Hiorter was able to link large magnetic fluctuations with an aurora being observed overhead. This evidence helped to support their theory that 'magnetic storms' are responsible for such compass fluctuations.

A variety of Native American myths surround the spectacle. The European explorer Samuel Hearne traveled with Chipewyan Dene in 1771 and recorded their views on the ed-thin ('caribou'). According to Hearne, the Dene people saw the resemblance between an aurora and the sparks produced when caribou fur is stroked. They believed that the lights were the spirits of their departed friends dancing in the sky, and when they shone brightly it meant that their deceased friends were very happy.During the night after the Battle of Fredericksburg, an aurora was seen from the battlefield. The Confederate Army took this as a sign that God was on their side, as the lights were rarely seen so far south. The painting Aurora Borealis by Frederic Edwin Church is widely interpreted to represent the conflict of the American Civil War.A mid 19th-century British source says auroras were a rare occurrence before the 18th-century. It quotes Halley as saying that before the aurora of 1716, no such phenomenon had been recorded for more than 80 years, and none of any consequence since 1574. It says no appearance is recorded in the Transactions of the French Academy of Sciences between 1666 and 1716. And that one aurora recorded in Berlin Miscellany for 1797 was called a very rare event. One observed in 1723 at Bologna was stated to be the first ever seen there. Celsius (1733) states the oldest residents of Uppsala thought the phenomenon a great rarity before 1716. The period between approximately 1645 to 1715 corresponds to the Maunder minimum in sunspot activity.
In Robert W. Service's satirical poem "The Ballad of the Northern Lights" (1908) a Yukon prospector discovers that the aurora is the glow from a radium mine. He stakes his claim, then goes to town looking for investors.
It was the Norwegian scientist Kristian Birkeland who, in the early 1900s, laid the foundation for our current understanding of geomagnetism and polar auroras.


Non-terrestrial auroras

Both Jupiter and Saturn have magnetic fields that are stronger than Earth's (Jupiter's equatorial field strength is 4.3 Gauss, compared to 0.3 Gauss for Earth), and both have extensive radiation belts. Auroras have been observed on both gas planets, most clearly using the Hubble Space Telescope, and the Cassini and Galileo spacecraft, as well as on Uranus and Neptune.The aurorae on Saturn seem, like Earth's, to be powered by the solar wind. However, Jupiter's aurorae are more complex. The Jupiter's main auroral oval is associated with the plasma produced by the volcanic moon, Io and the transport of this plasma within the planet's magnetosphere. An uncertain fraction of Jupiter's aurorae are powered by the solar wind. In addition, the moons, especially Io, are also powerful sources of aurora. These arise from electric currents along field lines ("field aligned currents"), generated by a dynamo mechanism due to the relative motion between the rotating planet and the moving moon. Io, which has active volcanism and an ionosphere, is a particularly strong source, and its currents also generate radio emissions, which have been studied since 1955. Using the Hubble Space Telescope, auroras over Io, Europa and Ganymede have all been observed.
Auroras have also been observed on Venus and Mars. Venus has no magnetic field and so Venusian auroras appear as bright and diffuse patches of varying shape and intensity, sometimes distributed over the full disc of the planet. A Venusian aurora originates when electrons from the solar wind collide with the night-side atmosphere.
An aurora was detected on Mars, on 14 August 2004, by the SPICAM instrument aboard Mars Express. The aurora was located at Terra Cimmeria, in the region of 177 East, 52 South. The total size of the emission region was about 30 km across, and possibly about 8 km high. By analyzing a map of crustal magnetic anomalies compiled with data from Mars Global Surveyor, scientists observed that the region of the emissions corresponded to an area where the strongest magnetic field is localized. This correlation indicated that the origin of the light emission was a flux of electrons moving along the crust magnetic lines and exciting the upper atmosphere of Mars.In September 2020, cometary auroras were announced on the comet 67P/Churyumov-Gerasimenko by multiple instruments on the Rosetta spacecraft. The auroras were observed at far-ultraviolet wavelengths. Coma observations revealed atomic emissions of hydrogen and oxygen caused by the photodissociation (not photoionization, like in terrestrial auroras) of water molecules in the comet's coma. The interaction of accelerated electrons from the solar wind with gas particles in the coma is responsible for the aurora. Since comet 67P has no magnetic field, the aurora is diffusely spread around the comet.Exoplanets, such as hot Jupiters, have been suggested to experience ionization in their upper atmospheres and generate an aurora modified by weather in their turbulent tropospheres. However, there is no current detection of an exoplanet aurora.
The first ever extra-solar auroras were discovered in July 2015 over the brown dwarf star LSR J1835+3259. The mainly red aurora was found to be a million times brighter than the Northern Lights, a result of the charged particles interacting with hydrogen in the atmosphere. It has been speculated that stellar winds may be stripping off material from the surface of the brown dwarf to produce their own electrons. Another possible explanation for the auroras is that an as-yet-undetected body around the dwarf star is throwing off material, as is the case with Jupiter and its moon Io.


Table tennis, also known as ping-pong and whiff-whaff, is a sport in which two or four players hit a lightweight ball, also known as the ping-pong ball, back and forth across a table using small rackets. The game takes place on a hard table divided by a net. Except for the initial serve, the rules are generally as follows: players must allow a ball played toward them to bounce one time on their side of the table and must return it so that it bounces on the opposite side at least once. A point is scored when a player fails to return the ball within the rules. Play is fast and demands quick reactions. Spinning the ball alters its trajectory and limits an opponent's options, giving the hitter a great advantage.
Table tennis is governed by the worldwide organization International Table Tennis Federation (ITTF), founded in 1926. ITTF currently includes 226 member associations. The table tennis official rules are specified in the ITTF handbook. Table tennis has been an Olympic sport since 1988, with several event categories. From 1988 until 2004, these were men's singles, women's singles, men's doubles and women's doubles. Since 2008, a team event has been played instead of the doubles.


History

The sport originated in Victorian England, where it was played among the upper-class as an after-dinner parlour game. It has been suggested that makeshift versions of the game were developed by British military officers in India around the 1860s or 1870s, who brought it back with them. A row of books stood up along the center of the table as a net, two more books served as rackets and were used to continuously hit a golf-ball.The name "ping-pong" was in wide use before British manufacturer J. Jaques & Son Ltd trademarked it in 1901. The name "ping-pong" then came to describe the game played using the rather expensive Jaques's equipment, with other manufacturers calling it table tennis. A similar situation arose in the United States, where Jaques sold the rights to the "ping-pong" name to Parker Brothers. Parker Brothers then enforced its trademark for the term in the 1920s making the various associations change their names to "table tennis" instead of the more common, but trademarked, term.The next major innovation was by James W. Gibb, a British enthusiast of table tennis, who discovered novelty celluloid balls on a trip to the US in 1901 and found them to be ideal for the game. This was followed by E.C. Goode who, in 1901, invented the modern version of the racket by fixing a sheet of pimpled, or stippled, rubber to the wooden blade. Table tennis was growing in popularity by 1901 to the extent that tournaments were being organized, books being written on the subject, and an unofficial world championship was held in 1902. In those early days, the scoring system was the same as in lawn tennis.Although both a "Table Tennis Association" and a "Ping Pong Association" existed by 1910, a new Table Tennis Association was founded in 1921, and in 1926 renamed the English Table Tennis Association. The International Table Tennis Federation (ITTF) followed in 1926. London hosted the first official World Championships in 1926. In 1933, the United States Table Tennis Association, now called USA Table Tennis, was formed.In the 1930s, Edgar Snow commented in Red Star Over China that the Communist forces in the Chinese Civil War had a "passion for the English game of table tennis" which he found "bizarre". On the other hand, the popularity of the sport waned in 1930s Soviet Union, partly because of the promotion of team and military sports, and partly because of a theory that the game had adverse health effects.In the 1950s, paddles that used a rubber sheet combined with an underlying sponge layer changed the game dramatically, introducing greater spin and speed. These were introduced to Britain by sports goods manufacturer S.W. Hancock Ltd. The use of speed glue beginning in the mid 1980s increased the spin and speed even further, resulting in changes to the equipment to "slow the game down". Table tennis was introduced as an Olympic sport at the Olympics in 1988.


Rule changes

After the 2000 Olympics in Sydney, the ITTF instituted several rule changes that were aimed at making table tennis more viable as a televised spectator sport. First, the older 38 mm (1.50 in) balls were officially replaced by 40 mm (1.57 in) balls in October 2000. This increased the ball's air resistance and effectively slowed down the game. By that time, players had begun increasing the thickness of the fast sponge layer on their paddles, which made the game excessively fast and difficult to watch on television. A few months later, the ITTF changed from a 21-point to an 11-point scoring system (and the serve rotation was reduced from five points to two), effective in September 2001. This was intended to make games more fast-paced and exciting. The ITTF also changed the rules on service to prevent a player from hiding the ball during service, in order to increase the average length of rallies and to reduce the server's advantage, effective in 2002.
For the opponent to have time to realize a serve is taking place, the ball must be tossed a minimum of 16 centimetres (6.3 in) in the air. The ITTF states that all events after July 2014 are played with a new poly material ball.


Equipment


Ball

The international rules specify that the game is played with a sphere having a mass of 2.7 grams (0.095 oz) and a diameter of 40 millimetres (1.57 in). The rules say that the ball shall bounce up 2426 cm (9.410.2 in) when dropped from a height of 30.5 cm (12.0 in) onto a standard steel block thereby having a coefficient of restitution of 0.89 to 0.92. Balls are now made of a polymer instead of celluloid as of 2015, colored white or orange, with a matte finish. The choice of ball color is made according to the table color and its surroundings. For example, a white ball is easier to see on a green or blue table than it is on a grey table. Manufacturers often indicate the quality of the ball with a star rating system, usually from one to three, three being the highest grade. As this system is not standard across manufacturers, the only way a ball may be used in official competition is upon ITTF approval (the ITTF approval can be seen printed on the ball).
The 40 mm ball was introduced after the end of the 2000 Summer Olympics; previously a 38 mm ball was standard. This created some controversies. Then World No 1 table tennis professional Vladimir Samsonov threatened to pull out of the World Cup, which was scheduled to debut the new regulation ball on October 12, 2000.


Table

The table is 2.74 m (9.0 ft) long, 1.525 m (5.0 ft) wide, and 76 cm (2.5 ft) high with any continuous material so long as the table yields a uniform bounce of about 23 cm (9.1 in) when a standard ball is dropped onto it from a height of 30 cm (11.8 in), or about 77%. The table or playing surface is uniformly dark coloured and matte, divided into two halves by a net at 15.25 cm (6.0 in) in height. The ITTF approves only wooden tables or their derivates. Concrete tables with a steel net or a solid concrete partition are sometimes available in outside public spaces, such as parks.


Racket/paddle/bat

Players are equipped with a laminated wooden racket covered with rubber on one or two sides depending on the grip of the player. The ITTF uses the term "racket", though "bat" is common in Britain, and "paddle" in the U.S. and Canada.
The wooden portion of the racket, often referred to as the "blade", commonly features anywhere between one and seven plies of wood, though cork, glass fiber, carbon fiber, aluminum fiber, and Kevlar are sometimes used. According to the ITTF regulations, at least 85% of the blade by thickness shall be of natural wood. Common wood types include balsa, limba, and cypress or "hinoki", which is popular in Japan. The average size of the blade is about 17 centimetres (6.7 in) long and 15 centimetres (5.9 in) wide, although the official restrictions only focus on the flatness and rigidity of the blade itself, these dimensions are optimal for most play styles.
Table tennis regulations allow different rubber surfaces on each side of the racket. Various types of surfaces provide various levels of spin or speed, and in some cases they nullify spin. For example, a player may have a rubber that provides much spin on one side of their racket, and one that provides no spin on the other. By flipping the racket in play, different types of returns are possible. To help a player distinguish between the rubber used by his opposing player, international rules specify that one side must be red while the other side must be black. The player has the right to inspect their opponent's racket before a match to see the type of rubber used and what colour it is. Despite high speed play and rapid exchanges, a player can see clearly what side of the racket was used to hit the ball. Current rules state that, unless damaged in play, the racket cannot be exchanged for another racket at any time during a match.


Gameplay


Starting a game
According to ITTF rule 2.13.1, the first service is decided by lot, normally a coin toss. It is also common for one player (or the umpire/scorer) to hide the ball in one or the other hand, usually hidden under the table, allowing the other player to guess which hand the ball is in. The correct or incorrect guess gives the "winner" the option to choose to serve, receive, or to choose which side of the table to use. (A common but non-sanctioned method is for the players to play the ball back and forth three times and then play out the point. This is commonly referred to as "serve to play", "rally to serve", "play for serve", or "volley for serve".)


Service and return

In game play, the player serving the ball commences a play. The server first stands with the ball held on the open palm of the hand not carrying the paddle, called the freehand, and tosses the ball directly upward without spin, at least 16 cm (6.3 in) high. The server strikes the ball with the racket on the ball's descent so that it touches first his court and then touches directly the receiver's court without touching the net assembly. In casual games, many players do not toss the ball upward; however, this is technically illegal and can give the serving player an unfair advantage.
The ball must remain behind the endline and above the upper surface of the table, known as the playing surface, at all times during the service. The server cannot use his/her body or clothing to obstruct sight of the ball; the opponent and the umpire must have a clear view of the ball at all times. If the umpire is doubtful of the legality of a service they may first interrupt play and give a warning to the server. If the serve is a clear failure or is doubted again by the umpire after the warning, the receiver scores a point.
If the service is "good", then the receiver must make a "good" return by hitting the ball back before it bounces a second time on receiver's side of the table so that the ball passes the net and touches the opponent's court, either directly or after touching the net assembly. Thereafter, the server and receiver must alternately make a return until the rally is over. Returning the serve is one of the most difficult parts of the game, as the server's first move is often the least predictable and thus most advantageous shot due to the numerous spin and speed choices at his or her disposal.


Let
A Let is a rally of which the result is not scored, and is called in the following circumstances:
The ball touches the net in service (service), provided the service is otherwise correct or the ball is obstructed by the player on the receiving side. Obstruction means a player touches the ball when it is above or traveling towards the playing surface, not having touched the player's court since last being struck by the player.
When the player on the receiving side is not ready and the service is delivered.
Player's failure to make a service or a return or to comply with the Laws is due to a disturbance outside the control of the player.
Play is interrupted by the umpire or assistant umpire.A let is also called foul service, if the ball hits the server's side of the table, if the ball does not pass further than the edge and if the ball hits the table edge and hits the net.


Scoring

A point is scored by the player for any of several results of the rally:
The opponent fails to make a correct service or return.
After making a service or a return, the ball touches anything other than the net assembly before being struck by the opponent.
The ball passes over the player's court or beyond their end line without touching their court, after being struck by the opponent.
The opponent obstructs the ball.
The opponent strikes the ball twice successively. Note that the hand that is holding the racket counts as part of the racket and that making a good return off one's hand or fingers is allowed. It is not a fault if the ball accidentally hits one's hand or fingers and then subsequently hits the racket.
The opponent strikes the ball with a side of the racket blade whose surface is not covered with rubber.
The opponent moves the playing surface or touches the net assembly.
The opponent's free hand touches the playing surface.
As a receiver under the expedite system, completing 13 returns in a rally.
The opponent that has been warned by the umpire commits a second offense in the same individual match or team match. If the third offence happens, 2 points will be given to the player. If the individual match or the team match has not ended, any unused penalty points can be transferred to the next game of that match.A game shall be won by the player first scoring 11 points unless both players score 10 points, when the game shall be won by the first player subsequently gaining a lead of 2 points. A match shall consist of the best of any odd number of games. In competition play, matches are typically best of five or seven games.


Alternation of services and ends
Service alternates between opponents every two points (regardless of winner of the rally) until the end of the game, unless both players score ten points or the expedite system is operated, when the sequences of serving and receiving stay the same but each player serves for only one point in turn (Deuce). The player serving first in a game receives first in the next game of the match.
After each game, players switch sides of the table. In the last possible game of a match, for example the seventh game in a best of seven matches, players change ends when the first player scores five points, regardless of whose turn it is to serve. If the sequence of serving and receiving is out of turn or the ends are not changed, points scored in the wrong situation are still calculated and the game shall be resumed with the order at the score that has been reached.


Doubles game

In addition to games between individual players, pairs may also play table tennis. Singles and doubles are both played in international competition, including the Olympic Games since 1988 and the Commonwealth Games since 2002. In 2005, the ITTF announced that doubles table tennis only was featured as a part of team events in the 2008 Olympics.
In doubles, all the rules of single play are applied except for the following.
Service

A line painted along the long axis of the table to create doubles courts bisects the table. This line's only purpose is to facilitate the doubles service rule, which is that service must originate from the right hand "box" in such a way that the first bounce of the serve bounces once in said right hand box and then must bounce at least once in the opponent side's right hand box (far left box for server), or the receiving pair score a point.Order of play, serving and receiving

Players must hit the ball in turn. For example, if A is paired with B, X is paired with Y, A is the server and X is the receiver. The order of play shall be AXBY. The rally proceeds this way until one side fails to make a legal return and the other side scores.
At each change of service, the previous receiver shall become the server and the partner of the previous server shall become the receiver. For example, if the previous order of play is AXBY, the order becomes XBYA after the change of service.
In the second or the latter games of a match, the game begins in reverse order of play. For example, if the order of play is AXBY at beginning of the first game, the order begins with XAYB or YBXA in the second game depending on either X or Y being chosen as the first server of the game. That means the first receiver of the game is the player who served to the first server of the game in the preceding game. In each game of a doubles match, the pair having the right to serve first shall choose which of them will do so. The receiving pair, however, can only choose in the first game of the match.
When a pair reaches 5 points in the final game, the pairs must switch ends of the table and change the receiver to reverse the order of play. For example, when the last order of play before a pair score 5 points in the final game is AXBY, the order after change shall be AYBX if A still has the second serve. Otherwise, X is the next server and the order becomes XAYB.


Expedite system
If a game is unfinished after 10 minutes' play and fewer than 18 points have been scored, the expedite system is initiated. The umpire interrupts the game, and the game resumes with players serving for one point in turn. If the expedite system is introduced while the ball is not in play, the previous receiver shall serve first. Under the expedite system, the server must win the point before the opponent makes 13 consecutive returns or the point goes to the opponent. The system can also be initiated at any time at the request of both players or pairs. Once introduced, the expedite system remains in force until the end of the match. A rule to shorten the time of a match, it is mainly seen in defensive players' games.


Grips
Though table tennis players grip their rackets in various ways, their grips can be classified into two major families of styles, penhold and shakehand. The rules of table tennis do not prescribe the manner in which one must grip the racket, and numerous grips are employed.


Penhold

The penhold grip is so-named because one grips the racket similarly to the way one holds a writing instrument. The style of play among penhold players can vary greatly from player to player. The most popular style, usually referred to as the Chinese penhold style, involves curling the middle, ring, and fourth finger on the back of the blade with the three fingers always touching one another. Chinese penholders favour a round racket head, for a more over-the-table style of play. In contrast, another style, sometimes referred to as the Japanese/Korean penhold grip, involves splaying those three fingers out across the back of the racket, usually with all three fingers touching the back of the racket, rather than stacked upon one another. Sometimes a combination of the two styles occurs, wherein the middle, ring and fourth fingers are straight, but still stacked, or where all fingers may be touching the back of the racket, but are also in contact with one another. Japanese and Korean penholders will often use a square-headed racket for an away-from-the-table style of play. Traditionally these square-headed rackets feature a block of cork on top of the handle, as well as a thin layer of cork on the back of the racket, for increased grip and comfort. Penhold styles are popular among players originating from East Asian countries such as China, Japan, South Korea, and Taiwan.
Traditionally, penhold players use only one side of the racket to hit the ball during normal play, and the side which is in contact with the last three fingers is generally not used. This configuration is sometimes referred to as "traditional penhold" and is more commonly found in square-headed racket styles. However, the Chinese developed a technique in the 1990s in which a penholder uses both sides of the racket to hit the ball, where the player produces a backhand stroke (most often topspin) known as a reverse penhold backhand by turning the traditional side of the racket to face one's self, and striking the ball with the opposite side of the racket. This stroke has greatly improved and strengthened the penhold style both physically and psychologically, as it eliminates the strategic weakness of the traditional penhold backhand.


Shakehand

The shakehand grip is so-named because the racket is grasped as if one is performing a handshake. Though it is sometimes referred to as the "tennis" or "Western" grip, it bears no relation to the Western tennis grip, which was popularized on the West Coast of the United States in which the racket is rotated 90, and played with the wrist turned so that on impact the knuckles face the target.  In table tennis, "Western" refers to Western nations, for this is the grip that players native to Europe and the Americas have almost exclusively employed.
The shakehand grip's simplicity and versatility, coupled with the acceptance among top-level Chinese trainers that the European style of play should be emulated and trained against, has established it as a common grip even in China. Many world class European and East Asian players currently use the shakehand grip, and it is generally accepted that shakehands is easier to learn than penholder, allowing a broader range of playing styles both offensive and defensive.


Seemiller
The Seemiller grip is named after the American table tennis champion Danny Seemiller, who used it. It is achieved by placing the thumb and index finger on either side of the bottom of the racquet head and holding the handle with the rest of the fingers. Since only one side of the racquet is used to hit the ball, two contrasting rubber types can be applied to the blade, offering the advantage of "twiddling" the racket to fool the opponent. Seemiller paired inverted rubber with anti-spin rubber. Many players today combine inverted and long-pipped rubber. The grip is considered exceptional for blocking, especially on the backhand side, and for forehand loops of backspin balls.
The Seemiller grip's popularity reached its apex in 1985 when four (Danny Seemiller, Ricky Seemiller, Eric Boggan and Brian Masters) of the United States' five participants in the World Championships used it.


Stance
'A good ready position will enable you to move quickly into position and to stay balanced whilst playing powerful strokes.'
The stance in table tennis is also known as the 'ready position'. It is the position every player initially adopts when receiving and returns to after playing a shot in order to be prepared to make the next shot. It involves the feet being spaced wider than shoulder width and a partial crouch being adopted; the crouch is an efficient posture for moving quickly from and also preloads the muscles enabling a more dynamic movement. The upper torso is positioned slightly forward and the player is looking forwards. The racket is held at the ready with a bent arm. The position should feel balanced and provide a solid base for striking and quick lateral movement. Players may tailor their stance based upon their personal preferences, and alter it during the game based upon the specific circumstances.


Types of strokes
Table tennis strokes generally break down into offensive and defensive categories.


Offensive strokes


Hit
Also known as speed drive, a direct hit on the ball propelling it forward back to the opponent. This stroke differs from speed drives in other racket sports like tennis because the racket is primarily perpendicular to the direction of the stroke and most of the energy applied to the ball results in speed rather than spin, creating a shot that does not arc much, but is fast enough that it can be difficult to return. A speed drive is used mostly for keeping the ball in play, applying pressure on the opponent, and potentially opening up an opportunity for a more powerful attack.


Loop
Perfected during the 1960s, the loop is essentially the reverse of the chop. The racket is parallel to the direction of the stroke ("closed") and the racket thus grazes the ball, resulting in a large amount of topspin. A good loop drive will arc quite a bit, and once striking the opponent's side of the table will jump forward, much like a kick serve in tennis. Most professional players nowadays, such as Ding Ning, Timo Boll and Zhang Jike, primarily use loop for offense.


Counter-hit
The counter-hit is usually a counterattack against drives, normally high loop drives. The racket is held closed and near to the ball, which is hit with a short movement "off the bounce" (immediately after hitting the table) so that the ball travels faster to the other side. Kenta Matsudaira is known for primarily using counter-hit for offense.


Flip
When a player tries to attack a ball that has not bounced beyond the edge of the table, the player does not have the room to wind up in a backswing. The ball may still be attacked, however, and the resulting shot is called a flip because the backswing is compressed into a quick wrist action. A flip is not a single stroke and can resemble either a loop drive or a loop in its characteristics. What identifies the stroke is that the backswing is compressed into a short wrist flick.


Smash
A player will typically execute a smash when the opponent has returned a ball that bounces too high or too close to the net. It is nearly always done with a forehand stroke. Smashing use rapid acceleration to impart as much speed on the ball as possible so that the opponent cannot react in time. The racket is generally perpendicular to the direction of the stroke. Because the speed is the main aim of this shot, the spin on the ball is often minimal, although it can be applied as well. An offensive table tennis player will think of a rally as a build-up to a winning smash. Smash is used more often with penhold grip.


Defensive strokes


Push
The push (or "slice" in Asia) is usually used for keeping the point alive and creating offensive opportunities. A push resembles a tennis slice: the racket cuts underneath the ball, imparting backspin and causing the ball to float slowly to the other side of the table. A push can be difficult to attack because the backspin on the ball causes it to drop toward the table upon striking the opponent's racket. In order to attack a push, a player must usually loop (if the push is long) or flip (if the push is short) the ball back over the net. Often, the best option for beginners is to simply push the ball back again, resulting in pushing rallies. Against good players, it may be the worst option because the opponent will counter with a loop, putting the first player in a defensive position. Pushing can have advantages in some circumstances, such as when the opponent makes easy mistakes.


Chop
A chop is the defensive, backspin counterpart to the offensive loop drive. A chop is essentially a bigger, heavier push, taken well back from the table. The racket face points primarily horizontally, perhaps a little bit upward, and the direction of the stroke is straight down. The object of a defensive chop is to match the topspin of the opponent's shot with backspin. A good chop will float nearly horizontally back to the table, in some cases having so much backspin that the ball actually rises. Such a chop can be extremely difficult to return due to its enormous amount of backspin. Some defensive players can also impart no-spin or sidespin variations of the chop. Some famous choppers include Joo Sae-hyuk and Wu Yang.


Block
A block is executed by simply placing the racket in front of the ball right after the ball bounces; thus, the ball rebounds back toward the opponent with nearly as much energy as it came in with. This requires precision, since the ball's spin, speed, and location all influence the correct angle of a block. It is very possible for an opponent to execute a perfect loop, drive, or smash, only to have the blocked shot come back just as fast. Due to the power involved in offensive strokes, often an opponent simply cannot recover quickly enough to return the blocked shot, especially if the block is aimed at an unexpected side of the table. Blocks almost always produce the same spin as was received, many times topspin.


Lob
The defensive lob propels the ball about five metres in height, only to land on the opponent's side of the table with great amounts of spin. The stroke itself consists of lifting the ball to an enormous height before it falls back to the opponent's side of the table. A lob can have nearly any kind of spin. Though the opponent may smash the ball hard and fast, a good defensive lob could be more difficult to return due to the unpredictability and heavy amounts of the spin on the ball. Thus, though backed off the table by tens of feet and running to reach the ball, a good defensive player can still win the point using good lobs. Lob is used less frequently by professional players. A notable exception is Michael Maze.


Effects of spin
Adding spin onto the ball causes major changes in table tennis gameplay. Although nearly every stroke or serve creates some kind of spin, understanding the individual types of spin allows players to defend against and use different spins effectively.


Backspin
Backspin is where the bottom half of the ball is rotating away from the player, and is imparted by striking the base of the ball with a downward movement. At the professional level, backspin is usually used defensively in order to keep the ball low. Backspin is commonly employed in service because it is harder to produce an offensive return, though at the professional level most people serve sidespin with either backspin or topspin.  Due to the initial lift of the ball, there is a limit on how much speed with which one can hit the ball without missing the opponent's side of the table. However, backspin also makes it harder for the opponent to return the ball with great speed because of the required angular precision of the return. Alterations are frequently made to regulations regarding equipment in an effort to maintain a balance between defensive and offensive spin choices. It is actually possible to smash with backspin offensively, but only on high balls that are close to the net.


Topspin
The topspin stroke has a smaller influence on the first part of the ball-curve. Like the backspin stroke, however, the axis of spin remains roughly perpendicular to the trajectory of the ball thus allowing for the Magnus effect to dictate the subsequent curvature. After the apex of the curve, the ball dips downwards as it approaches the opposing side, before bouncing. On the bounce, the topspin will accelerate the ball, much in the same way that a wheel which is already spinning would accelerate upon making contact with the ground. When the opponent attempts to return the ball, the topspin causes the ball to jump upwards and the opponent is forced to compensate for the topspin by adjusting the angle of his or her racket. This is known as "closing the racket".
The speed limitation of the topspin stroke is minor compared to the backspin stroke. This stroke is the predominant technique used in professional competition because it gives the opponent less time to respond. In table tennis topspin is regarded as an offensive technique due to increased ball speed, lower bio-mechanical efficiency and the pressure that it puts on the opponent by reducing reaction time. (It is possible to play defensive topspin-lobs from far behind the table, but only highly skilled players use this stroke with any tactical efficiency.) Topspin is the least common type of spin to be found in service at the professional level, simply because it is much easier to attack a top-spin ball that is not moving at high speed.


Sidespin
This type of spin is predominantly employed during service, wherein the contact angle of the racket can be more easily varied. Unlike the two aforementioned techniques, sidespin causes the ball to spin on an axis which is vertical, rather than horizontal. The axis of rotation is still roughly perpendicular to the trajectory of the ball. In this circumstance, the Magnus effect will still dictate the curvature of the ball to some degree. Another difference is that unlike backspin and topspin, sidespin will have relatively very little effect on the bounce of the ball, much in the same way that a spinning top would not travel left or right if its axis of rotation were exactly vertical. This makes sidespin a useful weapon in service, because it is less easily recognized when bouncing, and the ball "loses" less spin on the bounce. Sidespin can also be employed in offensive rally strokes, often from a greater distance, as an adjunct to topspin or backspin. This stroke is sometimes referred to as a "hook". The hook can even be used in some extreme cases to circumvent the net when away from the table.


Corkspin
Players employ this type of spin almost exclusively when serving, but at the professional level, it is also used from time to time in the lob. Unlike any of the techniques mentioned above, corkspin (or "drill-spin") has the axis of spin relatively parallel to the ball's trajectory, so that the Magnus effect has little or no effect on the trajectory of a cork-spun ball: upon bouncing, the ball will dart right or left (according to the direction of the spin), severely complicating the return. In theory this type of spin produces the most obnoxious effects, but it is less strategically practical than sidespin or backspin, because of the limitations that it imposes upon the opponent during their return. Aside from the initial direction change when bouncing, unless it goes out of reach, the opponent can counter with either topspin or backspin. A backspin stroke is similar in the fact that the corkspin stroke has a lower maximum velocity, simply due to the contact angle of the racket when producing the stroke. To impart a spin on the ball which is parallel to its trajectory, the racket must be swung more or less perpendicular to the trajectory of the ball, greatly limiting the forward momentum that the racket transfers to the ball. Corkspin is almost always mixed with another variety of spin, since alone, it is not only less effective but also harder to produce.


Competitions

Competitive table tennis is popular in East Asia and Europe, and has been gaining attention in the United States. The most important international competitions are the World Table Tennis Championships, the Table Tennis World Cup, the Olympics and the ITTF World Tour. Continental competitions include the following: 

European Championships
Europe Top-16
the Asian Championships
the Asian GamesChinese players have won 60% of the men's World Championships since 1959; in the women's competition for the Corbillin Cup, Chinese players have won all but three of the World Championships since 1971. Other strong teams come from East Asia and Europe, including countries such as Austria, Belarus, Germany, Hong Kong, Portugal, Japan, South Korea, Singapore, Sweden, and Taiwan.There are professional competitions at the clubs level; the respective leagues of Austria, Belgium, China (China Table Tennis Super League), Japan (T.League), France, Germany (Bundesliga), and Russia are examples of the highest level. There are also some important international club teams competitions such as the European Champions League and its former competitor, the European Club Cup, where the top club teams from European countries compete.


Naturalization in table tennis

According to the New York Times, 31% of the table tennis players at the 2016 Summer Olympics were naturalized. The rate was twice as high as the next sport, basketball, which featured 15% of naturalized players.In particular, Chinese-born players representing Singapore have won three Olympic medals, more than native Singaporeans have ever won in all sports. However, these successes have been very controversial in Singapore. In 2014, Singapore Table Tennis Association's president Lee Bee Wah quit over this issue; however, her successor Ellen Lee has basically continued on this path.The rate of naturalization accelerated after the ITTF's 2009 decision (one year after China won every possible Olympic medal in the sport) to reduce the number of entries per association in both the Olympics and the World Table Tennis Championships.In 2019, the ITTF adopted new regulations which state that players who acquired a new nationality may not represent their new association before:
1 year after the date of registration, if the player is under the age of 15 when registered and has never represented another association
3 years after the date of registration, if the player is under the age of 15 when registered and has already represented another association
5 years after the date of registration, if the player is under the age of 18 but at least 15 years of age when registered
7 years after the date of registration, if the player is under the age of 21 but at least 18 years of age when registered
9 years after the date of registration, if the player is at least 21 years old when registered


Notable players

An official hall of fame exists at the ITTF Museum. A Grand Slam is earned by a player who wins singles crowns at the Olympic Games, World Championships, and World Cup. Jan-Ove Waldner of Sweden first completed the grand slam at 1992 Olympic Games. Deng Yaping of China is the first female recorded at the inaugural Women's World Cup in 1996. The following table presents an exhaustive list of all players to have completed a grand slam.

Jean-Philippe Gatien (France) and Wang Hao (China) won both the World Championships and the World Cup, but lost in the gold medal matches at the Olympics. Jrgen Persson (Sweden) also won the titles except the Olympic Games. Persson is one of the three table tennis players to have competed at seven Olympic Games. Ma Lin (China) won both the Olympic gold and the World Cup, but lost (three times, in 1999, 2005, and 2007) in the finals of the World Championships.


Governance

Founded in 1926, the International Table Tennis Federation (ITTF) is the worldwide governing body for table tennis, which maintains an international ranking system in addition to organizing events like the World Table Tennis Championships.  In 2007, the governance for table tennis for persons with a disability was transferred from the International Paralympic Committee to the ITTF.On many continents, there is a governing body responsible for table tennis on that continent. For example, the European Table Tennis Union (ETTU) is the governing body responsible for table tennis in Europe. There are also national bodies and other local authorities responsible for the sport, such as USA Table Tennis (USATT), which is the national governing body for table tennis in the United States.


Variants
Hardbat table tennis uses rackets with short outward "pips" and no sponge, resulting in decreased speeds and reduced spin. World Championship of Ping Pong uses old-fashioned wooden paddles covered with sandpaper.
Round the World (also called Round Robin or Round the Table) table tennis is an informal party-type variation in which players line up on either side of the table. When a player hits the ball he sets the paddle down, and the player behind him picks it up to receive the return. When a player sets down his paddle, he moves to the line at the opposing side of the table. Players are eliminated as they lose a point. When only 2 players remain, a player hits the ball, sets his paddle down, spins and then retrieves his paddle to make the return. 


Hardness is a measure of the resistance to localized plastic deformation induced by either mechanical indentation or abrasion. In general, different materials differ in their hardness; for example hard metals such as titanium and beryllium are harder than soft metals such as sodium and metallic tin, or wood and common plastics. Macroscopic hardness is generally characterized by strong intermolecular bonds, but the behavior of solid materials under force is complex; therefore, there are different measurements of hardness: scratch hardness, indentation hardness, and rebound hardness.
Hardness is dependent on ductility, elastic stiffness, plasticity, strain, strength, toughness, viscoelasticity, and viscosity.
Common examples of hard matter are ceramics, concrete, certain metals, and superhard materials, which can be contrasted with soft matter.


Measuring hardness

There are three main types of hardness measurements: scratch, indentation, and rebound. Within each of these classes of measurement there are individual measurement scales. For practical reasons conversion tables are used to convert between one scale and another.


Scratch hardness

Scratch hardness is the measure of how resistant a sample is to fracture or permanent plastic deformation due to friction from a sharp object. The principle is that an object made of a harder material will scratch an object made of a softer material. When testing coatings, scratch hardness refers to the force necessary to cut through the film to the substrate. The most common test is Mohs scale, which is used in mineralogy. One tool to make this measurement is the sclerometer.
Another tool used to make these tests is the pocket hardness tester. This tool consists of a scale arm with graduated markings attached to a four-wheeled carriage. A scratch tool with a sharp rim is mounted at a predetermined angle to the testing surface. In order to use it a weight of known mass is added to the scale arm at one of the graduated markings, the tool is then drawn across the test surface. The use of the weight and markings allows a known pressure to be applied without the need for complicated machinery.


Indentation hardness

Indentation hardness measures the resistance of a sample to material deformation due to a constant compression load from a sharp object. Tests for indentation hardness are primarily used in engineering and metallurgy fields. The tests work on the basic premise of measuring the critical dimensions of an indentation left by a specifically dimensioned and loaded indenter.
Common indentation hardness scales are Rockwell, Vickers, Shore, and Brinell, amongst others.


Rebound hardness
Rebound hardness, also known as dynamic hardness, measures the height of the "bounce" of a diamond-tipped hammer dropped from a fixed height onto a material. This type of hardness is related to elasticity. The device used to take this measurement is known as a scleroscope.Two scales that measures rebound hardness are the Leeb rebound hardness test and Bennett hardness scale.
Ultrasonic Contact Impedance (UCI) method determines hardness by measuring the frequency of an oscillating rod. The rod consists of a metal shaft with vibrating element and a pyramid-shaped diamond mounted on one end.


Hardening

There are five hardening processes: Hall-Petch strengthening, work hardening, solid solution strengthening, precipitation hardening, and martensitic transformation.


Physics

In solid mechanics, solids generally have three responses to force, depending on the amount of force and the type of material:

They exhibit elasticitythe ability to temporarily change shape, but return to the original shape when the pressure is removed. "Hardness" in the elastic rangea small temporary change in shape for a given forceis known as stiffness in the case of a given object, or a high elastic modulus in the case of a material.
They exhibit plasticitythe ability to permanently change shape in response to the force, but remain in one piece. The yield strength is the point at which elastic deformation gives way to plastic deformation. Deformation in the plastic range is non-linear, and is described by the stress-strain curve. This response produces the observed properties of scratch and indentation hardness, as described and measured in materials science. Some materials exhibit both elasticity and viscosity when undergoing plastic deformation; this is called viscoelasticity.
They fracturesplit into two or more pieces.Strength is a measure of the extent of a material's elastic range, or elastic and plastic ranges together. This is quantified as compressive strength, shear strength, tensile strength depending on the direction of the forces involved. Ultimate strength is an engineering measure of the maximum load a part of a specific material and geometry can withstand.
Brittleness, in technical usage, is the tendency of a material to fracture with very little or no detectable plastic deformation beforehand. Thus in technical terms, a material can be both brittle and strong. In everyday usage "brittleness" usually refers to the tendency to fracture under a small amount of force, which exhibits both brittleness and a lack of strength (in the technical sense). For perfectly brittle materials, yield strength and ultimate strength are the same, because they do not experience detectable plastic deformation. The opposite of brittleness is ductility.
The toughness of a material is the maximum amount of energy it can absorb before fracturing, which is different from the amount of force that can be applied. Toughness tends to be small for brittle materials, because elastic and plastic deformations allow materials to absorb large amounts of energy.
Hardness increases with decreasing particle size. This is known as the Hall-Petch relationship. However, below a critical grain-size, hardness decreases with decreasing grain size. This is known as the inverse Hall-Petch effect.
Hardness of a material to deformation is dependent on its microdurability or small-scale shear modulus in any direction, not to any rigidity or stiffness properties such as its bulk modulus or Young's modulus. Stiffness is often confused for hardness. Some materials are stiffer than diamond (e.g. osmium) but are not harder, and are prone to spalling and flaking in squamose or acicular habits.


Mechanisms and theory

The key to understanding the mechanism behind hardness is understanding the metallic microstructure, or the structure and arrangement of the atoms at the atomic level. In fact, most important metallic properties critical to the manufacturing of todays goods are determined by the microstructure of a material. At the atomic level, the atoms in a metal are arranged in an orderly three-dimensional array called a crystal lattice. In reality, however, a given specimen of a metal likely never contains a consistent single crystal lattice. A given sample of metal will contain many grains, with each grain having a fairly consistent array pattern. At an even smaller scale, each grain contains irregularities.
There are two types of irregularities at the grain level of the microstructure that are responsible for the hardness of the material. These irregularities are point defects and line defects. A point defect is an irregularity located at a single lattice site inside of the overall three-dimensional lattice of the grain. There are three main point defects. If there is an atom missing from the array, a vacancy defect is formed. If there is a different type of atom at the lattice site that should normally be occupied by a metal atom, a substitutional defect is formed. If there exists an atom in a site where there should normally not be, an interstitial defect is formed. This is possible because space exists between atoms in a crystal lattice. While point defects are irregularities at a single site in the crystal lattice, line defects are irregularities on a plane of atoms. Dislocations are a type of line defect involving the misalignment of these planes. In the case of an edge dislocation, a half plane of atoms is wedged between two planes of atoms. In the case of a screw dislocation two planes of atoms are offset with a helical array running between them.In glasses, hardness seems to depend linearly on the number of topological constraints acting between the atoms of the network. Hence, the rigidity theory has allowed predicting hardness values with respect to composition.

Dislocations provide a mechanism for planes of atoms to slip and thus a method for plastic or permanent deformation.  Planes of atoms can flip from one side of the dislocation to the other effectively allowing the dislocation to traverse through the material and the material to deform permanently. The movement allowed by these dislocations causes a decrease in the material's hardness.
The way to inhibit the movement of planes of atoms, and thus make them harder, involves the interaction of dislocations with each other and interstitial atoms. When a dislocation intersects with a second dislocation, it can no longer traverse through the crystal lattice. The intersection of dislocations creates an anchor point and does not allow the planes of atoms to continue to slip over one another A dislocation can also be anchored by the interaction with interstitial atoms. If a dislocation comes in contact with two or more interstitial atoms, the slip of the planes will again be disrupted. The interstitial atoms create anchor points, or pinning points, in the same manner as intersecting dislocations.
By varying the presence of interstitial atoms and the density of dislocations, a particular metal's hardness can be controlled. Although seemingly counter-intuitive, as the density of dislocations increases, there are more intersections created and consequently more anchor points. Similarly, as more interstitial atoms are added, more pinning points that impede the movements of dislocations are formed. As a result, the more anchor points added, the harder the material will become.


Fashion is a form of self expression, at a particular period and place and in a specific context, of clothing, footwear, lifestyle, accessories, makeup, hairstyle, and body posture. In its everyday use, the term implies a look defined by the fashion industry as that which is trending. What is called fashion is thus that which is made available and popular by fashion system (industry and media).  
In reaction to increased mass-production of commodities clothing at lower prices and global reach, sustainability has become an urgent issue among politicians, brands, and consumers.


Definitions of fashion
As noted by fashion scholar Susan Kaiser, everyone is forced to appear, that is, there is no unmediated way of being before others. Whether we want it or not, everyone appearing before another is evaluated in relation to dress, their attire. This most commonly means how one looks, what colors, materials and silhouette one wears on the body. Even if the garments are all the same, they will appear different; if the item is washed, folded, mended, or new. 
Fashion is a term that is plagued by its many different uses, and by the unclear application of the concept. For example, the term connotes difference, but also sameness. It signifies the latest distinction, as well as the return of the old. While it may be defined by an insular and esteemed aesthetic elite, who make a look exclusive, this look is often using references from those excluded from making the distinction. 
Whereas a trend often connotes a peculiar aesthetic expression and often lasting shorter than a season, fashion is a distinctive and industry-supported expression traditionally tied to the fashion season and collections. Style is an expression that lasts over many seasons and is often connected to cultural movements and social markers, symbols, class, and culture (ex. Baroque, Rococo, etc.). According to sociologist Pierre Bourdieu, fashion connotes "the latest difference."Even though the terms are often used together, fashion differs from clothing and costumes  "clothing" describes the material and technical garment; "costume" has come to mean fancy-dress or masquerade wear. "Fashion," by contrast, describes the social and temporal system that "activates" dress as a social signifier in a certain time and context. Philosopher Giorgio Agamben connects fashion to the current intensity of the qualitative moment, to the temporal aspect the Greek called kairos, whereas clothing belongs to the quantitative, what the Greek called Chronos.While some exclusive brands may claim the label haute couture, the term is technically limited to members of the Chambre Syndicale de la Haute Couture in Paris. Haute couture is more aspirational; inspired by art and culture, and in most cases, reserved to the economic elite.
Fashion is also a source of art, allowing people to display their own unique taste and styling. Different designers are influenced by outside stimuli and reflect this inspiration in their works. For example, Gucci's 'stained green' jeans may look like a grass stain, but to many others they display purity, freshness and summer.


Clothing fashions

Fashion is a form of expression. Fashion is what people wear in a specific context. If a stranger would appear in this setting, adorning something different, the stranger would be considered "out of fashion."
Early Western travelers, traveling to India, Persia, Turkey, or China, would frequently remark on the absence of change in fashion in those countries. The Japanese shgun's secretary bragged (not completely accurately) to a Spanish visitor in 1609 that Japanese clothing had not changed in over a thousand years. However, there is considerable evidence in Ming China of rapidly changing fashions in Chinese clothing. Changes in costume often took place at times of economic or social change, as occurred in ancient Rome and the medieval Caliphate, followed by a long period without significant changes. In 8th-century Moorish Spain, the musician Ziryab introduced to Crdoba sophisticated clothing-styles based on seasonal and daily fashions from his native Baghdad, modified by his inspiration. Similar changes in fashion occurred in the 11th century in the Middle East following the arrival of the Turks, who introduced clothing styles from Central Asia and the Far East.Additionally, there is a long history of fashion in West Africa. The Cloth was used as a form of currency in trade with the Portuguese and Dutch as early as the 16th Century. Locally produced cloth and cheaper European imports were assembled into new styles to accommodate the growing elite class of West Africans and resident gold and slave traders. There was an exceptionally strong tradition of cloth-weaving in Oyo and the areas inhabited by the Igbo people.


Fashion in Europe and the Western hemisphere
According to scholars, a specific definition of fashion emerges with the rise of capitalism and more liberal societies in Europe during late medieval times. The beginning in Europe of continual and increasingly rapid change in clothing styles can be fairly reliably dated. Historians, including James Laver and Fernand Braudel, date the start of Western fashion in clothing to the middle of the 14th century, though they tend to rely heavily on contemporary imagery and illuminated manuscripts were not common before the fourteenth century. The most dramatic early change in fashion was a sudden drastic shortening and tightening of the male over-garment from calf-length to barely covering the buttocks, sometimes accompanied with stuffing in the chest to make it look bigger. This created the distinctive Western outline of a tailored top worn over leggings or trousers.
The pace of change accelerated considerably in the following century, and women's and men's fashion, especially in the dressing and adorning of the hair, became equally complex. Art historians are, therefore, able to use fashion with confidence and precision to date images, often to within five years, particularly in the case of images from the 15th century. Initially, changes in fashion led to a fragmentation across the upper classes of Europe of what had previously been a very similar style of dressing and the subsequent development of distinctive national styles. These national styles remained very different until a counter-movement in the 17th to 18th centuries imposed similar styles once again, mostly originating from Ancien Rgime France. Though the rich usually led fashion, the increasing affluence of early modern Europe led to the bourgeoisie and even peasants following trends at a distance, but still uncomfortably close for the elites  a factor that Fernand Braudel regards as one of the main motors of changing fashion.

In the 16th century, national differences were at their most pronounced. Ten 16th century portraits of German or Italian gentlemen may show ten entirely different hats. Albrecht Drer illustrated the differences in his actual (or composite) contrast of Nuremberg and Venetian fashions at the close of the 15th century (illustration, right). The "Spanish style" of the late 16th century began the move back to synchronicity among upper-class Europeans, and after a struggle in the mid-17th century, French styles decisively took over leadership, a process completed in the 18th century.Though different textile colors and patterns changed from year to year, the cut of a gentleman's coat and the length of his waistcoat, or the pattern to which a lady's dress was cut, changed more slowly. Men's fashions were primarily derived from military models, and changes in a European male silhouette were galvanized in theaters of European war where gentleman officers had opportunities to make notes of different styles such as the "Steinkirk" cravat or necktie.
Though there had been distribution of dressed dolls from France since the 16th century and Abraham Bosse had produced engravings of fashion in the 1620s, the pace of change picked up in the 1780s with increased publication of French engravings illustrating the latest Paris styles. By 1800, all Western Europeans were dressing alike (or thought they were); local variation became first a sign of provincial culture and later a badge of the conservative peasant.Although tailors and dressmakers were no doubt responsible for many innovations, and the textile industry indeed led many trends, the history of fashion design is generally understood to date from 1858 when the English-born Charles Frederick Worth opened the first authentic haute couture house in Paris. The Haute house was the name established by the government for the fashion houses that met the standards of the industry. These fashion houses have to adhere to standards such as keeping at least twenty employees engaged in making the clothes, showing two collections per year at fashion shows, and presenting a certain number of patterns to costumers. Since then, the idea of the fashion designer as a celebrity in his or her own right has become increasingly dominant.Although fashion can be feminine or masculine, additional trends are androgynous. The idea of unisex dressing originated in the 1960s when designers such as Pierre Cardin and Rudi Gernreich created garments, such as stretch jersey tunics or leggings, meant to be worn by both males and females. The impact of unisex wearability expanded more broadly to encompass various themes in fashion, including androgyny, mass-market retail, and conceptual clothing. The fashion trends of the 1970s, such as sheepskin jackets, flight jackets, duffel coats, and unstructured clothing, influenced men to attend social gatherings without a dinner jacket and to accessorize in new ways. Some men's styles blended the sensuality and expressiveness, and the growing gay-rights movement and an emphasis on youth allowed for a new freedom to experiment with style and with fabrics such as wool crepe, which had previously been associated with women's attire.

The four major current fashion capitals are acknowledged to be Paris, Milan, New York City, and London, which are all headquarters to the most significant fashion companies and are renowned for their major influence on global fashion. Fashion weeks are held in these cities, where designers exhibit their new clothing collections to audiences. A succession of major designers such as Coco Chanel and Yves Saint-Laurent have kept Paris as the center most watched by the rest of the world, although haute couture is now subsidized by the sale of ready-to-wear collections and perfume using the same branding.
Modern Westerners have a vast number of choices in selection of their clothes. What a person chooses to wear can reflect his or her personality or interests. When people who have high cultural status start to wear new or different styles, they may inspire a new fashion trend. People who like or respect these people are influenced by their style and begin wearing similarly styled clothes. 
Fashions may vary considerably within a society according to age, social class, generation, occupation, and geography, and may also vary over time. The terms fashionista and fashion victim refer to someone who slavishly follows current fashions.
In the early 2000s, Asian fashion has become increasingly significant in local and global markets. Countries such as China, Japan, India, and Pakistan have traditionally had large textile industries, which have often been drawn upon by Western designers, but now Asian clothing styles are also gaining influence based on their ideas.


Fashion industry

In its most common use, the term fashion refers to the current expressions on sale through the fashion industry. The global fashion industry is a product of the modern age. In the Western world, tailoring has since medieval times been controlled by guilds, but with the emergence of industrialism, the power of the guilds was undermined. Before the mid-19th century, most clothing was custom-made. It was handmade for individuals, either as home production or on order from dressmakers and tailors. By the beginning of the 20th century, with the rise of new technologies such as the sewing machine, the rise of global trade, the development of the factory system of production, and the proliferation of retail outlets such as department stores, clothing became increasingly mass-produced in standard sizes and sold at fixed prices.
Although the fashion industry developed first in Europe and America, as of 2017, it is an international and highly globalized industry, with clothing often designed in one country, manufactured in another, and sold worldwide. For example, an American fashion company might source fabric in China and have the clothes manufactured in Vietnam, finished in Italy, and shipped to a warehouse in the United States for distribution to retail outlets internationally. 
The fashion industry was for a long time one of the largest employers in the United States, and it remains so in the 21st century. However, U.S. employment in fashion began to decline considerably as production increasingly moved overseas, especially to China. Because data on the fashion industry typically are reported for national economies and expressed in terms of the industry's many separate sectors, aggregate figures for the world production of textiles and clothing are difficult to obtain. However, by any measure, the clothing industry accounts for a significant share of world economic output.
The fashion industry consists of four levels:

The production of raw materials, principally Fiber, and textiles but also leather and fur.
The production of fashion goods by designers, manufacturers, contractors, and others.
Retail sales.
Various forms of advertising and promotion.The levels of focus in the fashion industry consist of many separate but interdependent sectors. These sectors include: Textile Design and Production, Fashion Design and Manufacturing, Fashion Retailing, Marketing and Merchandising, Fashion Shows, and Media and Marketing. Each sector is devoted to the goal of satisfying consumer demand for apparel under conditions that enable participants in the industry to operate at a profit.


Fashion trend

A fashion trend signifies a specific look or expression that is spread across a population at a specific time and place. A trend is considered a more ephemeral look, not defined by the seasons can collections released by the fashion industry. A trend can thus emerge from street style, across cultures, from influencers and celebrities. 
Fashion trends are influenced by several factors, including cinema, celebrities, climate, creative explorations, innovations, designs, political, economic, social, and technological. Examining these factors is called a PEST analysis. Fashion forecasters can use this information to help determine the growth or decline of a particular trend.


Social influences

Fashion is inherently a social phenomenon. A person cannot have a fashion by oneself, but for something to be defined as fashion there needs to be dissemination and followers. This dissemination can take several forms; from the top down ("trickle-down") to bottom up ("bubble up"), or transversally across cultures and through viral memes and media.
Fashion relates to the social and cultural context of an environment. According to Matika, "Elements of popular culture become fused when a person's trend is associated with a preference for a genre of musiclike music, news or literature, fashion has been fused into everyday lives." Fashion is not only seen as purely aesthetic; fashion is also a medium for people to create an overall effect and express their opinions and overall art.
This mirrors what performers frequently accomplish through music videos. In the music video Formation by Beyonc, according to Carlos, "The pop star pays homage to her Creole roots.... tracing the roots of the Louisiana cultural nerve center from the post-abolition era to present day, Beyonc catalogs the evolution of the city's vibrant style and its tumultuous history all at once. Atop a New Orleans police car in a red-and-white Gucci high-collar dress and combat boots, she sits among the ruins of Hurricane Katrina, immediately implanting herself in the biggest national debate on police brutality and race relations in modern day."
The annual or seasonal runway show is a reflection of fashion trends and a designer's inspirations. For designers like Vivienne Westwood, runway shows are a platform for her voice on politics and current events. For her AW15 menswear show, according to Water, "where models with severely bruised faces channeled eco-warriors on a mission to save the planet." Another recent example is a staged feminist protest march for Chanel's SS15 show, rioting models chanting words of empowerment with signs like "Feminist but feminine" and "Ladies first." According to Water, "The show tapped into Chanel's long history of championing female independence: founder Coco Chanel was a trailblazer for liberating the female body in the post-WWI era, introducing silhouettes that countered the restrictive corsets then in favour."


Economic influences


Circular economy
With increasing environmental awareness, the economic imperative to "Spend now, think later" is getting increasingly scrutinized. Today's consumer tends to be more mindful about consumption, looking for just enough and better, more durable options. People have also become more conscious of the impact their everyday consumption has on the environment and society, and these initiatives are often described as a move towards sustainable fashion, yet critics argue a circular economy based on growth is an oxymoron, or an increasing spiral of consumption, rather than a utopian cradle-to-cradle circular solution.
In today's linear economical system, manufacturers extract resources from the earth to make products that will soon be discarded in landfills, on the other hand, under the circular model, the production of goods operates like systems in nature, where the waste and demise of a substance becomes the food and source of growth for something new. Companies such as MUD Jeans, which is based in the Netherlands employs a leasing scheme for jeans. This Dutch company "represents a new consuming philosophy that is about using instead of owning," according to MUD's website. The concept also protects the company from volatile cotton prices. Consumers pay 7.50 a month for a pair of jeans; after a year, they can return the jeans to Mud, trade them for a new pair and start another year-long lease, or keep them. MUD is responsible for any repairs during the lease period. Another ethical fashion company, Patagonia set up the first multi-seller branded store on EBay in order to facilitate secondhand sales; consumers who take the Common Threads pledge can sell in this store and have their gear listed on Patagonia.com's "Used Gear" section.


China's domestic spending
Consumption as a share of gross domestic product in China has fallen for six decades, from 76 percent in 1952 to 28 percent in 2011. China plans to reduce tariffs on a number of consumer goods and expand its 72-hour transit visa plan to more cities in an effort to stimulate domestic consumption.The announcement of import tax reductions follows changes in June 2015, when the government cut the tariffs on clothing, cosmetics and various other goods by half. Among the changes  easier tax refunds for overseas shoppers and accelerated openings of more duty-free shops in cities covered by the 72-hour visa scheme. The 72-hour visa was introduced in Beijing and Shanghai in January 2013 and has been extended to 18 Chinese cities.According to reports at the same time, Chinese consumer spending in other countries such as Japan has slowed even though the yen has dropped. There is clearly a trend in the next 5 years that the domestic fashion market will show an increase.
China is an interesting market for fashion retail as Chinese consumers' motivation to shop for fashion items are unique from Western Audiences. Demographics have limited association with shopping motivation, with occupation, income and education level having no impact; unlike in Western Countries. Chinese high-street shoppers prefer adventure and social shopping, while online shoppers are motivated by idea shopping. Another difference is how gratification and idea shopping influence spending over 1k per month on fashion items, and regular spending influenced by value shopping.


Marketing


Market research
Consumers of different groups have varying needs and demands. Factors taken into consideration when thinking of consumers' needs include key demographics.
To understand consumers' needs and predict fashion trends, fashion companies have to do market research There are two research methods: primary and secondary. Secondary methods are taking other information that has already been collected, for example using a book or an article for research. Primary research is collecting data through surveys, interviews, observation, and/or focus groups. Primary research often focuses on large sample sizes to determine customer's motivations to shop.The benefits of primary research are specific information about a fashion brand's consumer is explored. Surveys are helpful tools; questions can be open-ended or closed-ended. Negative factor surveys and interviews present is that the answers can be biased, due to wording in the survey or on face-to-face interactions. Focus groups, about 8 to 12 people, can be beneficial because several points can be addressed in depth. However, there are drawbacks to this tactic, too. With such a small sample size, it is hard to know if the greater public would react the same way as the focus group. Observation can really help a company gain insight on what a consumer truly wants. There is less of a bias because consumers are just performing their daily tasks, not necessarily realizing they are being observed. For example, observing the public by taking street style photos of people, the consumer did not get dressed in the morning knowing that would have their photo taken necessarily. They just wear what they would normally wear. Through observation patterns can be seen, helping trend forecasters know what their target market needs and wants.
Knowing the needs of consumers will increase fashion companies' sales and profits. Through research and studying the consumers' lives the needs of the customer can be obtained and help fashion brands know what trends the consumers are ready for.


Symbolic consumption
Consumption is driven not only by need, the symbolic meaning for consumers is also a factor. Consumers engaging in symbolic consumption may develop a sense of self over an extended period of time as various objects are collected as part of the process of establishing their identity and, when the symbolic meaning is shared in a social group, to communicate their identity to others. For teenagers, consumption plays a role in distinguishing the child self from the adult. Researchers have found that the fashion choices of teenagers are used for self-expression and also to recognize other teens who wear similar clothes. The symbolic association of clothing items can link individuals' personality and interests, with music as a prominent factor influencing fashion decisions.


Political influences

Political figures have played a central role in the development of fashion, at least since the time of French king Louis XIV. For example, First Lady Jacqueline Kennedy was a fashion icon of the early 1960s. Wearing Chanel suits, structural Givenchy shift dresses, and soft color Cassini coats with large buttons, she inspired trends of both elegant formal dressing and classic feminine style.Cultural upheavals have also had an impact on fashion trends. For example, during the 1960s, the U.S. economy was robust, the divorce rate was increasing, and the government approved the birth control pill. These factors inspired the younger generation to rebel against entrenched social norms. The civil rights movement, a struggle for social justice and equal opportunity for Blacks, and the women's liberation movement, seeking equal rights and opportunities and greater personal freedom for women, were in full bloom. In 1964, the leg-baring mini-skirt was introduced and became a white-hot trend. Fashion designers then began to experiment with the shapes of garments: loose sleeveless dresses, micro-minis, flared skirts, and trumpet sleeves. Fluorescent colors, print patterns, bell-bottom jeans, fringed vests, and skirts became de rigueur outfits of the 1960s. Concern and protest over U.S involvement in the failing Vietnam War also influenced fashion . Camouflage patterns in military clothing, developed to help military personnel be less visible to enemy forces, seeped into streetwear designs in the 1960s. Camouflage trends have disappeared and resurfaced several times since then, appearing in high fashion iterations in the 1990s. Designers such as Valentino, Dior, and Dolce & Gabbana combined camouflage into their runway and ready-to-wear collections.  Today, variations of camouflage, including pastel shades, in every article of clothing or accessory, continue to enjoy popularity.


Technology influences
Today, technology plays a sizable role in society, and technological influences are correspondingly increasing within the realm of fashion. Wearable technology has become incorporated; for example, clothing constructed with solar panels that charge devices and smart fabrics that enhance wearer comfort by changing color or texture based on environmental changes.  3D printing technology has influenced designers such as Iris van Herpen and Kimberly Ovitz. As the technology evolves, 3D printers will become more accessible to designers and eventually, consumers  these could potentially reshape design and production in the fashion industry entirely.
Internet technology, enabling the far reaches of online retailers and social media platforms, has created previously unimaginable ways for trends to be identified, marketed, and sold immediately. Trend-setting styles are easily displayed and communicated online to attract customers. Posts on Instagram or Facebook can quickly increase awareness about new trends in fashion, which subsequently may create high demand for specific items or brands, new "buy now button" technology can link these styles with direct sales.
Machine vision technology has been developed to track how fashions spread through society. The industry can now see the direct correlation on how fashion shows influence street-chic outfits. Effects such as these can now be quantified and provide valuable feedback to fashion houses, designers, and consumers regarding trends.


Media

The media plays a significant role when it comes to fashion. For instance, an important part of fashion is fashion journalism. Editorial critique, guidelines, and commentary can be found on television and in magazines, newspapers, fashion websites, social networks, and fashion blogs. In recent years, fashion blogging and YouTube videos have become a major outlet for spreading trends and fashion tips, creating an online culture of sharing one's style on a website or Instagram account. Through these media outlets, readers and viewers all over the world can learn about fashion, making it very accessible. In addition to fashion journalism, another media platform that is important in fashion industry is advertisement. Advertisements provide information to audiences and promote the sales of products and services. The fashion industry utilizes advertisements to attract consumers and promote its products to generate sales. A few decades ago when technology was still underdeveloped, advertisements heavily relied on radio, magazines, billboards, and newspapers. These days, there are more various ways in advertisements such as television ads, online-based ads using internet websites, and posts, videos, and live streaming in social media platforms.


Fashion in printed media
There are two subsets of print styling: editorial and lifestyle. Editorial styling is the high - fashion styling seen in fashion magazines, and this tends to be more artistic and fashion-forward. Lifestyle styling focuses on a more overtly commercial goal, like a department store advertisement, a website, or an advertisement where fashion is not what's being sold but the models hired to promote the product in the photo. The dressing practices of the powerful has traditionally been mediated through art and the practices of the courts. The looks of the French court were disseminated through prints, from the 16th century, but became prevalent with the promotion of the centralized court around king Louis XIV, and the style that became known under his name. At the beginning of the 20th century, fashion magazines began to include photographs of various fashion designs and became even more influential than in the past. In cities throughout the world these magazines were greatly sought after and had a profound effect on public taste in clothing. Talented illustrators drew exquisite fashion plates for the publications which covered the most recent developments in fashion and beauty. Perhaps the most famous of these magazines was La Gazette du Bon Ton, which was founded in 1912 by Lucien Vogel and regularly published until 1925 (with the exception of the war years).

Vogue, founded in the United States in 1892, has been the longest-lasting and most successful of the hundreds of fashion magazines that have come and gone. Increasing affluence after World War II and, most importantly, the advent of cheap color printing in the 1960s, led to a huge boost in its sales and heavy coverage of fashion in mainstream women's magazines, followed by men's magazines in the 1990s. One such example of Vogue's popularity is the younger version, Teen Vogue, which covers clothing and trends that are targeted more toward the "fashionista on a budget". Haute couture designers followed the trend by starting ready-to-wear and perfume lines which are heavily advertised in the magazines and now dwarf their original couture businesses. A recent development within fashion print media is the rise of text-based and critical magazines which aim to prove that fashion is not superficial, by creating a dialogue between fashion academia and the industry. Examples of this development are: Fashion Theory (1997), Fashion Practice: The Journal of Design, Creative Process & the Fashion Industry (2008), and Vestoj (2009). 


Fashion in television
Television coverage began in the 1950s with small fashion features. In the 1960s and 1970s, fashion segments on various entertainment shows became more frequent, and by the 1980s, dedicated fashion shows such as Fashion Television started to appear. FashionTV was the pioneer in this undertaking and has since grown to become the leader in both Fashion Television and new media channels. The Fashion Industry is beginning to promote their styles through Bloggers on social media's. Vogue specified Chiara Ferragni as "blogger of the moment" due to the rises of followers through her Fashion Blog, that became popular.A few days after the 2010 Fall Fashion Week in New York City came to a close, The New Islander's Fashion Editor, Genevieve Tax, criticized the fashion industry for running on a seasonal schedule of its own, largely at the expense of real-world consumers. "Because designers release their fall collections in the spring and their spring collections in the fall, fashion magazines such as Vogue always and only look forward to the upcoming season, promoting parkas come September while issuing reviews on shorts in January", she writes. "Savvy shoppers, consequently, have been conditioned to be extremely, perhaps impractically, farsighted with their buying."The fashion industry has been the subject of numerous films and television shows, including the reality show Project Runway and the drama series Ugly Betty. Specific fashion brands have been featured in film, not only as product placement opportunities, but as bespoke items that have subsequently led to trends in fashion.Videos in general have been very useful in promoting the fashion industry. This is evident not only from television shows directly spotlighting the fashion industry, but also movies, events and music videos which showcase fashion statements as well as promote specific brands through product placements.


Controversial advertisements in fashion industry


Racism in fashion advertisements
There are some fashion advertisements that were accused of racism and led to boycotts from the customers. Globally known, Swedish fashion brand H&M faced this issue with one of its children's wear advertisements in 2018. A black child wearing a hoodie with a slogan written as "coolest monkey in the jungle" right at the center was featured in the ad. When it was released, it immediately became controversial and even led to a boycott. A lot of people including celebrities posted on social media about their resentments towards H&M and refusal to work with and buy its products. H&M issued a statement saying "we apologise to anyone this may have offended", which seemed insincere to some. 
Another fashion advertisement regarding racism is from GAP, an American worldwide clothing brand. GAP collaborated with Ellen DeGeneres in 2016 for the advertisement. It features playful, four young girls where a tall white girl is leaning with her arm on a shorter black girl's head. When this ad was released, some viewers harshly criticized that it underlies passive racism. A representative from The Root, black culture magazine commented on the ad that it portrays the message that black people are undervalued and seen like props for white people to look better. There were different points of views on this issue, some saying that people are being too sensitive, and some getting offended. Regardless of various views and thoughts, GAP replaced the ad to different image and apologized to critics.


Sexism in fashion advertisements
Many fashion brands have published ads that were too provocative and sexy to attract customers attention. British high fashion brand, Jimmy Choo, was blamed for having sexism in its ad which featured a female British mode wearing the brand's boots. In this two-minute ad, men whistle at a model, walking on the street with red, sleeveless mini dress. This ad gained much backlash and criticism by the viewers since sexual harassment and misconduct were a huge issue during this time and even till now. Many people showed their dismay through social media posts, leading Jimmy Choo to pull down the ad from social media platforms.French luxury fashion brand Yves Saint Laurent also faced this issue with its print ad shown in Paris in 2017. A female model is wearing a fishnet tights with roller-skate stilettos, almost lying down with her legs opened in front of the camera. This advertisement brought harsh comments from the viewers and French advertising organization directors for going against the advertising codes related to "respect for decency, dignity and those prohibiting submission, violence or dependence, as well as the use of stereotypes." They even said that this ad is causing "mental harm to adolescents." Lot of sarcastic comments were made in social media about the ad and the poster was removed from the city.


Public relations and social media

Fashion public relations involves being in touch with a company's audiences and creating strong relationships with them, reaching out to media and initiating messages that project positive images of the company. Social media plays an important role in modern-day fashion public relations; enabling practitioners to reach a wide range of consumers through various platforms.Building brand awareness and credibility is a key implication of good public relations. In some cases, hype is built about new designers' collections before they are released into the market, due to the immense exposure generated by practitioners. Social media, such as blogs, micro blogs, podcasts, photo and video sharing sites have all become increasingly important to fashion public relations. The interactive nature of these platforms allows practitioners to engage and communicate with the public in real-time, and tailor their clients' brand or campaign messages to the target audience. With blogging platforms such as Instagram, Tumblr, WordPress, and other sharing sites, bloggers have emerged as expert fashion commentators, shaping brands and having a great impact on what is on trend. Women in the fashion public relations industry such as Sweaty Betty PR founder Roxy Jacenko and Oscar de la Renta's PR girl Erika Bearman, have acquired copious followers on their social media sites, by providing a brand identity and a behind the scenes look into the companies they work for.
Social media is changing the way practitioners deliver messages, as they are concerned with the media, and also customer relationship building. PR practitioners must provide effective communication among all platforms, in order to engage the fashion public in an industry socially connected via online shopping. Consumers have the ability to share their purchases on their personal social media pages (such as Facebook, Twitter, Instagram, etc.), and if practitioners deliver the brand message effectively and meet the needs of its public, word-of-mouth publicity will be generated and potentially provide a wide reach for the designer and their products.


Fashion in political activism
As fashion concerns people, and  signifies social hierarchies, fashion intersects with politics and the social organization of societies. Whereas haute couture and business suits are associated by people in power, also groups aiming to challenge the political order also use clothes to signal their position. The explicit use of fashion as a form of activism, is usually referred to as "fashion activism."
Whereas fashion designers and brands have traditionally kept themselves out of political conflicts, there has been a movement in the industry towards taking more explicit positions across the political spectrum. From maintaining a rather apolitical stance, designers and brands today engage more explicitly in current debates.For example, considering the U.S.'s political climate in the surrounding months of the 2016 presidential election, during 2017 fashion weeks in London, Milan, New York, Paris and So Paulo amongst others, many designers took the opportunity to take political stances leveraging their platforms and influence to reach their customers. This has also led to some confusion around democratic values, as fashion is not always the most inclusive platform for political debate, but a one-way broadcast of top-down messages.
When taking an explicit political stance, designers appear to favor issues around which can be phrased using clear language with virtuous undertones. For example, aiming to "amplify a greater message of unity, inclusion, diversity, and feminism in a fashion space", designer Mara Hoffman invited the founders of the Women's March on Washington to open her show which featured modern silhouettes of utilitarian wear, described by critics as "Made for a modern warrior" and "Clothing for those who still have work to do". Prabal Gurung debuted his collection of T-shirts featuring slogans such as "The Future is Female", "We Will Not Be Silenced", and "Nevertheless She Persisted", with proceeds going to the ACLU, Planned Parenthood, and Gurung's own charity, "Shikshya Foundation Nepal". Similarly, The Business of Fashion launched the #TiedTogether movement on Social Media, encouraging member of the industry from editors to models, to wear a white bandana advocating for "unity, solidarity, and inclusiveness during fashion week".Fashion may be used to promote a cause, such as to promote healthy behavior, to raise money for a cancer cure, or to raise money for local charities such as the Juvenile Protective Association or a children's hospice.One fashion cause is trashion, which is using trash to make clothes, jewelry, and other fashion items in order to promote awareness of pollution. There are a number of modern trashion artists such as Marina DeBris, Ann Wizer, and Nancy Judd. Other designers have used DIY fashions, in the tradition of the punk movement, to address elitism in the industry to promote more inclusion and diversity.


Anthropological perspective
From an academic lens, the sporting of various fashions has been seen as a form of fashion language, a mode of communication that produced various fashion statements, using a grammar of fashion. This is a perspective promoted in the work of influential French philosopher and semiotician Roland Barthes.
Anthropology, the study of culture and of human societies, examines fashion by asking why certain styles are deemed socially appropriate and others are not. From the theory of interactionism, a certain practice or expression is chosen by those in power in a community, and that becomes "the fashion" as defined at a certain time by the people under influence of those in power. If a particular style has a meaning in an already occurring set of beliefs, then that style may have a greater chance of become fashion.According to cultural theorists Ted Polhemus and Lynn Procter, one can describe fashion as adornment, of which there are two types: fashion and anti-fashion. Through the capitalization and commoditization of clothing, accessories, and shoes, etc., what once constituted anti-fashion becomes part of fashion as the lines between fashion and anti-fashion are blurred, as expressions that were once outside the changes of fashion are swept along with trends to signify new meanings. Examples range from how elements from ethnic dress becomes part of a trend and appear on catwalks or street cultures, for example how tattoos travel from sailors, laborers and criminals to popular culture.
To cultural theorist Malcolm Bernard, fashion and anti-fashion differ as follows: Anti-fashion is fixed and changes little over time. Anti-fashion varies depending on the cultural or social group one is associated with or where one lives, but within that group or locality the style changes little. Fashion is the exact opposite of anti-fashion. Fashion can change (evolve) very quickly
and is not affiliated with one group or area of the world but spreads throughout the world wherever people can communicate easily with each other. For example, Queen Elizabeth II's 1953 coronation gown is an example of anti-fashion because it is traditional and does not change over any period, whereas a gown from fashion-designer Dior's collection of 1953 is fashion because Dior's style changes every season as Dior comes up with a new gown to replace the old one. In the Dior gown the length, cut, fabric, and embroidery of the gown change from season to season. Anti-fashion is concerned with maintaining the status quo while fashion is concerned with social mobility. Time is expressed in terms of continuity in anti-fashion and as change in fashion. Fashion has changing modes of adornment while anti-fashion has fixed modes of adornment. Indigenous and peasant modes of adornment exemplify anti-fashion, as they are not primarily used for manifesting social hierarchies while also allowing for social mobility. From this theoretical lens, change in fashion is part of the larger industrial system and is structured by the powerful actors in this system to be a deliberate change in style, promoted through the channels influenced by the industry (such as paid ads etc).


Anthropology of dress in Indonesia
The change from anti-fashion to fashion because of the influence of western consumer-driven culture can be seen in eastern Indonesia. The ikat textiles of the Ngada area of eastern Indonesia are changing because of modernization and development. Traditionally, in the Ngada area there was no idea similar to that of the Western idea of fashion, but anti-fashion in the form of traditional textiles and ways to adorn oneself were widely popular. Textiles in Indonesia have played many roles for the local people. Textiles defined a person's rank and status; certain textiles indicated being part of the ruling class. People expressed their ethnic identity and social hierarchy through textiles. Because some Indonesians bartered ikat textiles for food, the textiles constituted economic goods, and as some textile-design motifs had spiritual religious meanings, textiles were also a way to communicate religious messages.In eastern Indonesia, both the production and use of traditional textiles have been transformed as the production, use and value associated with textiles have changed due to modernization. In the past, women produced the textiles either for home consumption or to trade with others. Today, this has changed as most textiles are not being produced at home. Western goods are considered modern and are valued more than traditional goods, including the sarong, which retain a lingering association with colonialism. Now, sarongs are used only for rituals and ceremonial occasions, whereas western clothes are worn to church or government offices. Civil servants working in urban areas are more likely than peasants to make the distinction between western and traditional clothes. Following Indonesia's independence from the Dutch in the 1940s, people increasingly started buying factory-made shirts and sarongs. In textile-producing areas the growing of cotton and the production of naturally colored thread became obsolete. Traditional motifs on textiles are no longer considered the property of a certain social class or age-group. Wives of government officials are promoting the use of traditional textiles in the form of western garments such as skirts, vests and blouses. This trend is also being followed by the general populace, and whoever can afford to hire a tailor is doing so to stitch traditional ikat textiles into western clothes. Thus traditional textiles are now fashion goods and are no longer confined to the black, white and brown colour-palette, but come in an array of colours. Traditional textiles are also being used in interior decoration and to make handbags, wallets and other accessories, which are considered fashionable by civil servants and their families. There is also a booming tourist trade in the eastern Indonesian city of Kupang where international and domestic tourists are eager to purchase traditionally printed western goods.The use of traditional textiles for fashion is becoming big business in eastern Indonesia, but these traditional textiles are losing their ethnic identity-markers and are being used as an item of fashion.


Intellectual property
In the fashion industry, intellectual property is not enforced as it is within the film industry and music industry. Robert Glariston, an intellectual property expert, mentioned in a fashion seminar held in LA that "Copyright law regarding clothing is a current hot-button issue in the industry. We often have to draw the line between designers being inspired by a design and those outright stealing it in different places." To take inspiration from others' designs contributes to the fashion industry's ability to establish clothing trends. For the past few years, WGSN has been a dominant source of fashion news and forecasts in encouraging fashion brands worldwide to be inspired by one another. Enticing consumers to buy clothing by establishing new trends is, some have argued, a key component of the industry's success. Intellectual property rules that interfere with this process of trend-making would, in this view, be counter-productive. On the other hand, it is often argued that the blatant theft of new ideas, unique designs, and design details by larger companies is what often contributes to the failure of many smaller or independent design companies.
Since fakes are distinguishable by their poorer quality, there is still a demand for luxury goods, and as only a trademark or logo can be copyrighted, many fashion brands make this one of the most visible aspects of the garment or accessory. In handbags, especially, the designer's brand may be woven into the fabric (or the lining fabric) from which the bag is made, making the brand an intrinsic element of the bag.
In 2005, the World Intellectual Property Organization (WIPO) held a conference calling for stricter intellectual property enforcement within the fashion industry to better protect small and medium businesses and promote competitiveness within the textile and clothing industries.


African-Americans in Fashion
African-Americans have used fashion through the years, to express themselves and their ideas. It has grown and developed with time. African-American influencers often have been known to start trends though modern-day social media, and even in past years they have been able to reach others with their fashion and style.


Modern Day Fashion
Celebrities like Rihanna, Lupita Nyong'o, Zendaya, and Michelle Obama have been a few of the many fashion idols in the black female community. For men, Pharrell Williams, Kanye West, and Ice Cube have also helped define modern-day fashion for black men. Today's fashion scene is not just clothes, but also hair and makeup. Recent trends have included the embracing of natural hair, the traditional clothing worn with modern clothing, or traditional patterns used in modern clothing styles. All of these trends come with the long-existing and persevering movement of "Black is Beautiful".


Early American Fashion
In the mid to end of the 1900s, African American style changed and developed with the times. Around the 1950s is really when the black community was able to create their own distinct styles. The term "Sunday attire" was coined, communities emphasized "Correct" dress, it was especially important when "stepping out" for social occasions with community members, a habit that continues in the early 2000s. Hair-dos and hairstyles also became a fashion statement, for example the "conk" which is hair that is slightly flattened and waved. Afros also emerged and they were often used to symbolize the rejection of white beauty standards at the time. Around the 1970s is when flashy costumes began to appear and black artists really started to define their presences through fashion. Around this time is also when movements started using fashion as one of their outlets.


Movements using Fashion


Civil Rights Movement
Black activists and supporters used fashion to express their solidarity and support of this civil rights movement. Supporters adorned symbolic clothing, accessories and hairstyles, usually native to Africa. Politics and fashion were fused together during this time and the use of these symbolic fashion statements sent a message to America and the rest of the world that African Americans were proud of their heritage. They aimed to send an even stronger message that black is beautiful and they were not afraid to embrace their identities. An example would the Kente cloth, it is a brightly colored strip of cloth that is stitched and woven together to create different accessories. This woven cloth of brightly colored strips of fabric became a strong symbolic representation of pride in African identity for African Americans of the 1960s and later. It was developed into what is called a dashiki, a flowing, loose-fitting, tunic-style shirt. This cloth became one of the most notorious symbols of this revolution.


Black Panther Party
The Black Panther Party (BPP) was an essential piece of the Black Power movement that allowed members that were involved to advocate for the African American race in different subjects like equality and politics. The BPP members wore a very distinctive uniform: a black leather jacket, black pants, light blue shirts, a black beret, an afro, dark sunglasses, and usually a fist in the air. Their image gave off a very militant like feel to it. This notable uniform was established in 1996, but a different uniform was still in place before; just the sunglasses and leather jackets. Each member wore this uniform at events, rallies, and in their day-today life. Very few members changed the essential parts of the outfit, but some added personal touches such as necklaces or other jewelry that was usually were a part of African culture. The Black Panther uniform did succeed in intimidating enemies and onlookers and clearly sent a message of black pride and power even though the initial intention of this party was to communicate solidarity among the Black Panther Party members.


Colorism in Fashion
Since the 1970s, fashion models of color, especially black men and women, have experienced an increase in discrimination in the fashion industry. In the years from 1970 to 1990, black designers and models were very successful, but as the 1990s came to an end, the fashion aesthetic changed and it did not include black models or designers. In today's fashion, black models, influencers, and designers account for one of the smallest percentages of the industry. There are many theories about this lack of diversity, that it can be attributed to the economic differences usually associated with race and class, or it can reflect the differences in arts schooling given to mostly black populated schools, and also blatant racism.


Statistics
A report from New York Fashion (Spring 2015) week found that while 79.69% of models on the runway were white, only 9.75% of models were black, 7.67% were Asian, and 2.12% were Latina. The lack of diversity also accounts for not only designers but models too, out of four hundred and seventy members of The Council of Fashion Designers of America (CFDA) only twelve of the members are black. From the same study on New York Fashion Week, it was shown that only 2.7% of the 260 designers presented were black men, and an even smaller percentage were black female designers. Even the relationship between independent designers and retailers can show the racial gap, only 1% of designers stocked at department stores being people of color. It was also found that in editorial spreads, over eighty percent of models pictured were white and only nine percent were black models. These numbers have stayed stagnant over the past few years.


Tokenism
Many fashion designers have come under fire over the years for what is known as tokenism. Designer or editors will add one or two members on an underrepresented group to help them appear as inclusive and diverse, and to also help them give the illusion that they have equality. This idea of tokenism helps designers avoid accusations of racism, sexism, body shaming, etc.


Cultural Appropriation
There are many examples of cultural appropriation in fashion. In many instances, designers can be found using aspects of culture inappropriately, in most cases taking traditional clothing from middle eastern, African, and Hispanic culture and adding it to their runway fashion. Some examples are in a 2018 Gucci runway show, white models wore Sikh headdresses, causing a lot of backlash. Victoria's secret was also under fire for putting traditional native headdresses on their models during a lingerie runway show. Marc Jacobs sent down models sporting dreadlocks in his spring 2017 New York Fashion Week show, also facing immense criticism.


A language is a structured system of communication used by humans, including speech (spoken language), gestures (sign language) and writing. Most languages have a writing system composed of glyphs to inscribe the original sound or gesture and its meaning.The scientific study of language is called linguistics. Critical examinations of languages, such as philosophy of language, the relationships between language and thought, etc, such as how words represent experience, have been debated at least since Gorgias and Plato in ancient Greek civilization. Thinkers such as Rousseau (1712  1778) have debated that language originated from emotions, while others like Kant (1724 1804), have held that languages originated from rational and logical thought. Twentieth century philosophers such as Wittgenstein (1889  1951) argued that philosophy is really the study of language itself. Major figures in contemporary linguistics of these times include Ferdinand de Saussure and Noam Chomsky.
Estimates of the number of human languages in the world vary between 5,000 and 7,000. However, any precise estimate depends on the arbitrary distinction (dichotomy) between languages and dialect. Natural languages are spoken or signed, but any language can be encoded into secondary media using auditory, visual, or tactile stimuli  for example, in writing, whistling, signing, or braille. In other words, human language is modality-independent, but written or signed language is the way to inscribe or encode the natural human speech or gestures. Depending on philosophical perspectives regarding the definition of language and meaning, when used as a general concept, "language" may refer to the cognitive ability to learn and use systems of complex communication, or to describe the set of rules that makes up these systems, or the set of utterances that can be produced from those rules. All languages rely on the process of semiosis to relate signs to particular meanings. Oral, manual and tactile languages contain a phonological system that governs how symbols are used to form sequences known as words or morphemes, and a syntactic system that governs how words and morphemes are combined to form phrases and utterances.
Human language has the properties of productivity and displacement, and relies on social convention and learning. Its complex structure affords a much wider range of expressions than any known system of animal communication. Language is thought to have originated when early hominins started gradually changing their primate communication systems, acquiring the ability to form a theory of other minds and a shared intentionality. This development is sometimes thought to have coincided with an increase in brain volume, and many linguists see the structures of language as having evolved to serve specific communicative and social functions. Language is processed in many different locations in the human brain, but especially in Broca's and Wernicke's areas. Humans acquire language through social interaction in early childhood, and children generally speak fluently by approximately three years old. The use of language is deeply entrenched in human culture. Therefore, in addition to its strictly communicative uses, language also has many social and cultural uses, such as signifying group identity, social stratification, as well as social grooming and entertainment.
Languages evolve and diversify over time, and the history of their evolution can be reconstructed by comparing modern languages to determine which traits their ancestral languages must have had in order for the later developmental stages to occur. A group of languages that descend from a common ancestor is known as a language family; in contrast, a language that has been demonstrated to not have any living or non-living relationship with another language is called a language isolate. There are also many unclassified languages whose relationships have not been established, and spurious languages may have not existed at all. Academic consensus holds that between 50% and 90% of languages spoken at the beginning of the 21st century will probably have become extinct by the year 2100.


Definitions

The English word language derives ultimately from Proto-Indo-European *dnwhs "tongue, speech, language" through Latin lingua, "language; tongue", and Old French language. The word is sometimes used to refer to codes, ciphers, and other kinds of artificially constructed communication systems such as formally defined computer languages used for computer programming. Unlike conventional human languages, a formal language in this sense is a system of signs for encoding and decoding information. This article specifically concerns the properties of natural human language as it is studied in the discipline of linguistics.
As an object of linguistic study, "language" has two primary meanings: an abstract concept, and a specific linguistic system, e.g. "French". The Swiss linguist Ferdinand de Saussure, who defined the modern discipline of linguistics, first explicitly formulated the distinction using the French word langage for language as a concept, langue as a specific instance of a language system, and parole for the concrete usage of speech in a particular language.When speaking of language as a general concept, definitions can be used which stress different aspects of the phenomenon. These definitions also entail different approaches and understandings of language, and they also inform different and often incompatible schools of linguistic theory. Debates about the nature and origin of language go back to the ancient world. Greek philosophers such as Gorgias and Plato debated the relation between words, concepts and reality. Gorgias argued that language could represent neither the objective experience nor human experience, and that communication and truth were therefore impossible. Plato maintained that communication is possible because language represents ideas and concepts that exist independently of, and prior to, language.During the Enlightenment and its debates about human origins, it became fashionable to speculate about the origin of language. Thinkers such as Rousseau and Herder argued that language had originated in the instinctive expression of emotions, and that it was originally closer to music and poetry than to the logical expression of rational thought. Rationalist philosophers such as Kant and Descartes held the opposite view. Around the turn of the 20th century, thinkers began to wonder about the role of language in shaping our experiences of the world  asking whether language simply reflects the objective structure of the world, or whether it creates concepts that it in turn impose on our experience of the objective world. This led to the question of whether philosophical problems are really firstly linguistic problems. The resurgence of the view that language plays a significant role in the creation and circulation of concepts, and that the study of philosophy is essentially the study of language, is associated with what has been called the linguistic turn and philosophers such as Wittgenstein in 20th-century philosophy. These debates about language in relation to meaning and reference, cognition and consciousness remain active today.


Mental faculty, organ or instinct
One definition sees language primarily as the mental faculty that allows humans to undertake linguistic behaviour: to learn languages and to produce and understand utterances. This definition stresses the universality of language to all humans, and it emphasizes the biological basis for the human capacity for language as a unique development of the human brain. Proponents of the view that the drive to language acquisition is innate in humans argue that this is supported by the fact that all cognitively normal children raised in an environment where language is accessible will acquire language without formal instruction. Languages may even develop spontaneously in environments where people live or grow up together without a common language; for example, creole languages and spontaneously developed sign languages such as Nicaraguan Sign Language. This view, which can be traced back to the philosophers Kant and Descartes, understands language to be largely innate, for example, in Chomsky's theory of Universal Grammar, or American philosopher Jerry Fodor's extreme innatist theory. These kinds of definitions are often applied in studies of language within a cognitive science framework and in neurolinguistics.


Formal symbolic system
Another definition sees language as a formal system of signs governed by grammatical rules of combination to communicate meaning. This definition stresses that human languages can be described as closed structural systems consisting of rules that relate particular signs to particular meanings. This structuralist view of language was first introduced by Ferdinand de Saussure, and his structuralism remains foundational for many approaches to language.Some proponents of Saussure's view of language have advocated a formal approach which studies language structure by identifying its basic elements and then by presenting a formal account of the rules according to which the elements combine in order to form words and sentences. The main proponent of such a theory is Noam Chomsky, the originator of the generative theory of grammar, who has defined language as the construction of sentences that can be generated using transformational grammars. Chomsky considers these rules to be an innate feature of the human mind and to constitute the rudiments of what language is. By way of contrast, such transformational grammars are also commonly used in formal logic, in formal linguistics, and in applied computational linguistics. In the philosophy of language, the view of linguistic meaning as residing in the logical relations between propositions and reality was developed by philosophers such as Alfred Tarski, Bertrand Russell, and other formal logicians.


Tool for communication

Yet another definition sees language as a system of communication that enables humans to exchange verbal or symbolic utterances. This definition stresses the social functions of language and the fact that humans use it to express themselves and to manipulate objects in their environment. Functional theories of grammar explain grammatical structures by their communicative functions, and understand the grammatical structures of language to be the result of an adaptive process by which grammar was "tailored" to serve the communicative needs of its users.This view of language is associated with the study of language in pragmatic, cognitive, and interactive frameworks, as well as in sociolinguistics and linguistic anthropology. Functionalist theories tend to study grammar as dynamic phenomena, as structures that are always in the process of changing as they are employed by their speakers. This view places importance on the study of linguistic typology, or the classification of languages according to structural features, as it can be shown that processes of grammaticalization tend to follow trajectories that are partly dependent on typology. In the philosophy of language, the view of pragmatics as being central to language and meaning is often associated with Wittgenstein's later works and with ordinary language philosophers such as J.L. Austin, Paul Grice, John Searle, and W.O. Quine.


Distinctive features of human language

A number of features, many of which were described by Charles Hockett and called design features set human language apart from communication used by non-human animals.
Communication systems used by other animals such as bees or apes are closed systems that consist of a finite, usually very limited, number of possible ideas that can be expressed. In contrast, human language is open-ended and productive, meaning that it allows humans to produce a vast range of utterances from a finite set of elements, and to create new words and sentences. This is possible because human language is based on a dual code, in which a finite number of elements which are meaningless in themselves (e.g. sounds, letters or gestures) can be combined to form an infinite number of larger units of meaning (words and sentences). However, one study has demonstrated that an Australian bird, the chestnut-crowned babbler, is capable of using the same acoustic elements in different arrangements to create two functionally distinct vocalizations. Additionally, pied babblers have demonstrated the ability to generate two functionally distinct vocalisations composed of the same sound type, which can only be distinguished by the number of repeated elements.Several species of animals have proved to be able to acquire forms of communication through social learning: for instance a bonobo named Kanzi learned to express itself using a set of symbolic lexigrams. Similarly, many species of birds and whales learn their songs by imitating other members of their species. However, while some animals may acquire large numbers of words and symbols, none have been able to learn as many different signs as are generally known by an average 4 year old human, nor have any acquired anything resembling the complex grammar of human language.Human languages differ from animal communication systems in that they employ grammatical and semantic categories, such as noun and verb, present and past, which may be used to express exceedingly complex meanings. It is distinguished by the property of recursivity: for example, a noun phrase can contain another noun phrase (as in "[[the chimpanzee]'s lips]") or a clause can contain another clause (as in "[I see [the dog is running]]"). Human language is the only known natural communication system whose adaptability may be referred to as modality independent. This means that it can be used not only for communication through one channel or medium, but through several. For example, spoken language uses the auditive modality, whereas sign languages and writing use the visual modality, and braille writing uses the tactile modality.Human language is unusual in being able to refer to abstract concepts and to imagined or hypothetical events as well as events that took place in the past or may happen in the future. This ability to refer to events that are not at the same time or place as the speech event is called displacement, and while some animal communication systems can use displacement (such as the communication of bees that can communicate the location of sources of nectar that are out of sight), the degree to which it is used in human language is also considered unique.


Origin

Theories about the origin of language differ in regard to their basic assumptions about what language is. Some theories are based on the idea that language is so complex that one cannot imagine it simply appearing from nothing in its final form, but that it must have evolved from earlier pre-linguistic systems among our pre-human ancestors. These theories can be called continuity-based theories. The opposite viewpoint is that language is such a unique human trait that it cannot be compared to anything found among non-humans and that it must therefore have appeared suddenly in the transition from pre-hominids to early man. These theories can be defined as discontinuity-based. Similarly, theories based on the generative view of language pioneered by Noam Chomsky see language mostly as an innate faculty that is largely genetically encoded, whereas functionalist theories see it as a system that is largely cultural, learned through social interaction.Chomsky is one prominent proponent of a discontinuity-based theory of human language origins. He suggests that for scholars interested in the nature of language, "talk about the evolution of the language capacity is beside the point." Chomsky proposes that perhaps "some random mutation took place [...] and it reorganized the brain, implanting a language organ in an otherwise primate brain." Though cautioning against taking this story literally, Chomsky insists that "it may be closer to reality than many other fairy tales that are told about evolutionary processes, including language."
Continuity-based theories are held by a majority of scholars, but they vary in how they envision this development. Those who see language as being mostly innate, for example psychologist Steven Pinker, hold the precedents to be animal cognition, whereas those who see language as a socially learned tool of communication, such as psychologist Michael Tomasello, see it as having developed from animal communication in primates: either gestural or vocal communication to assist in cooperation. Other continuity-based models see language as having developed from music, a view already espoused by Rousseau, Herder, Humboldt, and Charles Darwin. A prominent proponent of this view is archaeologist Steven Mithen. Stephen Anderson states that the age of spoken languages is estimated at 60,000 to 100,000 years and that: Researchers on the evolutionary origin of language generally find it plausible to suggest that language was invented only once, and that all modern spoken languages are thus in some way related, even if that relation can no longer be recovered ... because of limitations on the methods available for reconstruction.
Because language emerged in the early prehistory of man, before the existence of any written records, its early development has left no historical traces, and it is believed that no comparable processes can be observed today. Theories that stress continuity often look at animals to see if, for example, primates display any traits that can be seen as analogous to what pre-human language must have been like. Early human fossils can be inspected for traces of physical adaptation to language use or pre-linguistic forms of symbolic behaviour. Among the signs in human fossils that may suggest linguistic abilities are: the size of the brain relative to body mass, the presence of a larynx capable of advanced sound production and the nature of tools and other manufactured artifacts.It was mostly undisputed that pre-human australopithecines did not have communication systems significantly different from those found in great apes in general. However, a 2017 study on Ardipithecus ramidus challenges this belief. Scholarly opinions vary as to the developments since the appearance of the genus Homo some 2.5 million years ago. Some scholars assume the development of primitive language-like systems (proto-language) as early as Homo habilis (2.3 million years ago) while others place the development of primitive symbolic communication only with Homo erectus (1.8 million years ago) or Homo heidelbergensis (0.6 million years ago), and the development of language proper with Anatomically Modern Homo sapiens with the Upper Paleolithic revolution less than 100,000 years ago.


Study

The study of language, linguistics, has been developing into a science since the first grammatical descriptions of particular languages in India more than 2000 years ago, after the development of the Brahmi script. Modern linguistics is a science that concerns itself with all aspects of language, examining it from all of the theoretical viewpoints described above.


Subdisciplines
The academic study of language is conducted within many different disciplinary areas and from different theoretical angles, all of which inform modern approaches to linguistics. For example, descriptive linguistics examines the grammar of single languages, theoretical linguistics develops theories on how best to conceptualize and define the nature of language based on data from the various extant human languages, sociolinguistics studies how languages are used for social purposes informing in turn the study of the social functions of language and grammatical description, neurolinguistics studies how language is processed in the human brain and allows the experimental testing of theories, computational linguistics builds on theoretical and descriptive linguistics to construct computational models of language often aimed at processing natural language or at testing linguistic hypotheses, and historical linguistics relies on grammatical and lexical descriptions of languages to trace their individual histories and reconstruct trees of language families by using the comparative method.


Early history
The formal study of language is often considered to have started in India with Pini, the 5th century BC grammarian who formulated 3,959 rules of Sanskrit morphology. However, Sumerian scribes already studied the differences between Sumerian and Akkadian grammar around 1900 BC. Subsequent grammatical traditions developed in all of the ancient cultures that adopted writing.In the 17th century AD, the French Port-Royal Grammarians developed the idea that the grammars of all languages were a reflection of the universal basics of thought, and therefore that grammar was universal. In the 18th century, the first use of the comparative method by British philologist and expert on ancient India William Jones sparked the rise of comparative linguistics. The scientific study of language was broadened from Indo-European to language in general by Wilhelm von Humboldt. Early in the 20th century, Ferdinand de Saussure introduced the idea of language as a static system of interconnected units, defined through the oppositions between them.By introducing a distinction between diachronic and synchronic analyses of language, he laid the foundation of the modern discipline of linguistics. Saussure also introduced several basic dimensions of linguistic analysis that are still fundamental in many contemporary linguistic theories, such as the distinctions between syntagm and paradigm, and the Langue-parole distinction, distinguishing language as an abstract system (langue), from language as a concrete manifestation of this system (parole).


Modern linguistics
In the 1960s, Noam Chomsky formulated the generative theory of language. According to this theory, the most basic form of language is a set of syntactic rules that is universal for all humans and which underlies the grammars of all human languages. This set of rules is called Universal Grammar; for Chomsky, describing it is the primary objective of the discipline of linguistics. Thus, he considered that the grammars of individual languages are only of importance to linguistics insofar as they allow us to deduce the universal underlying rules from which the observable linguistic variability is generated.In opposition to the formal theories of the generative school, functional theories of language propose that since language is fundamentally a tool, its structures are best analyzed and understood by reference to their functions. Formal theories of grammar seek to define the different elements of language and describe the way they relate to each other as systems of formal rules or operations, while functional theories seek to define the functions performed by language and then relate them to the linguistic elements that carry them out. The framework of cognitive linguistics interprets language in terms of the concepts (which are sometimes universal, and sometimes specific to a particular language) which underlie its forms. Cognitive linguistics is primarily concerned with how the mind creates meaning through language.


Physiological and neural architecture of language and speech
Speaking is the default modality for language in all cultures. The production of spoken language depends on sophisticated capacities for controlling the lips, tongue and other components of the vocal apparatus, the ability to acoustically decode speech sounds, and the neurological apparatus required for acquiring and producing language. The study of the genetic bases for human language is at an early stage: the only gene that has definitely been implicated in language production is FOXP2, which may cause a kind of congenital language disorder if affected by mutations.


The brain

 The brain is the coordinating center of all linguistic activity; it controls both the production of linguistic cognition and of meaning and the mechanics of speech production. Nonetheless, our knowledge of the neurological bases for language is quite limited, though it has advanced considerably with the use of modern imaging techniques. The discipline of linguistics dedicated to studying the neurological aspects of language is called neurolinguistics.Early work in neurolinguistics involved the study of language in people with brain lesions, to see how lesions in specific areas affect language and speech. In this way, neuroscientists in the 19th century discovered that two areas in the brain are crucially implicated in language processing. The first area is Wernicke's area, which is in the posterior section of the superior temporal gyrus in the dominant cerebral hemisphere. People with a lesion in this area of the brain develop receptive aphasia, a condition in which there is a major impairment of language comprehension, while speech retains a natural-sounding rhythm and a relatively normal sentence structure. The second area is Broca's area, in the posterior inferior frontal gyrus of the dominant hemisphere. People with a lesion to this area develop expressive aphasia, meaning that they know what they want to say, they just cannot get it out. They are typically able to understand what is being said to them, but unable to speak fluently. Other symptoms that may be present in expressive aphasia include problems with word repetition. The condition affects both spoken and written language. Those with this aphasia also exhibit ungrammatical speech and show inability to use syntactic information to determine the meaning of sentences. Both expressive and receptive aphasia also affect the use of sign language, in analogous ways to how they affect speech, with expressive aphasia causing signers to sign slowly and with incorrect grammar, whereas a signer with receptive aphasia will sign fluently, but make little sense to others and have difficulties comprehending others' signs. This shows that the impairment is specific to the ability to use language, not to the physiology used for speech production.With technological advances in the late 20th century, neurolinguists have also incorporated non-invasive techniques such as functional magnetic resonance imaging (fMRI) and electrophysiology to study language processing in individuals without impairments.


Anatomy of speech

Spoken language relies on human physical ability to produce sound, which is a longitudinal wave propagated through the air at a frequency capable of vibrating the ear drum. This ability depends on the physiology of the human speech organs. These organs consist of the lungs, the voice box (larynx), and the upper vocal tract  the throat, the mouth, and the nose. By controlling the different parts of the speech apparatus, the airstream can be manipulated to produce different speech sounds.The sound of speech can be analyzed into a combination of segmental and suprasegmental elements. The segmental elements are those that follow each other in sequences, which are usually represented by distinct letters in alphabetic scripts, such as the Roman script. In free flowing speech, there are no clear boundaries between one segment and the next, nor usually are there any audible pauses between them. Segments therefore are distinguished by their distinct sounds which are a result of their different articulations, and can be either vowels or consonants. Suprasegmental phenomena encompass such elements as stress, phonation type, voice timbre, and prosody or intonation, all of which may have effects across multiple segments.Consonants and vowel segments combine to form syllables, which in turn combine to form utterances; these can be distinguished phonetically as the space between two inhalations. Acoustically, these different segments are characterized by different formant structures, that are visible in a spectrogram of the recorded sound wave. Formants are the amplitude peaks in the frequency spectrum of a specific sound.Vowels are those sounds that have no audible friction caused by the narrowing or obstruction of some part of the upper vocal tract. They vary in quality according to the degree of lip aperture and the placement of the tongue within the oral cavity. Vowels are called close when the lips are relatively closed, as in the pronunciation of the vowel [i] (English "ee"), or open when the lips are relatively open, as in the vowel [a] (English "ah"). If the tongue is located towards the back of the mouth, the quality changes, creating vowels such as [u] (English "oo"). The quality also changes depending on whether the lips are rounded as opposed to unrounded, creating distinctions such as that between [i] (unrounded front vowel such as English "ee") and [y] (rounded front vowel such as German "").Consonants are those sounds that have audible friction or closure at some point within the upper vocal tract. Consonant sounds vary by place of articulation, i.e. the place in the vocal tract where the airflow is obstructed, commonly at the lips, teeth, alveolar ridge, palate, velum, uvula, or glottis. Each place of articulation produces a different set of consonant sounds, which are further distinguished by manner of articulation, or the kind of friction, whether full closure, in which case the consonant is called occlusive or stop, or different degrees of aperture creating fricatives and approximants. Consonants can also be either voiced or unvoiced, depending on whether the vocal cords are set in vibration by airflow during the production of the sound. Voicing is what separates English [s] in bus (unvoiced sibilant) from [z] in buzz (voiced sibilant).Some speech sounds, both vowels and consonants, involve release of air flow through the nasal cavity, and these are called nasals or nasalized sounds. Other sounds are defined by the way the tongue moves within the mouth such as the l-sounds (called laterals, because the air flows along both sides of the tongue), and the r-sounds (called rhotics).By using these speech organs, humans can produce hundreds of distinct sounds: some appear very often in the world's languages, whereas others are much more common in certain language families, language areas, or even specific to a single language.


Structure
When described as a system of symbolic communication, language is traditionally seen as consisting of three parts: signs, meanings, and a code connecting signs with their meanings. The study of the process of semiosis, how signs and meanings are combined, used, and interpreted is called semiotics. Signs can be composed of sounds, gestures, letters, or symbols, depending on whether the language is spoken, signed, or written, and they can be combined into complex signs, such as words and phrases. When used in communication, a sign is encoded and transmitted by a sender through a channel to a receiver who decodes it.

Some of the properties that define human language as opposed to other communication systems are: the arbitrariness of the linguistic sign, meaning that there is no predictable connection between a linguistic sign and its meaning; the duality of the linguistic system, meaning that linguistic structures are built by combining elements into larger structures that can be seen as layered, e.g. how sounds build words and words build phrases; the discreteness of the elements of language, meaning that the elements out of which linguistic signs are constructed are discrete units, e.g. sounds and words, that can be distinguished from each other and rearranged in different patterns; and the productivity of the linguistic system, meaning that the finite number of linguistic elements can be combined into a theoretically infinite number of combinations.The rules by which signs can be combined to form words and phrases are called syntax or grammar. The meaning that is connected to individual signs, morphemes, words, phrases, and texts is called semantics. The division of language into separate but connected systems of sign and meaning goes back to the first linguistic studies of de Saussure and is now used in almost all branches of linguistics.


Semantics

Languages express meaning by relating a sign form to a meaning, or its content. Sign forms must be something that can be perceived, for example, in sounds, images, or gestures, and then related to a specific meaning by social convention. Because the basic relation of meaning for most linguistic signs is based on social convention, linguistic signs can be considered arbitrary, in the sense that the convention is established socially and historically, rather than by means of a natural relation between a specific sign form and its meaning.Thus, languages must have a vocabulary of signs related to specific meaning. The English sign "dog" denotes, for example, a member of the species Canis familiaris. In a language, the array of arbitrary signs connected to specific meanings is called the lexicon, and a single sign connected to a meaning is called a lexeme. Not all meanings in a language are represented by single words. Often, semantic concepts are embedded in the morphology or syntax of the language in the form of grammatical categories.All languages contain the semantic structure of predication: a structure that predicates a property, state, or action. Traditionally, semantics has been understood to be the study of how speakers and interpreters assign truth values to statements, so that meaning is understood to be the process by which a predicate can be said to be true or false about an entity, e.g. "[x [is y]]" or "[x [does y]]". Recently, this model of semantics has been complemented with more dynamic models of meaning that incorporate shared knowledge about the context in which a sign is interpreted into the production of meaning. Such models of meaning are explored in the field of pragmatics.


Sounds and symbols

Depending on modality, language structure can be based on systems of sounds (speech), gestures (sign languages), or graphic or tactile symbols (writing). The ways in which languages use sounds or signs to construct meaning are studied in phonology.Sounds as part of a linguistic system are called phonemes. Phonemes are abstract units of sound, defined as the smallest units in a language that can serve to distinguish between the meaning of a pair of minimally different words, a so-called minimal pair. In English, for example, the words bat [bt] and pat [pt] form a minimal pair, in which the distinction between /b/ and /p/ differentiates the two words, which have different meanings. However, each language contrasts sounds in different ways. For example, in a language that does not distinguish between voiced and unvoiced consonants, the sounds [p] and [b] (if they both occur) could be considered a single phoneme, and consequently, the two pronunciations would have the same meaning. Similarly, the English language does not distinguish phonemically between aspirated and non-aspirated pronunciations of consonants, as many other languages like Korean and Hindi do: the unaspirated /p/ in spin [spn] and the aspirated /p/ in pin [pn] are considered to be merely different ways of pronouncing the same phoneme (such variants of a single phoneme are called allophones), whereas in Mandarin Chinese, the same difference in pronunciation distinguishes between the words [p] 'crouch' and [p] 'eight' (the accent above the  means that the vowel is pronounced with a high tone).All spoken languages have phonemes of at least two different categories, vowels and consonants, that can be combined to form syllables. As well as segments such as consonants and vowels, some languages also use sound in other ways to convey meaning. Many languages, for example, use stress, pitch, duration, and tone to distinguish meaning. Because these phenomena operate outside of the level of single segments, they are called suprasegmental. Some languages have only a few phonemes, for example, Rotokas and Pirah language with 11 and 10 phonemes respectively, whereas languages like Taa may have as many as 141 phonemes. In sign languages, the equivalent to phonemes (formerly called cheremes) are defined by the basic elements of gestures, such as hand shape, orientation, location, and motion, which correspond to manners of articulation in spoken language.Writing systems represent language using visual symbols, which may or may not correspond to the sounds of spoken language. The Latin alphabet (and those on which it is based or that have been derived from it) was originally based on the representation of single sounds, so that words were constructed from letters that generally denote a single consonant or vowel in the structure of the word. In syllabic scripts, such as the Inuktitut syllabary, each sign represents a whole syllable. In logographic scripts, each sign represents an entire word, and will generally bear no relation to the sound of that word in spoken language.
Because all languages have a very large number of words, no purely logographic scripts are known to exist. Written language represents the way spoken sounds and words follow one after another by arranging symbols according to a pattern that follows a certain direction. The direction used in a writing system is entirely arbitrary and established by convention. Some writing systems use the horizontal axis (left to right as the Latin script or right to left as the Arabic script), while others such as traditional Chinese writing use the vertical dimension (from top to bottom). A few writing systems use opposite directions for alternating lines, and others, such as the ancient Maya script, can be written in either direction and rely on graphic cues to show the reader the direction of reading.In order to represent the sounds of the world's languages in writing, linguists have developed the International Phonetic Alphabet, designed to represent all of the discrete sounds that are known to contribute to meaning in human languages.


Grammar

Grammar is the study of how meaningful elements called morphemes within a language can be combined into utterances. Morphemes can either be free or bound. If they are free to be moved around within an utterance, they are usually called words, and if they are bound to other words or morphemes, they are called affixes. The way in which meaningful elements can be combined within a language is governed by rules. The study of the rules for the internal structure of words are called morphology. The rules of the internal structure of phrases and sentences are called syntax.


Grammatical categories

Grammar can be described as a system of categories and a set of rules that determine how categories combine to form different aspects of meaning. Languages differ widely in whether they are encoded through the use of categories or lexical units. However, several categories are so common as to be nearly universal. Such universal categories include the encoding of the grammatical relations of participants and predicates by grammatically distinguishing between their relations to a predicate, the encoding of temporal and spatial relations on predicates, and a system of grammatical person governing reference to and distinction between speakers and addressees and those about whom they are speaking.


Word classes
Languages organize their parts of speech into classes according to their functions and positions relative to other parts. All languages, for instance, make a basic distinction between a group of words that prototypically denotes things and concepts and a group of words that prototypically denotes actions and events. The first group, which includes English words such as "dog" and "song", are usually called nouns. The second, which includes "think" and "sing", are called verbs. Another common category is the adjective: words that describe properties or qualities of nouns, such as "red" or "big". Word classes can be "open" if new words can continuously be added to the class, or relatively "closed" if there is a fixed number of words in a class. In English, the class of pronouns is closed, whereas the class of adjectives is open, since an infinite number of adjectives can be constructed from verbs (e.g. "saddened") or nouns (e.g. with the -like suffix, as in "noun-like"). In other languages such as Korean, the situation is the opposite, and new pronouns can be constructed, whereas the number of adjectives is fixed.Word classes also carry out differing functions in grammar. Prototypically, verbs are used to construct predicates, while nouns are used as arguments of predicates. In a sentence such as "Sally runs", the predicate is "runs", because it is the word that predicates a specific state about its argument "Sally". Some verbs such as "curse" can take two arguments, e.g. "Sally cursed John". A predicate that can only take a single argument is called intransitive, while a predicate that can take two arguments is called transitive.Many other word classes exist in different languages, such as conjunctions like "and" that serve to join two sentences, articles that introduce a noun, interjections such as "wow!", or ideophones like "splash" that mimic the sound of some event. Some languages have positionals that describe the spatial position of an event or entity. Many languages have classifiers that identify countable nouns as belonging to a particular type or having a particular shape. For instance, in Japanese, the general noun classifier for humans is nin (), and it is used for counting humans, whatever they are called:
san-nin no gakusei () lit. "3 human-classifier of student"  three studentsFor trees, it would be:

san-bon no ki () lit. "3 classifier-for-long-objects of tree"  three trees


Morphology
In linguistics, the study of the internal structure of complex words and the processes by which words are formed is called morphology. In most languages, it is possible to construct complex words that are built of several morphemes. For instance, the English word "unexpected" can be analyzed as being composed of the three morphemes "un-", "expect" and "-ed".Morphemes can be classified according to whether they are independent morphemes, so-called roots, or whether they can only co-occur attached to other morphemes. These bound morphemes or affixes can be classified according to their position in relation to the root: prefixes precede the root, suffixes follow the root, and infixes are inserted in the middle of a root. Affixes serve to modify or elaborate the meaning of the root. Some languages change the meaning of words by changing the phonological structure of a word, for example, the English word "run", which in the past tense is "ran". This process is called ablaut. Furthermore, morphology distinguishes between the process of inflection, which modifies or elaborates on a word, and the process of derivation, which creates a new word from an existing one. In English, the verb "sing" has the inflectional forms "singing" and "sung", which are both verbs, and the derivational form "singer", which is a noun derived from the verb with the agentive suffix "-er".Languages differ widely in how much they rely on morphological processes of word formation. In some languages, for example, Chinese, there are no morphological processes, and all grammatical information is encoded syntactically by forming strings of single words. This type of morpho-syntax is often called isolating, or analytic, because there is almost a full correspondence between a single word and a single aspect of meaning. Most languages have words consisting of several morphemes, but they vary in the degree to which morphemes are discrete units. In many languages, notably in most Indo-European languages, single morphemes may have several distinct meanings that cannot be analyzed into smaller segments. For example, in Latin, the word bonus, or "good", consists of the root bon-, meaning "good", and the suffix -us, which indicates masculine gender, singular number, and nominative case. These languages are called fusional languages, because several meanings may be fused into a single morpheme. The opposite of fusional languages are agglutinative languages which construct words by stringing morphemes together in chains, but with each morpheme as a discrete semantic unit. An example of such a language is Turkish, where for example, the word evlerinizden, or "from your houses", consists of the morphemes, ev-ler-iniz-den with the meanings house-plural-your-from. The languages that rely on morphology to the greatest extent are traditionally called polysynthetic languages. They may express the equivalent of an entire English sentence in a single word. For example, in Persian the single word nafahmidamesh means I didn't understand it consisting of morphemes na-fahm-id-am-esh with the meanings, "negation.understand.past.I.it". As another example with more complexity, in the Yupik word tuntussuqatarniksatengqiggtuq, which means "He had not yet said again that he was going to hunt reindeer", the word consists of the morphemes tuntu-ssur-qatar-ni-ksaite-ngqiggte-uq with the meanings, "reindeer-hunt-future-say-negation-again-third.person.singular.indicative", and except for the morpheme tuntu ("reindeer") none of the other morphemes can appear in isolation.Many languages use morphology to cross-reference words within a sentence. This is sometimes called agreement. For example, in many Indo-European languages, adjectives must cross-reference the noun they modify in terms of number, case, and gender, so that the Latin adjective bonus, or "good", is inflected to agree with a noun that is masculine gender, singular number, and nominative case. In many polysynthetic languages, verbs cross-reference their subjects and objects. In these types of languages, a single verb may include information that would require an entire sentence in English. For example, in the Basque phrase ikusi nauzu, or "you saw me", the past tense auxiliary verb n-au-zu (similar to English "do") agrees with both the subject (you) expressed by the n- prefix, and with the object (me) expressed by the  zu suffix. The sentence could be directly transliterated as "see you-did-me"


Syntax

Another way in which languages convey meaning is through the order of words within a sentence. The grammatical rules for how to produce new sentences from words that are already known is called syntax. The syntactical rules of a language determine why a sentence in English such as "I love you" is meaningful, but "*love you I" is not. Syntactical rules determine how word order and sentence structure is constrained, and how those constraints contribute to meaning. For example, in English, the two sentences "the slaves were cursing the master" and "the master was cursing the slaves" mean different things, because the role of the grammatical subject is encoded by the noun being in front of the verb, and the role of object is encoded by the noun appearing after the verb. Conversely, in Latin, both Dominus servos vituperabat and Servos vituperabat dominus mean "the master was reprimanding the slaves", because servos, or "slaves", is in the accusative case, showing that they are the grammatical object of the sentence, and dominus, or "master", is in the nominative case, showing that he is the subject.Latin uses morphology to express the distinction between subject and object, whereas English uses word order. Another example of how syntactic rules contribute to meaning is the rule of inverse word order in questions, which exists in many languages. This rule explains why when in English, the phrase "John is talking to Lucy" is turned into a question, it becomes "Who is John talking to?", and not "John is talking to who?". The latter example may be used as a way of placing special emphasis on "who", thereby slightly altering the meaning of the question. Syntax also includes the rules for how complex sentences are structured by grouping words together in units, called phrases, that can occupy different places in a larger syntactic structure. Sentences can be described as consisting of phrases connected in a tree structure, connecting the phrases to each other at different levels. To the right is a graphic representation of the syntactic analysis of the English sentence "the cat sat on the mat". The sentence is analyzed as being constituted by a noun phrase, a verb, and a prepositional phrase; the prepositional phrase is further divided into a preposition and a noun phrase, and the noun phrases consist of an article and a noun.The reason sentences can be seen as being composed of phrases is because each phrase would be moved around as a single element if syntactic operations were carried out. For example, "the cat" is one phrase, and "on the mat" is another, because they would be treated as single units if a decision was made to emphasize the location by moving forward the prepositional phrase: "[And] on the mat, the cat sat". There are many different formalist and functionalist frameworks that propose theories for describing syntactic structures, based on different assumptions about what language is and how it should be described. Each of them would analyze a sentence such as this in a different manner.


Typology and universals

Languages can be classified in relation to their grammatical types. Languages that belong to different families nonetheless often have features in common, and these shared features tend to correlate. For example, languages can be classified on the basis of their basic word order, the relative order of the verb, and its constituents in a normal indicative sentence. In English, the basic order is SVO (subjectverbobject): "The snake(S) bit(V) the man(O)", whereas for example, the corresponding sentence in the Australian language Gamilaraay would be duyugu nama dayn yiy (snake man bit), SOV. Word order type is relevant as a typological parameter, because basic word order type corresponds with other syntactic parameters, such as the relative order of nouns and adjectives, or of the use of prepositions or postpositions. Such correlations are called implicational universals. For example, most (but not all) languages that are of the SOV type have postpositions rather than prepositions, and have adjectives before nouns.All languages structure sentences into Subject, Verb, and Object, but languages differ in the way they classify the relations between actors and actions. English uses the nominative-accusative word typology: in English transitive clauses, the subjects of both intransitive sentences ("I run") and transitive sentences ("I love you") are treated in the same way, shown here by the nominative pronoun I. Some languages, called ergative, Gamilaraay among them, distinguish instead between Agents and Patients. In ergative languages, the single participant in an intransitive sentence, such as "I run", is treated the same as the patient in a transitive sentence, giving the equivalent of "me run". Only in transitive sentences would the equivalent of the pronoun "I" be used. In this way the semantic roles can map onto the grammatical relations in different ways, grouping an intransitive subject either with Agents (accusative type) or Patients (ergative type) or even making each of the three roles differently, which is called the tripartite type.The shared features of languages which belong to the same typological class type may have arisen completely independently. Their co-occurrence might be due to universal laws governing the structure of natural languages, "language universals", or they might be the result of languages evolving convergent solutions to the recurring communicative problems that humans use language to solve.


Social contexts of use and transmission

While humans have the ability to learn any language, they only do so if they grow up in an environment in which language exists and is used by others. Language is therefore dependent on communities of speakers in which children learn language from their elders and peers and themselves transmit language to their own children. Languages are used by those who speak them to communicate and to solve a plethora of social tasks. Many aspects of language use can be seen to be adapted specifically to these purposes. Due to the way in which language is transmitted between generations and within communities, language perpetually changes, diversifying into new languages or converging due to language contact. The process is similar to the process of evolution, where the process of descent with modification leads to the formation of a phylogenetic tree.However, languages differ from biological organisms in that they readily incorporate elements from other languages through the process of diffusion, as speakers of different languages come into contact. Humans also frequently speak more than one language, acquiring their first language or languages as children, or learning new languages as they grow up. Because of the increased language contact in the globalizing world, many small languages are becoming endangered as their speakers shift to other languages that afford the possibility to participate in larger and more influential speech communities.


Usage and meaning

When studying the way in which words and signs are used, it is often the case that words have different meanings, depending on the social context of use. An important example of this is the process called deixis, which describes the way in which certain words refer to entities through their relation between a specific point in time and space when the word is uttered. Such words are, for example, the word, "I" (which designates the person speaking), "now" (which designates the moment of speaking), and "here" (which designates the position of speaking). Signs also change their meanings over time, as the conventions governing their usage gradually change. The study of how the meaning of linguistic expressions changes depending on context is called pragmatics. Deixis is an important part of the way that we use language to point out entities in the world. Pragmatics is concerned with the ways in which language use is patterned and how these patterns contribute to meaning. For example, in all languages, linguistic expressions can be used not just to transmit information, but to perform actions. Certain actions are made only through language, but nonetheless have tangible effects, e.g. the act of "naming", which creates a new name for some entity, or the act of "pronouncing someone man and wife", which creates a social contract of marriage. These types of acts are called speech acts, although they can also be carried out through writing or hand signing.The form of linguistic expression often does not correspond to the meaning that it actually has in a social context. For example, if at a dinner table a person asks, "Can you reach the salt?", that is, in fact, not a question about the length of the arms of the one being addressed, but a request to pass the salt across the table. This meaning is implied by the context in which it is spoken; these kinds of effects of meaning are called conversational implicatures. These social rules for which ways of using language are considered appropriate in certain situations and how utterances are to be understood in relation to their context vary between communities, and learning them is a large part of acquiring communicative competence in a language.


Acquisition

All healthy, normally developing human beings learn to use language. Children acquire the language or languages used around them: whichever languages they receive sufficient exposure to during childhood. The development is essentially the same for children acquiring sign or oral languages. This learning process is referred to as first-language acquisition, since unlike many other kinds of learning, it requires no direct teaching or specialized study. In The Descent of Man, naturalist Charles Darwin called this process "an instinctive tendency to acquire an art".

First language acquisition proceeds in a fairly regular sequence, though there is a wide degree of variation in the timing of particular stages among normally developing infants. Studies published in 2013 have indicated that unborn fetuses are capable of language acquisition to some degree. From birth, newborns respond more readily to human speech than to other sounds. Around one month of age, babies appear to be able to distinguish between different speech sounds. Around six months of age, a child will begin babbling, producing the speech sounds or handshapes of the languages used around them. Words appear around the age of 12 to 18 months; the average vocabulary of an eighteen-month-old child is around 50 words. A child's first utterances are holophrases (literally "whole-sentences"), utterances that use just one word to communicate some idea. Several months after a child begins producing words, he or she will produce two-word utterances, and within a few more months will begin to produce telegraphic speech, or short sentences that are less grammatically complex than adult speech, but that do show regular syntactic structure. From roughly the age of three to five years, a child's ability to speak or sign is refined to the point that it resembles adult language.Acquisition of second and additional languages can come at any age, through exposure in daily life or courses. Children learning a second language are more likely to achieve native-like fluency than adults, but in general, it is very rare for someone speaking a second language to pass completely for a native speaker. An important difference between first language acquisition and additional language acquisition is that the process of additional language acquisition is influenced by languages that the learner already knows.


Culture

Languages, understood as the particular set of speech norms of a particular community, are also a part of the larger culture of the community that speaks them. Languages differ not only in pronunciation, vocabulary, and grammar, but also through having different "cultures of speaking." Humans use language as a way of signalling identity with one cultural group as well as difference from others. Even among speakers of one language, several different ways of using the language exist, and each is used to signal affiliation with particular subgroups within a larger culture. Linguists and anthropologists, particularly sociolinguists, ethnolinguists, and linguistic anthropologists have specialized in studying how ways of speaking vary between speech communities.Linguists use the term "varieties" to refer to the different ways of speaking a language. This term includes geographically or socioculturally defined dialects as well as the jargons or styles of subcultures. Linguistic anthropologists and sociologists of language define communicative style as the ways that language is used and understood within a particular culture.Because norms for language use are shared by members of a specific group, communicative style also becomes a way of displaying and constructing group identity. Linguistic differences may become salient markers of divisions between social groups, for example, speaking a language with a particular accent may imply membership of an ethnic minority or social class, one's area of origin, or status as a second language speaker. These kinds of differences are not part of the linguistic system, but are an important part of how people use language as a social tool for constructing groups.However, many languages also have grammatical conventions that signal the social position of the speaker in relation to others through the use of registers that are related to social hierarchies or divisions. In many languages, there are stylistic or even grammatical differences between the ways men and women speak, between age groups, or between social classes, just as some languages employ different words depending on who is listening. For example, in the Australian language Dyirbal, a married man must use a special set of words to refer to everyday items when speaking in the presence of his mother-in-law. Some cultures, for example, have elaborate systems of "social deixis", or systems of signalling social distance through linguistic means. In English, social deixis is shown mostly through distinguishing between addressing some people by first name and others by surname, and in titles such as "Mrs.", "boy", "Doctor", or "Your Honor", but in other languages, such systems may be highly complex and codified in the entire grammar and vocabulary of the language. For instance, in languages of east Asia such as Thai, Burmese, and Javanese, different words are used according to whether a speaker is addressing someone of higher or lower rank than oneself in a ranking system with animals and children ranking the lowest and gods and members of royalty as the highest.


Writing, literacy and technology

Throughout history a number of different ways of representing language in graphic media have been invented. These are called writing systems.
The use of writing has made language even more useful to humans. It makes it possible to store large amounts of information outside of the human body and retrieve it again, and it allows communication across physical distances and timespans that would otherwise be impossible. Many languages conventionally employ different genres, styles, and registers in written and spoken language, and in some communities, writing traditionally takes place in an entirely different language than the one spoken. There is some evidence that the use of writing also has effects on the cognitive development of humans, perhaps because acquiring literacy generally requires explicit and formal education.The invention of the first writing systems is roughly contemporary with the beginning of the Bronze Age in the late 4th millennium BC. The Sumerian archaic cuneiform script and the Egyptian hieroglyphs are generally considered to be the earliest writing systems, both emerging out of their ancestral proto-literate symbol systems from 34003200 BC with the earliest coherent texts from about 2600 BC. It is generally agreed that Sumerian writing was an independent invention; however, it is debated whether Egyptian writing was developed completely independently of Sumerian, or was a case of cultural diffusion. A similar debate exists for the Chinese script, which developed around 1200 BC. The pre-Columbian Mesoamerican writing systems (including among others Olmec and Maya scripts) are generally believed to have had independent origins.


Change

All languages change as speakers adopt or invent new ways of speaking and pass them on to other members of their speech community. Language change happens at all levels from the phonological level to the levels of vocabulary, morphology, syntax, and discourse. Even though language change is often initially evaluated negatively by speakers of the language who often consider changes to be "decay" or a sign of slipping norms of language usage, it is natural and inevitable.Changes may affect specific sounds or the entire phonological system. Sound change can consist of the replacement of one speech sound or phonetic feature by another, the complete loss of the affected sound, or even the introduction of a new sound in a place where there had been none. Sound changes can be conditioned in which case a sound is changed only if it occurs in the vicinity of certain other sounds. Sound change is usually assumed to be regular, which means that it is expected to apply mechanically whenever its structural conditions are met, irrespective of any non-phonological factors. On the other hand, sound changes can sometimes be sporadic, affecting only one particular word or a few words, without any seeming regularity. Sometimes a simple change triggers a chain shift in which the entire phonological system is affected. This happened in the Germanic languages when the sound change known as Grimm's law affected all the stop consonants in the system. The original consonant *b became /b/ in the Germanic languages, the previous *b in turn became /p/, and the previous *p became /f/. The same process applied to all stop consonants and explains why Italic languages such as Latin have p in words like pater and pisces, whereas Germanic languages, like English, have father and fish.Another example is the Great Vowel Shift in English, which is the reason that the spelling of English vowels do not correspond well to their current pronunciation. This is because the vowel shift brought the already established orthography out of synchronization with pronunciation. Another source of sound change is the erosion of words as pronunciation gradually becomes increasingly indistinct and shortens words, leaving out syllables or sounds. This kind of change caused Latin mea domina to eventually become the French madame and American English ma'am.Change also happens in the grammar of languages as discourse patterns such as idioms or particular constructions become grammaticalized. This frequently happens when words or morphemes erode and the grammatical system is unconsciously rearranged to compensate for the lost element. For example, in some varieties of Caribbean Spanish the final /s/ has eroded away. Since Standard Spanish uses final /s/ in the morpheme marking the second person subject "you" in verbs, the Caribbean varieties now have to express the second person using the pronoun t. This means that the sentence "what's your name" is como te llamas? [komo te jamas] in Standard Spanish, but [komo tu te jama] in Caribbean Spanish. The simple sound change has affected both morphology and syntax. Another common cause of grammatical change is the gradual petrification of idioms into new grammatical forms, for example, the way the English "going to" construction lost its aspect of movement and in some varieties of English has almost become a full-fledged future tense (e.g. I'm gonna).
Language change may be motivated by "language internal" factors, such as changes in pronunciation motivated by certain sounds being difficult to distinguish aurally or to produce, or through patterns of change that cause some rare types of constructions to drift towards more common types. Other causes of language change are social, such as when certain pronunciations become emblematic of membership in certain groups, such as social classes, or with ideologies, and therefore are adopted by those who wish to identify with those groups or ideas. In this way, issues of identity and politics can have profound effects on language structure.


Contact

One important source of language change is contact and resulting diffusion of linguistic traits between languages. Language contact occurs when speakers of two or more languages or varieties interact on a regular basis. Multilingualism is likely to have been the norm throughout human history and most people in the modern world are multilingual. Before the rise of the concept of the ethno-national state, monolingualism was characteristic mainly of populations inhabiting small islands. But with the ideology that made one people, one state, and one language the most desirable political arrangement, monolingualism started to spread throughout the world. Nonetheless, there are only 250 countries in the world corresponding to some 6000 languages, which means that most countries are multilingual and most languages therefore exist in close contact with other languages.When speakers of different languages interact closely, it is typical for their languages to influence each other. Through sustained language contact over long periods, linguistic traits diffuse between languages, and languages belonging to different families may converge to become more similar. In areas where many languages are in close contact, this may lead to the formation of language areas in which unrelated languages share a number of linguistic features. A number of such language areas have been documented, among them, the Balkan language area, the Mesoamerican language area, and the Ethiopian language area. Also, larger areas such as South Asia, Europe, and Southeast Asia have sometimes been considered language areas, because of widespread diffusion of specific areal features.Language contact may also lead to a variety of other linguistic phenomena, including language convergence, borrowing, and relexification (replacement of much of the native vocabulary with that of another language). In situations of extreme and sustained language contact, it may lead to the formation of new mixed languages that cannot be considered to belong to a single language family. One type of mixed language called pidgins occurs when adult speakers of two different languages interact on a regular basis, but in a situation where neither group learns to speak the language of the other group fluently. In such a case, they will often construct a communication form that has traits of both languages, but which has a simplified grammatical and phonological structure. The language comes to contain mostly the grammatical and phonological categories that exist in both languages. Pidgin languages are defined by not having any native speakers, but only being spoken by people who have another language as their first language. But if a Pidgin language becomes the main language of a speech community, then eventually children will grow up learning the pidgin as their first language. As the generation of child learners grow up, the pidgin will often be seen to change its structure and acquire a greater degree of complexity. This type of language is generally called a creole language. An example of such mixed languages is Tok Pisin, the official language of Papua New-Guinea, which originally arose as a Pidgin based on English and Austronesian languages; others are Kreyl ayisyen, the French-based creole language spoken in Haiti, and Michif, a mixed language of Canada, based on the Native American language Cree and French.


Linguistic diversity

SIL Ethnologue defines a "living language" as "one that has at least one speaker for whom it is their first language". The exact number of known living languages varies from 6,000 to 7,000, depending on the precision of one's definition of "language", and in particular, on how one defines the distinction between languages and dialects. As of 2016, Ethnologue cataloged 7,097 living human languages. The Ethnologue establishes linguistic groups based on studies of mutual intelligibility, and therefore often includes more categories than more conservative classifications. For example, the Danish language that most scholars consider a single language with several dialects is classified as two distinct languages (Danish and Jutish) by the Ethnologue.According to the Ethnologue, 389 languages (nearly 6%) have more than a million speakers. These languages together account for 94% of the world's population, whereas 94% of the world's languages account for the remaining 6% of the global population.


Languages and dialects

There is no clear distinction between a language and a dialect, notwithstanding a famous aphorism attributed to linguist Max Weinreich that "a language is a dialect with an army and navy". For example, national boundaries frequently override linguistic difference in determining whether two linguistic varieties are languages or dialects. Hakka, Cantonese and Mandarin are, for example, often classified as "dialects" of Chinese, even though they are more different from each other than Swedish is from Norwegian. Before the Yugoslav civil war, Serbo-Croatian was generally considered a single language with two normative variants, but due to sociopolitical reasons, Croatian and Serbian are now often treated as separate languages and employ different writing systems. In other words, the distinction may hinge on political considerations as much as on cultural differences, distinctive writing systems, or degree of mutual intelligibility.


Language families of the world

The world's languages can be grouped into language families consisting of languages that can be shown to have common ancestry. Linguists recognize many hundreds of language families, although some of them can possibly be grouped into larger units as more evidence becomes available and in-depth studies are carried out. At present, there are also dozens of language isolates: languages that cannot be shown to be related to any other languages in the world. Among them are Basque, spoken in Europe, Zuni of New Mexico, Purpecha of Mexico, Ainu of Japan, Burushaski of Pakistan, and many others.The language family of the world that has the most speakers is the Indo-European languages, spoken by 46% of the world's population. This family includes major world languages like English, Spanish, French, German, Russian, and Hindustani (Hindi/Urdu). The Indo-European family achieved prevalence first during the Eurasian Migration Period (c. 400800 AD), and subsequently through the European colonial expansion, which brought the Indo-European languages to a politically and often numerically dominant position in the Americas and much of Africa. The Sino-Tibetan languages are spoken by 20% of the world's population and include many of the languages of East Asia, including Hakka, Mandarin Chinese, Cantonese, and hundreds of smaller languages.Africa is home to a large number of language families, the largest of which is the Niger-Congo language family, which includes such languages as Swahili, Shona, and Yoruba. Speakers of the Niger-Congo languages account for 6.9% of the world's population. A similar number of people speak the Afroasiatic languages, which include the populous Semitic languages such as Arabic, Hebrew language, and the languages of the Sahara region, such as the Berber languages and Hausa.The Austronesian languages are spoken by 5.5% of the world's population and stretch from Madagascar to maritime Southeast Asia all the way to Oceania. It includes such languages as Malagasy, Mori, Samoan, and many of the indigenous languages of Indonesia and Taiwan. The Austronesian languages are considered to have originated in Taiwan around 3000 BC and spread through the Oceanic region through island-hopping, based on an advanced nautical technology. Other populous language families are the Dravidian languages of South Asia (among them Kannada Tamil and Telugu), the Turkic languages of Central Asia (such as Turkish), the Austroasiatic (among them Khmer), and TaiKadai languages of Southeast Asia (including Thai).The areas of the world in which there is the greatest linguistic diversity, such as the Americas, Papua New Guinea, West Africa, and South-Asia, contain hundreds of small language families. These areas together account for the majority of the world's languages, though not the majority of speakers. In the Americas, some of the largest language families include the Quechumaran, Arawak, and Tupi-Guarani families of South America, the Uto-Aztecan, Oto-Manguean, and Mayan of Mesoamerica, and the Na-Dene, Iroquoian, and Algonquian language families of North America. In Australia, most indigenous languages belong to the Pama-Nyungan family, whereas New Guinea is home to a large number of small families and isolates, as well as a number of Austronesian languages.


Language endangerment

Language endangerment occurs when a language is at risk of falling out of use as its speakers die out or shift to speaking another language. Language loss occurs when the language has no more native speakers, and becomes a dead language. If eventually no one speaks the language at all, it becomes an extinct language. While languages have always gone extinct throughout human history, they have been disappearing at an accelerated rate in the 20th and 21st centuries due to the processes of globalization and neo-colonialism, where the economically powerful languages dominate other languages.The more commonly spoken languages dominate the less commonly spoken languages, so the less commonly spoken languages eventually disappear from populations. Of the between 6,000 and 7,000 languages spoken as of 2010, between 5090% of those are expected to have become extinct by the year 2100. The top 20 languages, those spoken by more than 50 million speakers each, are spoken by 50% of the world's population, whereas many of the other languages are spoken by small communities, most of them with less than 10,000 speakers.The United Nations Educational, Scientific and Cultural Organization (UNESCO) operates with five levels of language endangerment: "safe", "vulnerable" (not spoken by children outside the home), "definitely endangered" (not spoken by children), "severely endangered" (only spoken by the oldest generations), and "critically endangered" (spoken by few members of the oldest generation, often semi-speakers). Notwithstanding claims that the world would be better off if most adopted a single common lingua franca, such as English or Esperanto, there is a consensus that the loss of languages harms the cultural diversity of the world. It is a common belief, going back to the biblical narrative of the tower of Babel in the Old Testament, that linguistic diversity causes political conflict, but this is contradicted by the fact that many of the world's major episodes of violence have taken place in situations with low linguistic diversity, such as the Yugoslav and American Civil War, or the genocide of Rwanda, whereas many of the most stable political units have been highly multilingual.Many projects aim to prevent or slow this loss by revitalizing endangered languages and promoting education and literacy in minority languages. Across the world, many countries have enacted specific legislation to protect and stabilize the language of indigenous speech communities. A minority of linguists have argued that language loss is a natural process that should not be counteracted, and that documenting endangered languages for posterity is sufficient.


Sponges, the members of the phylum Porifera (; meaning "pore bearer"), are a basal Metazoa (animal) clade as a sister of the Diploblasts. They are multicellular organisms that have bodies full of pores and channels allowing water to circulate through them, consisting of jelly-like mesohyl sandwiched between two thin layers of cells. The branch of zoology that studies sponges is known as spongiology.
Sponges have unspecialized cells that can transform into other types and that often migrate between the main cell layers and the mesohyl in the process. Sponges do not have nervous, digestive or circulatory systems. Instead, most rely on maintaining a constant water flow through their bodies to obtain food and oxygen and to remove wastes. Sponges were first to branch off the evolutionary tree from the common ancestor of all animals, making them the sister group of all other animals.


Etymology
The term sponge derives from the Ancient Greek word  (spngos).


Overview

Sponges are similar to other animals in that they are multicellular, heterotrophic, lack cell walls and produce sperm cells. Unlike other animals, they lack true tissues and organs. Some of them are radially symmetrical, but most are asymmetrical. The shapes of their bodies are adapted for maximal efficiency of water flow through the central cavity, where the water deposits nutrients and then leaves through a hole called the osculum. Many sponges have internal skeletons of spongin and/or spicules (skeletal-like fragments) of calcium carbonate or silicon dioxide. All sponges are sessile aquatic animals, meaning that they attach to an underwater surface and remain fixed in place (i.e., do not travel). Although there are freshwater species, the great majority are marine (salt-water) species, ranging in habitat from tidal zones to depths exceeding 8,800 m (5.5 mi).
Although most of the approximately 5,00010,000 known species of sponges feed on bacteria and other microscopic food in the water, some host photosynthesizing microorganisms as endosymbionts, and these alliances often produce more food and oxygen than they consume. A few species of sponges that live in food-poor environments have evolved as carnivores that prey mainly on small crustaceans.Most species use sexual reproduction, releasing sperm cells into the water to fertilize ova that in some species are released and in others are retained by the "mother." The fertilized eggs develop into larvae, which swim off in search of places to settle. Sponges are known for regenerating from fragments that are broken off, although this only works if the fragments include the right types of cells. A few species reproduce by budding. When environmental conditions become less hospitable to the sponges, for example as temperatures drop, many freshwater species and a few marine ones produce gemmules, "survival pods" of unspecialized cells that remain dormant until conditions improve; they then either form completely new sponges or recolonize the skeletons of their parents.In most sponges, an internal gelatinous matrix called mesohyl functions as an endoskeleton, and it is the only skeleton in soft sponges that encrust such hard surfaces as rocks. More commonly, the mesohyl is stiffened by mineral spicules, by spongin fibers, or both. Demosponges use spongin; many species have silica spicules, whereas some species have calcium carbonate exoskeletons. Demosponges constitute about 90% of all known sponge species, including all freshwater ones, and they have the widest range of habitats. Calcareous sponges, which have calcium carbonate spicules and, in some species, calcium carbonate exoskeletons, are restricted to relatively shallow marine waters where production of calcium carbonate is easiest. The fragile glass sponges, with "scaffolding" of silica spicules, are restricted to polar regions and the ocean depths where predators are rare. Fossils of all of these types have been found in rocks dated from 580 million years ago. In addition Archaeocyathids, whose fossils are common in rocks from 530 to 490 million years ago, are now regarded as a type of sponge.

The single-celled choanoflagellates resemble the choanocyte cells of sponges which are used to drive their water flow systems and capture most of their food. This along with phylogenetic studies of ribosomal molecules have been used as morphological evidence to suggest sponges are the sister group to the rest of animals. Some studies have shown that sponges do not form a monophyletic group, in other words do not include all and only the descendants of a common ancestor. Recent phylogenetic analyses suggested that comb jellies rather than sponges are the sister group to the rest of animals. However reanalysis of the data showed that the computer algorithms used for analysis were misled by the presence of specific ctenophore genes that were markedly different from those of other species, leaving sponges as either the sister group to all other animals, or an ancestral paraphyletic grade.The few species of demosponge that have entirely soft fibrous skeletons with no hard elements have been used by humans over thousands of years for several purposes, including as padding and as cleaning tools. By the 1950s, though, these had been overfished so heavily that the industry almost collapsed, and most sponge-like materials are now synthetic. Sponges and their microscopic endosymbionts are now being researched as possible sources of medicines for treating a wide range of diseases. Dolphins have been observed using sponges as tools while foraging.


Distinguishing features

Sponges constitute the phylum Porifera, and have been defined as sessile metazoans (multicelled immobile animals) that have water intake and outlet openings connected by chambers lined with choanocytes, cells with whip-like flagella. However, a few carnivorous sponges have lost these water flow systems and the choanocytes. All known living sponges can remold their bodies, as most types of their cells can move within their bodies and a few can change from one type to another.Even if a few sponges are able to produce mucus  which acts as a microbial barrier in all other animals  no sponge with the ability to secrete a functional mucus layer has been recorded. Without such a mucus layer their living tissue is covered by a layer of microbial symbionts, which can contribute up to 4050% of the sponge wet mass. This inability to prevent microbes from penetrating their porous tissue could be a major reason why they have never evolved a more complex anatomy.Like cnidarians (jellyfish, etc.) and ctenophores (comb jellies), and unlike all other known metazoans, sponges' bodies consist of a non-living jelly-like mass (mesohyl) sandwiched between two main layers of cells. Cnidarians and ctenophores have simple nervous systems, and their cell layers are bound by internal connections and by being mounted on a basement membrane (thin fibrous mat, also known as "basal lamina"). Sponges have no nervous systems, their middle jelly-like layers have large and varied populations of cells, and some types of cells in their outer layers may move into the middle layer and change their functions.


Basic structure


Cell types

A sponge's body is hollow and is held in shape by the mesohyl, a jelly-like substance made mainly of collagen and reinforced by a dense network of fibers also made of collagen. The inner surface is covered with choanocytes, cells with cylindrical or conical collars surrounding one flagellum per choanocyte. The wave-like motion of the whip-like flagella drives water through the sponge's body. All sponges have ostia, channels leading to the interior through the mesohyl, and in most sponges these are controlled by tube-like porocytes that form closable inlet valves. Pinacocytes, plate-like cells, form a single-layered external skin over all other parts of the mesohyl that are not covered by choanocytes, and the pinacocytes also digest food particles that are too large to enter the ostia, while those at the base of the animal are responsible for anchoring it.Other types of cell live and move within the mesohyl:
Lophocytes are amoeba-like cells that move slowly through the mesohyl and secrete collagen fibres.
Collencytes are another type of collagen-producing cell.
Rhabdiferous cells secrete polysaccharides that also form part of the mesohyl.
Oocytes and spermatocytes are reproductive cells.
Sclerocytes secrete the mineralized spicules ("little spines") that form the skeletons of many sponges and in some species provide some defense against predators.
In addition to or instead of sclerocytes, demosponges have spongocytes that secrete a form of collagen that polymerizes into spongin, a thick fibrous material that stiffens the mesohyl.
Myocytes ("muscle cells") conduct signals and cause parts of the animal to contract.
"Grey cells" act as sponges' equivalent of an immune system.
Archaeocytes (or amoebocytes) are amoeba-like cells that are totipotent, in other words each is capable of transformation into any other type of cell. They also have important roles in feeding and in clearing debris that block the ostia.Many larval sponges possess neuron-less eyes that are based on cryptochromes. They mediate phototaxic behavior.


Glass sponges' syncytia

Glass sponges present a distinctive variation on this basic plan. Their spicules, which are made of silica, form a scaffolding-like framework between whose rods the living tissue is suspended like a cobweb that contains most of the cell types. This tissue is a syncytium that in some ways behaves like many cells that share a single external membrane, and in others like a single cell with multiple nuclei. The mesohyl is absent or minimal. The syncytium's cytoplasm, the soupy fluid that fills the interiors of cells, is organized into "rivers" that transport nuclei, organelles ("organs" within cells) and other substances. Instead of choanocytes, they have further syncytia, known as choanosyncytia, which form bell-shaped chambers where water enters via perforations. The insides of these chambers are lined with "collar bodies", each consisting of a collar and flagellum but without a nucleus of its own. The motion of the flagella sucks water through passages in the "cobweb" and expels it via the open ends of the bell-shaped chambers.Some types of cells have a single nucleus and membrane each, but are connected to other single-nucleus cells and to the main syncytium by "bridges" made of cytoplasm. The sclerocytes that build spicules have multiple nuclei, and in glass sponge larvae they are connected to other tissues by cytoplasm bridges; such connections between sclerocytes have not so far been found in adults, but this may simply reflect the difficulty of investigating such small-scale features. The bridges are controlled by "plugged junctions" that apparently permit some substances to pass while blocking others.


Water flow and body structures

Most sponges work rather like chimneys: they take in water at the bottom and eject it from the osculum ("little mouth") at the top. Since ambient currents are faster at the top, the suction effect that they produce by Bernoulli's principle does some of the work for free. Sponges can control the water flow by various combinations of wholly or partially closing the osculum and ostia (the intake pores) and varying the beat of the flagella, and may shut it down if there is a lot of sand or silt in the water.Although the layers of pinacocytes and choanocytes resemble the epithelia of more complex animals, they are not bound tightly by cell-to-cell connections or a basal lamina (thin fibrous sheet underneath). The flexibility of these layers and re-modeling of the mesohyl by lophocytes allow the animals to adjust their shapes throughout their lives to take maximum advantage of local water currents.The simplest body structure in sponges is a tube or vase shape known as "asconoid", but this severely limits the size of the animal. The body structure is characterized by a stalk-like spongocoel surrounded by a single layer of choanocytes. If it is simply scaled up, the ratio of its volume to surface area increases, because surface increases as the square of length or width while volume increases proportionally to the cube. The amount of tissue that needs food and oxygen is determined by the volume, but the pumping capacity that supplies food and oxygen depends on the area covered by choanocytes. Asconoid sponges seldom exceed 1 mm (0.039 in) in diameter.

Some sponges overcome this limitation by adopting the "syconoid" structure, in which the body wall is pleated. The inner pockets of the pleats are lined with choanocytes, which connect to the outer pockets of the pleats by ostia. This increase in the number of choanocytes and hence in pumping capacity enables syconoid sponges to grow up to a few centimeters in diameter.
The "leuconoid" pattern boosts pumping capacity further by filling the interior almost completely with mesohyl that contains a network of chambers lined with choanocytes and connected to each other and to the water intakes and outlet by tubes. Leuconid sponges grow to over 1 m (3.3 ft) in diameter, and the fact that growth in any direction increases the number of choanocyte chambers enables them to take a wider range of forms, for example "encrusting" sponges whose shapes follow those of the surfaces to which they attach. All freshwater and most shallow-water marine sponges have leuconid bodies. The networks of water passages in glass sponges are similar to the leuconid structure.
In all three types of structure the cross-section area of the choanocyte-lined regions is much greater than that of the intake and outlet channels. This makes the flow slower near the choanocytes and thus makes it easier for them to trap food particles. For example, in Leuconia, a small leuconoid sponge about 10 centimetres (3.9 in) tall and 1 centimetre (0.39 in) in diameter, water enters each of more than 80,000 intake canals at 6 cm per minute. However, because Leuconia has more than 2 million flagellated chambers whose combined diameter is much greater than that of the canals, water flow through chambers slows to 3.6 cm per hour, making it easy for choanocytes to capture food. All the water is expelled through a single osculum at about 8.5 cm per second, fast enough to carry waste products some distance away.


Skeleton
In zoology a skeleton is any fairly rigid structure of an animal, irrespective of whether it has joints and irrespective of whether it is biomineralized. The mesohyl functions as an endoskeleton in most sponges, and is the only skeleton in soft sponges that encrust hard surfaces such as rocks. More commonly the mesohyl is stiffened by mineral spicules, by spongin fibers or both. Spicules, which are present in most but not all species, may be made of silica or calcium carbonate, and vary in shape from simple rods to three-dimensional "stars" with up to six rays. Spicules are produced by sclerocyte cells, and may be separate, connected by joints, or fused.Some sponges also secrete exoskeletons that lie completely outside their organic components. For example, sclerosponges ("hard sponges") have massive calcium carbonate exoskeletons over which the organic matter forms a thin layer with choanocyte chambers in pits in the mineral. These exoskeletons are secreted by the pinacocytes that form the animals' skins.


Vital functions


Movement
Although adult sponges are fundamentally sessile animals, some marine and freshwater species can move across the sea bed at speeds of 14 mm (0.0390.157 in) per day, as a result of amoeba-like movements of pinacocytes and other cells. A few species can contract their whole bodies, and many can close their oscula and ostia. Juveniles drift or swim freely, while adults are stationary.


Respiration, feeding and excretion

Sponges do not have distinct circulatory, respiratory, digestive, and excretory systems  instead the water flow system supports all these functions. They filter food particles out of the water flowing through them. Particles larger than 50 micrometers cannot enter the ostia and pinacocytes consume them by phagocytosis (engulfing and intracellular digestion). Particles from 0.5 m to 50 m are trapped in the ostia, which taper from the outer to inner ends. These particles are consumed by pinacocytes or by archaeocytes which partially extrude themselves through the walls of the ostia. Bacteria-sized particles, below 0.5 micrometers, pass through the ostia and are caught and consumed by choanocytes. Since the smallest particles are by far the most common, choanocytes typically capture 80% of a sponge's food supply. Archaeocytes transport food packaged in vesicles from cells that directly digest food to those that do not. At least one species of sponge has internal fibers that function as tracks for use by nutrient-carrying archaeocytes, and these tracks also move inert objects.It used to be claimed that glass sponges could live on nutrients dissolved in sea water and were very averse to silt. However, a study in 2007 found no evidence of this and concluded that they extract bacteria and other micro-organisms from water very efficiently (about 79%) and process suspended sediment grains to extract such prey. Collar bodies digest food and distribute it wrapped in vesicles that are transported by dynein "motor" molecules along bundles of microtubules that run throughout the syncytium.Sponges' cells absorb oxygen by diffusion from water into cells as water flows through body, into which carbon dioxide and other soluble waste products such as ammonia also diffuse. Archeocytes remove mineral particles that threaten to block the ostia, transport them through the mesohyl and generally dump them into the outgoing water current, although some species incorporate them into their skeletons.


Carnivorous sponges

A few species that live in waters where the supply of food particles is very poor prey on crustaceans and other small animals. So far only 137 species have been discovered. Most belong to the family Cladorhizidae, but a few members of the Guitarridae and Esperiopsidae are also carnivores. In most cases little is known about how they actually capture prey, although some species are thought to use either sticky threads or hooked spicules. Most carnivorous sponges live in deep waters, up to 8,840 m (5.49 mi), and the development of deep-ocean exploration techniques is expected to lead to the discovery of several more. However, one species has been found in Mediterranean caves at depths of 1723 m (5675 ft), alongside the more usual filter feeding sponges. The cave-dwelling predators capture crustaceans under 1 mm (0.039 in) long by entangling them with fine threads, digest them by enveloping them with further threads over the course of a few days, and then return to their normal shape; there is no evidence that they use venom.Most known carnivorous sponges have completely lost the water flow system and choanocytes. However, the genus Chondrocladia uses a highly modified water flow system to inflate balloon-like structures that are used for capturing prey.


Endosymbionts
Freshwater sponges often host green algae as endosymbionts within archaeocytes and other cells, and benefit from nutrients produced by the algae. Many marine species host other photosynthesizing organisms, most commonly cyanobacteria but in some cases dinoflagellates. Symbiotic cyanobacteria may form a third of the total mass of living tissue in some sponges, and some sponges gain 48% to 80% of their energy supply from these micro-organisms. In 2008 a University of Stuttgart team reported that spicules made of silica conduct light into the mesohyl, where the photosynthesizing endosymbionts live. Sponges that host photosynthesizing organisms are most common in waters with relatively poor supplies of food particles, and often have leafy shapes that maximize the amount of sunlight they collect.A recently discovered carnivorous sponge that lives near hydrothermal vents hosts methane-eating bacteria, and digests some of them.


"Immune" system
Sponges do not have the complex immune systems of most other animals. However, they reject grafts from other species but accept them from other members of their own species. In a few marine species, gray cells play the leading role in rejection of foreign material. When invaded, they produce a chemical that stops movement of other cells in the affected area, thus preventing the intruder from using the sponge's internal transport systems. If the intrusion persists, the grey cells concentrate in the area and release toxins that kill all cells in the area. The "immune" system can stay in this activated state for up to three weeks.


Reproduction


Asexual

Sponges have three asexual methods of reproduction: after fragmentation; by budding; and by producing gemmules. Fragments of sponges may be detached by currents or waves. They use the mobility of their pinacocytes and choanocytes and reshaping of the mesohyl to re-attach themselves to a suitable surface and then rebuild themselves as small but functional sponges over the course of several days. The same capabilities enable sponges that have been squeezed through a fine cloth to regenerate. A sponge fragment can only regenerate if it contains both collencytes to produce mesohyl and archeocytes to produce all the other cell types. A very few species reproduce by budding.Gemmules are "survival pods" which a few marine sponges and many freshwater species produce by the thousands when dying and which some, mainly freshwater species, regularly produce in autumn. Spongocytes make gemmules by wrapping shells of spongin, often reinforced with spicules, round clusters of archeocytes that are full of nutrients. Freshwater gemmules may also include phytosynthesizing symbionts. The gemmules then become dormant, and in this state can survive cold, drying out, lack of oxygen and extreme variations in salinity. Freshwater gemmules often do not revive until the temperature drops, stays cold for a few months and then reaches a near-"normal" level. When a gemmule germinates, the archeocytes round the outside of the cluster transform into pinacocytes, a membrane over a pore in the shell bursts, the cluster of cells slowly emerges, and most of the remaining archeocytes transform into other cell types needed to make a functioning sponge. Gemmules from the same species but different individuals can join forces to form one sponge. Some gemmules are retained within the parent sponge, and in spring it can be difficult to tell whether an old sponge has revived or been "recolonized" by its own gemmules.


Sexual
Most sponges are hermaphrodites (function as both sexes simultaneously), although sponges have no gonads (reproductive organs). Sperm are produced by choanocytes or entire choanocyte chambers that sink into the mesohyl and form spermatic cysts while eggs are formed by transformation of archeocytes, or of choanocytes in some species. Each egg generally acquires a yolk by consuming "nurse cells". During spawning, sperm burst out of their cysts and are expelled via the osculum. If they contact another sponge of the same species, the water flow carries them to choanocytes that engulf them but, instead of digesting them, metamorphose to an ameboid form and carry the sperm through the mesohyl to eggs, which in most cases engulf the carrier and its cargo.A few species release fertilized eggs into the water, but most retain the eggs until they hatch. There are four types of larvae, but all are balls of cells with an outer layer of cells whose flagellae or cilia enable the larvae to move. After swimming for a few days the larvae sink and crawl until they find a place to settle. Most of the cells transform into archeocytes and then into the types appropriate for their locations in a miniature adult sponge.Glass sponge embryos start by dividing into separate cells, but once 32 cells have formed they rapidly transform into larvae that externally are ovoid with a band of cilia round the middle that they use for movement, but internally have the typical glass sponge structure of spicules with a cobweb-like main syncitium draped around and between them and choanosyncytia with multiple collar bodies in the center. The larvae then leave their parents' bodies.


Life cycle
Sponges in temperate regions live for at most a few years, but some tropical species and perhaps some deep-ocean ones may live for 200 years or more. Some calcified demosponges grow by only 0.2 mm (0.0079 in) per year and, if that rate is constant, specimens 1 m (3.3 ft) wide must be about 5,000 years old. Some sponges start sexual reproduction when only a few weeks old, while others wait until they are several years old.


Coordination of activities
Adult sponges lack neurons or any other kind of nervous tissue. However, most species have the ability to perform movements that are coordinated all over their bodies, mainly contractions of the pinacocytes, squeezing the water channels and thus expelling excess sediment and other substances that may cause blockages. Some species can contract the osculum independently of the rest of the body. Sponges may also contract in order to reduce the area that is vulnerable to attack by predators. In cases where two sponges are fused, for example if there is a large but still unseparated bud, these contraction waves slowly become coordinated in both of the "Siamese twins". The coordinating mechanism is unknown, but may involve chemicals similar to neurotransmitters. However, glass sponges rapidly transmit electrical impulses through all parts of the syncytium, and use this to halt the motion of their flagella if the incoming water contains toxins or excessive sediment. Myocytes are thought to be responsible for closing the osculum and for transmitting signals between different parts of the body.Sponges contain genes very similar to those that contain the "recipe" for the post-synaptic density, an important signal-receiving structure in the neurons of all other animals. However, in sponges these genes are only activated in "flask cells" that appear only in larvae and may provide some sensory capability while the larvae are swimming. This raises questions about whether flask cells represent the predecessors of true neurons or are evidence that sponges' ancestors had true neurons but lost them as they adapted to a sessile lifestyle.


Ecology


Habitats

Sponges are worldwide in their distribution, living in a wide range of ocean habitats, from the polar regions to the tropics. Most live in quiet, clear waters, because sediment stirred up by waves or currents would block their pores, making it difficult for them to feed and breathe. The greatest numbers of sponges are usually found on firm surfaces such as rocks, but some sponges can attach themselves to soft sediment by means of a root-like base.Sponges are more abundant but less diverse in temperate waters than in tropical waters, possibly because organisms that prey on sponges are more abundant in tropical waters. Glass sponges are the most common in polar waters and in the depths of temperate and tropical seas, as their very porous construction enables them to extract food from these resource-poor waters with the minimum of effort. Demosponges and calcareous sponges are abundant and diverse in shallower non-polar waters.The different classes of sponge live in different ranges of habitat:


As primary producers
Sponges with photosynthesizing endosymbionts produce up to three times more oxygen than they consume, as well as more organic matter than they consume. Such contributions to their habitats' resources are significant along Australia's Great Barrier Reef but relatively minor in the Caribbean.


Defenses

Many sponges shed spicules, forming a dense carpet several meters deep that keeps away echinoderms which would otherwise prey on the sponges. They also produce toxins that prevent other sessile organisms such as bryozoans or sea squirts from growing on or near them, making sponges very effective competitors for living space. One of many examples includes ageliferin.
A few species, the Caribbean fire sponge Tedania ignis, cause a severe rash in humans who handle them. Turtles and some fish feed mainly on sponges. It is often said that sponges produce chemical defenses against such predators. However, experiments have been unable to establish a relationship between the toxicity of chemicals produced by sponges and how they taste to fish, which would diminish the usefulness of chemical defenses as deterrents. Predation by fish may even help to spread sponges by detaching fragments. However, some studies have shown fish showing a preference for non chemically defended sponges, and another study found that high levels of coral predation did predict the presence of chemically defended species.Glass sponges produce no toxic chemicals, and live in very deep water where predators are rare.


Predation
Sponge flies, also known as spongilla-flies (Neuroptera, Sisyridae), are specialist predators of freshwater sponges. The female lays her eggs on vegetation overhanging water. The larvae hatch and drop into the water where they seek out sponges to feed on. They use their elongated mouthparts to pierce the sponge and suck the fluids within. The larvae of some species cling to the surface of the sponge while others take refuge in the sponge's internal cavities. The fully grown larvae leave the water and spin a cocoon in which to pupate.


Bioerosion
The Caribbean chicken-liver sponge Chondrilla nucula secretes toxins that kill coral polyps, allowing the sponges to grow over the coral skeletons. Others, especially in the family Clionaidae, use corrosive substances secreted by their archeocytes to tunnel into rocks, corals and the shells of dead mollusks. Sponges may remove up to 1 m (3.3 ft) per year from reefs, creating visible notches just below low-tide level.


Diseases
Caribbean sponges of the genus Aplysina suffer from Aplysina red band syndrome. This causes Aplysina to develop one or more rust-colored bands, sometimes with adjacent bands of necrotic tissue. These lesions may completely encircle branches of the sponge. The disease appears to be contagious and impacts approximately 10 percent of A. cauliformis on Bahamian reefs. The rust-colored bands are caused by a cyanobacterium, but it is unknown whether this organism actually causes the disease.


Collaboration with other organisms
In addition to hosting photosynthesizing endosymbionts, sponges are noted for their wide range of collaborations with other organisms. The relatively large encrusting sponge Lissodendoryx colombiensis is most common on rocky surfaces, but has extended its range into seagrass meadows by letting itself be surrounded or overgrown by seagrass sponges, which are distasteful to the local starfish and therefore protect Lissodendoryx against them; in return the seagrass sponges get higher positions away from the sea-floor sediment.Shrimps of the genus Synalpheus form colonies in sponges, and each shrimp species inhabits a different sponge species, making Synalpheus one of the most diverse crustacean genera. Specifically, Synalpheus regalis utilizes the sponge not only as a food source, but also as a defense against other shrimp and predators. As many as 16,000 individuals inhabit a single loggerhead sponge, feeding off the larger particles that collect on the sponge as it filters the ocean to feed itself.


Sponge loop

Most sponges are detritivores which filter organic debris particles and microscopic life forms from ocean water. In particular, sponges occupy an important role as detritivores in coral reef food webs by recycling detritus to higher trophic levels.The hypothesis has been made that coral reef sponges facilitate the transfer of coral-derived organic matter to their associated detritivores via the production of sponge detritus, as shown in the diagram. Several sponge species are able to convert coral-derived DOM into sponge detritus, and transfer organic matter produced by corals further up the reef food web. Corals release organic matter as both dissolved and particulate mucus, as well as cellular material such as expelled Symbiodinium.Organic matter could be transferred from corals to sponges by all these pathways, but DOM likely makes up the largest fraction, as the majority (56 to 80%) of coral mucus dissolves in the water column, and coral loss of fixed carbon due to expulsion of Symbiodinium is typically negligible (0.01%) compared with mucus release (up to ~40%). Coral-derived organic matter could also be indirectly transferred to sponges via bacteria, which can also consume coral mucus.


Sponge holobiont
Besides a one to one symbiotic relationship, it is possible for a host to become symbiotic with a microbial consortia.  Sponges are able to host a wide range of microbial communities that can also be very specific.  The microbial communities that form a symbiotic relationship with the sponge can amount to as much as 35% of the biomass of its host.  The term for this specific symbiotic relationship, where a microbial consortia pairs with a host is called a holobiotic relationship.  The sponge as well as the microbial community associated with it will produce a large range of secondary metabolites that help protect it against predators through mechanisms such as chemical defense.Some of these relationships include endosymbionts within bacteriocyte cells, and cyanobacteria or microalgae found below the pinacoderm cell layer where they are able to receive the highest amount of light, used for phototrophy.  They can host over 50 different microbial phyla and candidate phyla, including Alphaprotoebacteria, Actinobacteria, Chloroflexi, Nitrospirae, Cyanobacteria, the taxa Gamma-, the candidate phylum Poribacteria, and Thaumarchaea.


Systematics and evolutionary history


Taxonomy
Linnaeus, who classified most kinds of sessile animals as belonging to the order Zoophyta in the class Vermes, mistakenly identified the genus Spongia as plants in the order Algae. For a long time thereafter sponges were assigned to a separate subkingdom, Parazoa ("beside the animals"), separate from the Eumetazoa which formed the rest of the kingdom Animalia. They have been regarded as a paraphyletic phylum, from which the higher animals have evolved. Other research indicates Porifera is monophyletic.The phylum Porifera is further divided into classes mainly according to the composition of their skeletons:
Hexactinellida (glass sponges) have silicate spicules, the largest of which have six rays and may be individual or fused. The main components of their bodies are syncytia in which large numbers of cell share a single external membrane.
Calcarea have skeletons made of calcite, a form of calcium carbonate, which may form separate spicules or large masses. All the cells have a single nucleus and membrane.
Most Demospongiae have silicate spicules or spongin fibers or both within their soft tissues. However, a few also have massive external skeletons made of aragonite, another form of calcium carbonate. All the cells have a single nucleus and membrane.
Archeocyatha are known only as fossils from the Cambrian period.In the 1970s, sponges with massive calcium carbonate skeletons were assigned to a separate class, Sclerospongiae, otherwise known as "coralline sponges".
However, in the 1980s it was found that these were all members of either the Calcarea or the Demospongiae.So far scientific publications have identified about 9,000 poriferan species, of which: about 400 are glass sponges; about 500 are calcareous species; and the rest are demosponges. However, some types of habitat, vertical rock and cave walls and galleries in rock and coral boulders, have been investigated very little, even in shallow seas.


Classes
Sponges were traditionally distributed in three classes: calcareous sponges (Calcarea), glass sponges (Hexactinellida) and demosponges (Demospongiae). However, studies have shown that the Homoscleromorpha, a group thought to belong to the Demospongiae, is actually phylogenetically well separated. Therefore, they have recently been recognized as the fourth class of sponges.Sponges are divided into classes mainly according to the composition of their skeletons: These are arranged in evolutionary order as shown below in ascending order of their evolution from top to bottom:


Fossil record

Although molecular clocks and biomarkers suggest sponges existed well before the Cambrian explosion of life, silica spicules like those of demosponges are absent from the fossil record until the Cambrian. One unsubstantiated report exists of spicules in rocks dated around 750 million years ago. Well-preserved fossil sponges from about 580 million years ago in the Ediacaran period have been found in the Doushantuo Formation. These fossils, which include spicules, pinacocytes, porocytes, archeocytes, sclerocytes and the internal cavity, have been classified as demosponges. Fossils of glass sponges have been found from around 540 million years ago in rocks in Australia, China and Mongolia. Early Cambrian sponges from Mexico belonging to the genus Kiwetinokia show evidence of fusion of several smaller spicules to form a single large spicule. Calcium carbonate spicules of calcareous sponges have been found in Early Cambrian rocks from about 530 to 523 million years ago in Australia. Other probable demosponges have been found in the Early Cambrian Chengjiang fauna, from 525 to 520 million years ago. Freshwater sponges appear to be much younger, as the earliest known fossils date from the Mid-Eocene period about 48 to 40 million years ago. Although about 90% of modern sponges are demosponges, fossilized remains of this type are less common than those of other types because their skeletons are composed of relatively soft spongin that does not fossilize well.
Earliest sponge symbionts are known from the early Silurian.A chemical tracer is 24-isopropylcholestane, which is a stable derivative of 24-isopropylcholesterol, which is said to be produced by demosponges but not by eumetazoans ("true animals", i.e. cnidarians and bilaterians). Since choanoflagellates are thought to be animals' closest single-celled relatives, a team of scientists examined the biochemistry and genes of one choanoflagellate species. They concluded that this species could not produce 24-isopropylcholesterol but that investigation of a wider range of choanoflagellates would be necessary in order to prove that the fossil 24-isopropylcholestane could only have been produced by demosponges.
Although a previous publication reported traces of the chemical 24-isopropylcholestane in ancient rocks dating to 1,800 million years ago, recent research using a much more accurately dated rock series has revealed that these biomarkers only appear before the end of the Marinoan glaciation approximately 635 million years ago, and that "Biomarker analysis has yet to reveal any convincing evidence for ancient sponges pre-dating the first globally extensive Neoproterozoic glacial episode (the Sturtian, ~713 million years ago in Oman)". While it has been argued that this 'sponge biomarker' could have originated from marine algae, recent research suggests that the algae's ability to produce this biomarker evolved only in the Carboniferous; as such, the biomarker remains strongly supportive of the presence of demosponges in the Cryogenian.Archaeocyathids, which some classify as a type of coralline sponge, are very common fossils in rocks from the Early Cambrian about 530 to 520 million years ago, but apparently died out by the end of the Cambrian 490 million years ago.
It has been suggested that they were produced by: sponges; cnidarians; algae; foraminiferans; a completely separate phylum of animals, Archaeocyatha; or even a completely separate kingdom of life, labeled Archaeata or Inferibionta. Since the 1990s archaeocyathids have been regarded as a distinctive group of sponges.

It is difficult to fit chancelloriids into classifications of sponges or more complex animals. An analysis in 1996 concluded that they were closely related to sponges on the grounds that the detailed structure of chancellorid sclerites ("armor plates") is similar to that of fibers of spongin, a collagen protein, in modern keratose (horny) demosponges such as Darwinella. However, another analysis in 2002 concluded that chancelloriids are not sponges and may be intermediate between sponges and more complex animals, among other reasons because their skins were thicker and more tightly connected than those of sponges. In 2008 a detailed analysis of chancelloriids' sclerites concluded that they were very similar to those of halkieriids, mobile bilaterian animals that looked like slugs in chain mail and whose fossils are found in rocks from the very Early Cambrian to the Mid Cambrian. If this is correct, it would create a dilemma, as it is extremely unlikely that totally unrelated organisms could have developed such similar sclerites independently, but the huge difference in the structures of their bodies makes it hard to see how they could be closely related.


Relationships to other animal groups

In the 1990s sponges were widely regarded as a monophyletic group, all of them having descended from a common ancestor that was itself a sponge, and as the "sister-group" to all other metazoans (multi-celled animals), which themselves form a monophyletic group. On the other hand, some 1990s analyses also revived the idea that animals' nearest evolutionary relatives are choanoflagellates, single-celled organisms very similar to sponges' choanocytes  which would imply that most Metazoa evolved from very sponge-like ancestors and therefore that sponges may not be monophyletic, as the same sponge-like ancestors may have given rise both to modern sponges and to non-sponge members of Metazoa.Analyses since 2001 have concluded that Eumetazoa (more complex than sponges) are more closely related to particular groups of sponges than to the rest of the sponges. Such conclusions imply that sponges are not monophyletic, because the last common ancestor of all sponges would also be a direct ancestor of the Eumetazoa, which are not sponges. A study in 2001 based on comparisons of ribosome DNA concluded that the most fundamental division within sponges was between glass sponges and the rest, and that Eumetazoa are more closely related to calcareous sponges, those with calcium carbonate spicules, than to other types of sponge. In 2007 one analysis based on comparisons of RNA and another based mainly on comparison of spicules concluded that demosponges and glass sponges are more closely related to each other than either is to calcareous sponges, which in turn are more closely related to Eumetazoa.Other anatomical and biochemical evidence links the Eumetazoa with Homoscleromorpha, a sub-group of demosponges. A comparison in 2007 of nuclear DNA, excluding glass sponges and comb jellies, concluded that: Homoscleromorpha are most closely related to Eumetazoa; calcareous sponges are the next closest; the other demosponges are evolutionary "aunts" of these groups; and the chancelloriids, bag-like animals whose fossils are found in Cambrian rocks, may be sponges. The sperm of Homoscleromorpha share with those of Eumetazoa features that those of other sponges lack. In both Homoscleromorpha and Eumetazoa layers of cells are bound together by attachment to a carpet-like basal membrane composed mainly of "type IV" collagen, a form of collagen not found in other sponges  although the spongin fibers that reinforce the mesohyl of all demosponges is similar to "type IV" collagen.

The analyses described above concluded that sponges are closest to the ancestors of all Metazoa, of all multi-celled animals including both sponges and more complex groups. However, another comparison in 2008 of 150 genes in each of 21 genera, ranging from fungi to humans but including only two species of sponge, suggested that comb jellies (ctenophora) are the most basal lineage of the Metazoa included in the sample. If this is correct, either modern comb jellies developed their complex structures independently of other Metazoa, or sponges' ancestors were more complex and all known sponges are drastically simplified forms. The study recommended further analyses using a wider range of sponges and other simple Metazoa such as Placozoa. The results of such an analysis, published in 2009, suggest that a return to the previous view may be warranted. 'Family trees' constructed using a combination of all available data  morphological, developmental and molecular  concluded that the sponges are in fact a monophyletic group, and with the cnidarians form the sister group to the bilaterians.A very large and internally consistent alignment of 1,719 proteins at the metazoan scale, published in 2017, showed that (i) sponges  represented by Homoscleromorpha, Calcarea, Hexactinellida, and Demospongiae  are monophyletic, (ii) sponges are sister-group to all other multicellular animals, (iii) ctenophores emerge as the second-earliest branching animal lineage, and (iv) placozoans emerge as the third animal lineage, followed by cnidarians sister-group to bilaterians.


Notable spongiologists


Use


By dolphins
A report in 1997 described use of sponges  as a tool by bottlenose dolphins in Shark Bay in Western Australia. A dolphin will attach a marine sponge to its rostrum, which is presumably then used to protect it when searching for food in the sandy sea bottom. The behavior, known as sponging, has only been observed in this bay, and is almost exclusively shown by females. A study in 2005 concluded that mothers teach the behavior to their daughters, and that all the sponge-users are closely related, suggesting that it is a fairly recent innovation.


By humans


Skeleton

The calcium carbonate or silica spicules of most sponge genera make them too rough for most uses, but two genera, Hippospongia and Spongia, have soft, entirely fibrous skeletons. Early Europeans used soft sponges for many purposes, including padding for helmets, portable drinking utensils and municipal water filters. Until the invention of synthetic sponges, they were used as cleaning tools, applicators for paints and ceramic glazes and discreet contraceptives. However, by the mid-20th century, over-fishing brought both the animals and the industry close to extinction.
See also sponge diving.
Many objects with sponge-like textures are now made of substances not derived from poriferans. Synthetic sponges include personal and household cleaning tools, breast implants, and contraceptive sponges. Typical materials used are cellulose foam, polyurethane foam, and less frequently, silicone foam.
The luffa "sponge", also spelled loofah, which is commonly sold for use in the kitchen or the shower, is not derived from an animal but mainly from the fibrous "skeleton" of the sponge gourd (Luffa aegyptiaca, Cucurbitaceae).


Antibiotic compounds
Sponges have medicinal potential due to the presence in sponges themselves or their microbial symbionts of chemicals that may be used to control viruses, bacteria, tumors and fungi.


Other biologically active compounds

Lacking any protective shell or means of escape, sponges have evolved to synthesize a variety of unusual compounds. One such class is the oxidized fatty acid derivatives called oxylipins. Members of this family have been found to have anti-cancer, anti-bacterial and anti-fungal properties. One example isolated from the Okinawan plakortis sponges, plakoridine A, has shown potential as a cytotoxin to murine lymphoma cells.


A silent film is a film with no synchronized recorded sound (and in particular, no audible dialogue). In silent films for entertainment, the plot may be conveyed by the use of title cards, written indications of the plot and key dialogue lines. The idea of combining motion pictures with recorded sound is nearly as old as film itself, but because of the technical challenges involved, the introduction of synchronized dialogue became practical only in the late 1920s with the perfection of the Audion amplifier tube and the advent of the Vitaphone system.The term "silent film" is something of a misnomer, as these films were almost always accompanied by live sounds. During the silent era that existed from the mid-1890s to the late 1920s, a pianist, theater organistor even, in large cities, a small orchestrawould often play music to accompany the films. Pianists and organists would play either from sheet music, or improvisation. Sometimes a person would even narrate the intertitle cards for the audience.  Though at the time the technology to synchronize sound with the film did not exist, music was seen as an essential part of the viewing experience. The term is also frequently used to describe sound-era films that have a recorded music-only soundtrack without dialogue, such as City Lights and The Artist.
The term silent film is a retronyma term created to retroactively distinguish something. Early sound films, starting with The Jazz Singer in 1927, were variously referred to as the "talkies", "sound films", or "talking pictures". Within a decade, the widespread production of silent films for popular entertainment had ceased, and the industry had moved fully into the sound era, in which movies were accompanied by synchronized sound recordings of spoken dialogue, music and sound effects.
Most early motion pictures are considered lost because the nitrate film used in that era was extremely unstable and flammable. Additionally, many films were deliberately destroyed because they had negligible continuing financial value in this era. It has often been claimed that around 75 percent of silent films produced in the US have been lost, though these estimates may be inaccurate due to a lack of numerical data.


Elements and beginnings (18331936)

The earliest precursors to film began with image projection through the use of a device known as the magic lantern, which utilized a glass lens, a shutter, and a persistent light source (such as a powerful lantern) to project images from glass slides onto a wall. These slides were originally hand-painted, but, after the advent of photography in the 19th century, still photographs were sometimes used. Thus the invention of a practical photography apparatus preceded cinema by only fifty years.The next significant step toward the invention of cinema was the development of an understanding of image movement. Simulations of movement date as far back as to 1828only four years after Paul Roget discovered the phenomenon he called "Persistence of Vision". Roget showed that when a series of still images is shown at a considerable speed in front of a viewer's eye, the images merge into one registered image that appears to show movement. This is an optical illusion, since the image is not actually moving. This experience was further demonstrated through Roget's introduction of the thaumatrope, a device that spun at a fairly high speed a disk with an image on its surface.The invention of film allowed for true motion pictures rather than optical illusions. The film, which consisted of flexible and transparent celluloid, could record split second pictures. Developed by tienne-Jules Marey, he was one of the first to experiment with film. In 1882, Marey developed a camera that could take 12 photographs per second (superimposed into one image) of animals or humans in motion.

The three features necessary for motion pictures to work were "a camera with sufficiently high shutter speed, a filmstrip capable of taking multiple exposures swiftly, and means of projecting the developed images on a screen". The first projected proto-movie was made by Eadweard Muybridge between 1877 and 1880. Muybridge set up a row of cameras along a racetrack and timed image exposures to capture the many stages of a horse's gallop. The oldest surviving film (of the genre called "pictorial realism") was created by Louis Le Prince in 1888. It was a two-second film of people walking in "Oakwood streets" garden, titled Roundhay Garden Scene. The development of American inventor Thomas Edison's Kinetograph, a photographic device that captured sequential images, and his Kinetoscope, a device for viewing those images, allowed for the creation and exhibition of short films. Edison also made a business of selling Kinetograph and Kinetoscope equipment, which laid the foundation for widespread film production.Due to Edison's lack of securing an international patent on his film inventions, similar devices were "invented" around the world. In France, for example, Auguste and Louis Lumire created the Cinmatographe, which proved to be a more portable and practical device than both of Edison's as it combined a camera, film processor, and projector in one unit. In contrast to Edison's "peepshow"-style kinetoscope, which only one person could watch through a viewer, the cinematograph allowed simultaneous viewing by multiple people. Their first film, Sortie de l'usine Lumire de Lyon, shot in 1894, is considered the first true motion picture. The invention of celluloid film, which was strong and flexible, greatly facilitated the making of motion pictures (although the celluloid was highly flammable and decayed quickly). This film was 35 mm wide and was pulled using four sprocket holes, which became the industry standard (see 35 mm film). This doomed the cinematograph, which only worked with film with a single sprocket hole.


Silent film era

The work of Muybridge, Marey, and Le Prince laid the foundation for future development of motion picture cameras, projectors and transparent celluloid film, which lead to the development of cinema as we know it today. American inventor George Eastman, who had first manufactured photographic dry plates in 1878, made headway on a stable type of celluloid film in 1888.
The art of motion pictures grew into full maturity in the "silent era" (1894 in film  1929 in film). The height of the silent era (from the early 1910s in film to the late 1920s) was a particularly fruitful period, full of artistic innovation. The film movements of Classical Hollywood as well as French Impressionism, German Expressionism, and Soviet Montage began in this period. Silent filmmakers pioneered the art form to the extent that virtually every style and genre of film-making of the 20th and 21st centuries has its artistic roots in the silent era. The silent era was also a pioneering one from a technical point of view. Three-point lighting, the close-up, long shot, panning, and continuity editing all became prevalent long before silent films were replaced by "talking pictures" or "talkies" in the late 1920s. Some scholars claim that the artistic quality of cinema decreased for several years, during the early 1930s, until film directors, actors, and production staff adapted fully to the new "talkies" around the mid 1930s.The visual quality of silent moviesespecially those produced in the 1920swas often high, but there remains a widely held misconception that these films were primitive, or are barely watchable by modern standards. This misconception comes from the general public's unfamiliarity with the medium, as well as from carelessness on the part of the industry. Most silent films are poorly preserved, leading to their deterioration, and well-preserved films are often played back at the wrong speed or suffer from censorship cuts and missing frames and scenes, giving the appearance of poor editing. Many silent films exist only in second- or third-generation copies, often made from already damaged and neglected film stock. Another widely held misconception is that silent films lacked color. In fact, color was far more prevalent in silent films than in the first few decades of sound films. By the early 1920s, 80 per cent of movies could be seen in some sort of color, usually in the form of film tinting or toning or even hand coloring, but also with fairly natural two-color processes such as Kinemacolor and Technicolor. Traditional colorization processes ceased with the adoption of sound-on-film technology. Traditional film colorization, all of which involved the use of dyes in some form, interfered with the high resolution required for built-in recorded sound, and were therefore abandoned. The innovative three-strip technicolor process introduced in the mid-30s was costly and fraught with limitations, and color would not have the same prevalence in film as it did in the silents for nearly four decades.


Intertitles

As motion pictures gradually increased in running time, a replacement was needed for the in-house interpreter who would explain parts of the film to the audience. Because silent films had no synchronized sound for dialogue, onscreen intertitles were used to narrate story points, present key dialogue and sometimes even comment on the action for the audience. The title writer became a key professional in silent film and was often separate from the scenario writer who created the story. Intertitles (or titles as they were generally called at the time) "often were graphic elements themselves, featuring illustrations or abstract decorations that commented on the action".


Live music and other sound accompaniment
Showings of silent films almost always featured live music, starting with the guitarist, at the first public projection of movies by the Lumire brothers on December 28, 1895, in Paris. This was furthered in 1896 by the first motion-picture exhibition in the United States at Koster and Bial's Music Hall in New York City. At this event, Edison set the precedent that all exhibitions should be accompanied by an orchestra. From the beginning, music was recognized as essential, contributing atmosphere, and giving the audience vital emotional cues. (Musicians sometimes played on film sets during shooting for similar reasons.) However, depending on the size of the exhibition site, musical accompaniment could drastically change in scale. Small town and neighborhood movie theatres usually had a pianist. Beginning in the mid-1910s, large city theaters tended to have organists or ensembles of musicians. Massive theater organs, which were designed to fill a gap between a simple piano soloist and a larger orchestra, had a wide range of special effects. Theatrical organs such as the famous "Mighty Wurlitzer" could simulate some orchestral sounds along with a number of percussion effects such as bass drums and cymbals, and sound effects ranging from "train and boat whistles [to] car horns and bird whistles; ... some could even simulate pistol shots, ringing phones, the sound of surf, horses' hooves, smashing pottery, [and] thunder and rain".Musical scores for early silent films were either improvised or compiled of classical or theatrical repertory music. Once full features became commonplace, however, music was compiled from photoplay music by the pianist, organist, orchestra conductor or the movie studio itself, which included a cue sheet with the film. These sheets were often lengthy, with detailed notes about effects and moods to watch for. Starting with the mostly original score composed by Joseph Carl Breil for D. W. Griffith's groundbreaking, but racially devastating epic The Birth of a Nation (1915), it became relatively common for the biggest-budgeted films to arrive at the exhibiting theater with original, specially composed scores. However, the first designated full-blown scores had in fact been composed in 1908, by Camille Saint-Sans for The Assassination of the Duke of Guise, and by Mikhail Ippolitov-Ivanov for Stenka Razin.
When organists or pianists used sheet music, they still might add improvisational flourishes to heighten the drama on screen. Even when special effects were not indicated in the score, if an organist was playing a theater organ capable of an unusual sound effect such as "galloping horses", it would be used during scenes of dramatic horseback chases.
At the height of the silent era, movies were the single largest source of employment for instrumental musicians, at least in the United States. However, the introduction of talkies, coupled with the roughly simultaneous onset of the Great Depression, was devastating to many musicians.
A number of countries devised other ways of bringing sound to silent films. The early cinema of Brazil, for example, featured fitas cantatas (singing films), filmed operettas with singers performing behind the screen. In Japan, films had not only live music but also the benshi, a live narrator who provided commentary and character voices. The benshi became a central element in Japanese film, as well as providing translation for foreign (mostly American) movies. The popularity of the benshi was one reason why silent films persisted well into the 1930s in Japan.


Score restorations from 1980 to the present
Few film scores survive intact from the silent period, and musicologists are still confronted by questions when they attempt to precisely reconstruct those that remain. Scores used in current reissues or screenings of silent films may be complete reconstructions of compositions, newly composed for the occasion, assembled from already existing music libraries, or improvised on the spot in the manner of the silent-era theater musician.
Interest in the scoring of silent films fell somewhat out of fashion during the 1960s and 1970s. There was a belief in many college film programs and repertory cinemas that audiences should experience silent film as a pure visual medium, undistracted by music. This belief may have been encouraged by the poor quality of the music tracks found on many silent film reprints of the time. Since around 1980, there has been a revival of interest in presenting silent films with quality musical scores (either reworkings of period scores or cue sheets, or the composition of appropriate original scores). An early effort of this kind was Kevin Brownlow's 1980 restoration of Abel Gance's Napolon (1927), featuring a score by Carl Davis. A slightly re-edited and sped-up version of Brownlow's restoration was later distributed in the United States by Francis Ford Coppola, with a live orchestral score composed by his father Carmine Coppola.
In 1984, an edited restoration of Metropolis (1927) was released with a new rock music score by producer-composer Giorgio Moroder. Although the contemporary score, which included pop songs by Freddie Mercury, Pat Benatar, and Jon Anderson of Yes, was controversial, the door had been opened for a new approach to the presentation of classic silent films.
Today, a large number of soloists, music ensembles, and orchestras perform traditional and contemporary scores for silent films internationally. The legendary theater organist Gaylord Carter continued to perform and record his original silent film scores until shortly before his death in 2000; some of those scores are available on DVD reissues. Other purveyors of the traditional approach include organists such as Dennis James and pianists such as Neil Brand, Gnter Buchwald, Philip C. Carli, Ben Model, and William P. Perry. Other contemporary pianists, such as Stephen Horne and Gabriel Thibaudeau, have often taken a more modern approach to scoring.
Orchestral conductors such as Carl Davis and Robert Israel have written and compiled scores for numerous silent films; many of these have been featured in showings on Turner Classic Movies or have been released on DVD. Davis has composed new scores for classic silent dramas such as The Big Parade (1925) and Flesh and the Devil (1927). Israel has worked mainly in silent comedy, scoring the films of Harold Lloyd, Buster Keaton, Charley Chase and others. Timothy Brock has restored many of Charlie Chaplin's scores, in addition to composing new scores.
Contemporary music ensembles are helping to introduce classic silent films to a wider audience through a broad range of musical styles and approaches. Some performers create new compositions using traditional musical instruments, while others add electronic sounds, modern harmonies, rhythms, improvisation and sound design elements to enhance the viewing experience. Among the contemporary ensembles in this category are Un Drame Musical Instantan, Alloy Orchestra, Club Foot Orchestra, Silent Orchestra, Mont Alto Motion Picture Orchestra, Minima and the Caspervek Trio, RPM Orchestra. Donald Sosin and his wife Joanna Seaton specialize in adding vocals to silent films, particularly where there is onscreen singing that benefits from hearing the actual song being performed. Films in this category include Griffith's Lady of the Pavements with Lupe Vlez, Edwin Carewe's Evangeline with Dolores del Ro, and Rupert Julian's The Phantom of the Opera with Mary Philbin and Virginia Pearson.The Silent Film Sound and Music Archive digitizes music and cue sheets written for silent film and makes it available for use by performers, scholars, and enthusiasts.


Acting techniques

Silent-film actors emphasized body language and facial expression so that the audience could better understand what an actor was feeling and portraying on screen. Much silent film acting is apt to strike modern-day audiences as simplistic or campy. The melodramatic acting style was in some cases a habit actors transferred from their former stage experience. Vaudeville was an especially popular origin for many American silent film actors. The pervading presence of stage actors in film was the cause of this outburst from director Marshall Neilan in 1917: "The sooner the stage people who have come into pictures get out, the better for the pictures."  In other cases, directors such as John Griffith Wray required their actors to deliver larger-than-life expressions for emphasis. As early as 1914, American viewers had begun to make known their preference for greater naturalness on screen.Silent films became less vaudevillian in the mid-1910s, as the differences between stage and screen became apparent. Due to the work of directors such as D. W. Griffith, cinematography became less stage-like, and the development of the close up allowed for understated and realistic acting. Lillian Gish has been called film's "first true actress" for her work in the period, as she pioneered new film performing techniques, recognizing the crucial differences between stage and screen acting. Directors such as Albert Capellani and Maurice Tourneur began to insist on naturalism in their films. By the mid-1920s many American silent films had adopted a more naturalistic acting style, though not all actors and directors accepted naturalistic, low-key acting straight away; as late as 1927, films featuring expressionistic acting styles, such as Metropolis, were still being released. Greta Garbo, who made her debut in 1926, would become known for her naturalistic acting.
According to Anton Kaes, a silent film scholar from the University of California, Berkeley, American silent cinema began to see a shift in acting techniques between 1913 and 1921, influenced by techniques found in German silent film. This is mainly attributed to the influx of emigrants from the Weimar Republic, "including film directors, producers, cameramen, lighting and stage technicians, as well as actors and actresses".


Projection speed
Until the standardization of the projection speed of 24 frames per second (fps) for sound films between 1926 and 1930, silent films were shot at variable speeds (or "frame rates") anywhere from 12 to 40 fps, depending on the year and studio. "Standard silent film speed" is often said to be 16 fps as a result of the Lumire brothers' Cinmatographe, but industry practice varied considerably; there was no actual standard. William Kennedy Laurie Dickson, an Edison employee, settled on the astonishingly fast 40 frames per second. Additionally, cameramen of the era insisted that their cranking technique was exactly 16 fps, but modern examination of the films shows this to be in error, that they often cranked faster. Unless carefully shown at their intended speeds silent films can appear unnaturally fast or slow. However, some scenes were intentionally undercranked during shooting to accelerate the actionparticularly for comedies and action films.

Slow projection of a cellulose nitrate base film carried a risk of fire, as each frame was exposed for a longer time to the intense heat of the projection lamp; but there were other reasons to project a film at a greater pace. Often projectionists received general instructions from the distributors on the musical director's cue sheet as to how fast particular reels or scenes should be projected. In rare instances, usually for larger productions, cue sheets produced specifically for the projectionist provided a detailed guide to presenting the film. Theaters alsoto maximize profitsometimes varied projection speeds depending on the time of day or popularity of a film, or to fit a film into a prescribed time slot.All motion-picture film projectors require a moving shutter to block the light whilst the film is moving, otherwise the image is smeared in the direction of the movement. However this shutter causes the image to flicker, and images with low rates of flicker are very unpleasant to watch. Early studies by Thomas Edison for his Kinetoscope machine determined that any rate below 46 images per second "will strain the eye". and this holds true for projected images under normal cinema conditions also. The solution adopted for the Kinetoscope was to run the film at over 40 frames/sec, but this was expensive for film. However, by using projectors with dual- and triple-blade shutters the flicker rate is multiplied two or three times higher than the number of film frames  each frame being flashed two or three times on screen. A three-blade shutter projecting a 16 fps film will slightly surpass Edison's figure, giving the audience 48 images per second. During the silent era projectors were commonly fitted with 3-bladed shutters. Since the introduction of sound with its 24 frame/sec standard speed 2-bladed shutters have become the norm for 35 mm cinema projectors, though three-bladed shutters have remained standard on 16 mm and 8 mm projectors, which are frequently used to project amateur footage shot at 16 or 18 frames/sec. A 35 mm film frame rate of 24 fps translates to a film speed of 456 millimetres (18.0 in) per second. One 1,000-foot (300 m) reel requires 11 minutes and 7 seconds to be projected at 24 fps, while a 16 fps projection of the same reel would take 16 minutes and 40 seconds, or 304 millimetres (12.0 in) per second.In the 1950s, many telecine conversions of silent films at grossly incorrect frame rates for broadcast television may have alienated viewers. Film speed is often a vexed issue among scholars and film buffs in the presentation of silents today, especially when it comes to DVD releases of restored films, such as the case of the 2002 restoration of Metropolis.


Tinting

With the lack of natural color processing available, films of the silent era were frequently dipped in dyestuffs and dyed various shades and hues to signal a mood or represent a time of day. Hand tinting dates back to 1895 in the United States with Edison's release of selected hand-tinted prints of Butterfly Dance. Additionally, experiments in color film started as early as in 1909, although it took a much longer time for color to be adopted by the industry and an effective process to be developed. Blue represented night scenes, yellow or amber meant day. Red represented fire and green represented a mysterious atmosphere. Similarly, toning of film (such as the common silent film generalization of sepia-toning) with special solutions replaced the silver particles in the film stock with salts or dyes of various colors. A combination of tinting and toning could be used as an effect that could be striking.
Some films were hand-tinted, such as Annabelle Serpentine Dance (1894), from Edison Studios. In it, Annabelle Whitford, a young dancer from Broadway, is dressed in white veils that appear to change colors as she dances. This technique was designed to capture the effect of the live performances of Loie Fuller, beginning in 1891, in which stage lights with colored gels turned her white flowing dresses and sleeves into artistic movement. Hand coloring was often used in the early "trick" and fantasy films of Europe, especially those by Georges Mlis. Mlis began hand-tinting his work as early as 1897 and the 1899 Cendrillion (Cinderella) and 1900 Jeanne d'Arc (Joan of Arc) provide early examples of hand-tinted films in which the color was a critical part of the scenography or mise en scne; such precise tinting used the workshop of Elisabeth Thuillier in Paris, with teams of female artists adding layers of color to each frame by hand rather than using a more common (and less expensive) process of stenciling. A newly restored version of Mlis' A Trip to the Moon, originally released in 1902, shows an exuberant use of color designed to add texture and interest to the image.Comments by an American distributor in a 1908 film-supply catalog further underscore France's continuing dominance in the field of hand-coloring films during the early silent era. The distributor offers for sale at varying prices "High-Class" motion pictures by Path, Urban-Eclipse, Gaumont, Kalem, Itala Film, Ambrosio Film, and Selig. Several of the longer, more prestigious films in the catalog are offered in both standard black-and-white "plain stock" as well as in "hand-painted" color. A plain-stock copy, for example, of the 1907 release Ben Hur is offered for $120 ($3,415 USD today), while a colored version of the same 1000-foot, 15-minute film costs $270 ($7,683) including the extra $150 coloring charge, which amounted to 15 cents more per foot. Although the reasons for the cited extra charge were likely obvious to customers, the distributor explains why his catalog's colored films command such significantly higher prices and require more time for delivery. His explanation also provides insight into the general state of film-coloring services in the United States by 1908:

 The coloring of moving picture films is a line of work which cannot be satisfactorily performed in the United States. In view of the enormous amount of labor involved which calls for individual hand painting of every one of sixteen pictures to the foot or 16,000 separate pictures for each 1,000 feet of film very few American colorists will undertake the work at any price.As film coloring has progressed much more rapidly in France than in any other country, all of our coloring is done for us by the best coloring establishment in Paris and we have found that we obtain better quality, cheaper prices and quicker deliveries, even in coloring American made films, than if the work were done elsewhere.
By the beginning of the 1910s, with the onset of feature-length films, tinting was used as another mood setter, just as commonplace as music. The director D. W. Griffith displayed a constant interest and concern about color, and used tinting as a special effect in many of his films. His 1915 epic, The Birth of a Nation, used a number of colors, including amber, blue, lavender, and a striking red tint for scenes such as the "burning of Atlanta" and the ride of the Ku Klux Klan at the climax of the picture. Griffith later invented a color system in which colored lights flashed on areas of the screen to achieve a color.
With the development of sound-on-film technology and the industry's acceptance of it, tinting was abandoned altogether, because the dyes used in the tinting process interfered with the soundtracks present on film strips.


Early studios
The early studios were located in the New York City area. Edison Studios were first in West Orange, New Jersey (1892), they were moved to the Bronx, New York (1907). Fox (1909) and Biograph (1906) started in Manhattan, with studios in St George, Staten Island. Others films were shot in Fort Lee, New Jersey. In December 1908, Edison led the formation of the Motion Picture Patents Company in an attempt to control the industry and shut out smaller producers. The "Edison Trust", as it was nicknamed, was made up of Edison, Biograph, Essanay Studios, Kalem Company, George Kleine Productions, Lubin Studios, Georges Mlis, Path, Selig Studios, and Vitagraph Studios, and dominated distribution through the General Film Company. This company dominated the industry as both a vertical and horizontal monopoly and is a contributing factor in studios' migration to the West Coast. The Motion Picture Patents Co. and the General Film Co. were found guilty of antitrust violation in October 1915, and were dissolved.
The Thanhouser film studio was founded in New Rochelle, New York, in 1909 by American theatrical impresario Edwin Thanhouser. The company produced and released 1,086 films between 1910 and 1917, including the first film serial ever, The Million Dollar Mystery, released in 1914. The first westerns were filmed at Fred Scott's Movie Ranch in South Beach, Staten Island. Actors costumed as cowboys and Native Americans galloped across Scott's movie ranch set, which had a frontier main street, a wide selection of stagecoaches and a 56-foot stockade. The island provided a serviceable stand-in for locations as varied as the Sahara desert and a British cricket pitch. War scenes were shot on the plains of Grasmere, Staten Island. The Perils of Pauline and its even more popular sequel The Exploits of Elaine were filmed largely on the island. So was the 1906 blockbuster Life of a Cowboy, by Edwin S. Porter Company and filming moved to the West Coast around 1912.


Top-grossing silent films in the United States

The following are American films from the silent film era that had earned the highest gross income as of 1932. The amounts given are gross rentals (the distributor's share of the box-office) as opposed to exhibition gross.


During the sound era


Transition
Although attempts to create sync-sound motion pictures go back to the Edison lab in 1896, only from the early 1920s were the basic technologies such as vacuum tube amplifiers and high-quality loudspeakers available. The next few years saw a race to design, implement, and market several rival sound-on-disc and sound-on-film sound formats, such as Photokinema (1921), Phonofilm (1923), Vitaphone (1926), Fox Movietone (1927) and RCA Photophone (1928).
Warner Bros. was the first studio to accept sound as an element in film production and utilize Vitaphone, a sound-on-disc technology, to do so. The studio then released The Jazz Singer in 1927, which marked the first commercially successful sound film, but silent films were still the majority of features released in both 1927 and 1928, along with so-called goat-glanded films: silents with a subsection of sound film inserted. Thus the modern sound film era may be regarded as coming to dominance beginning in 1929.
For a listing of notable silent era films, see List of years in film for the years between the beginning of film and 1928. The following list includes only films produced in the sound era with the specific artistic intention of being silent.

City Girl, F. W. Murnau, 1930
Earth, Aleksandr Dovzhenko, 1930
The Silent Enemy, H.P. Carver, 1930
Borderline, Kenneth Macpherson, 1930
City Lights, Charlie Chaplin, 1931
Tabu, F. W. Murnau, 1931
I Was Born, But..., Yasujir Ozu, 1932
Passing Fancy, Yasujir Ozu, 1933
The Goddess, Wu Yonggang, 1934
A Story of Floating Weeds, Yasujir Ozu, 1934
Legong, Henri de la Falaise, 1935
An Inn in Tokyo, Yasujir Ozu, 1935
Happiness, Aleksandr Medvedkin, 1935
Cosmic Voyage, Vasili Zhuravlov, 1936


Later homages
Several filmmakers have paid homage to the comedies of the silent era, including Charlie Chaplin, with Modern Times (1936), Orson Welles with Too Much Johnson (1938), Jacques Tati with Les Vacances de Monsieur Hulot (1953), Pierre Etaix with The Suitor (1962), and Mel Brooks with Silent Movie (1976). Taiwanese director Hou Hsiao-hsien's acclaimed drama Three Times (2005) is silent during its middle third, complete with intertitles; Stanley Tucci's The Impostors has an opening silent sequence in the style of early silent comedies. Brazilian filmmaker Renato Falco's Margarette's Feast (2003) is silent. Writer / Director Michael Pleckaitis puts his own twist on the genre with Silent (2007). While not silent, the Mr. Bean television series and movies have used the title character's non-talkative nature to create a similar style of humor. A lesser-known example is Jrme Savary's La fille du garde-barrire (1975), an homage to silent-era films that uses intertitles and blends comedy, drama, and explicit sex scenes (which led to it being refused a cinema certificate by the British Board of Film Classification).
In 1990, Charles Lane directed and starred in Sidewalk Stories, a low budget salute to sentimental silent comedies, particularly Charlie Chaplin's The Kid.
The German film Tuvalu (1999) is mostly silent; the small amount of dialog is an odd mix of European languages, increasing the film's universality. Guy Maddin won awards for his homage to Soviet era silent films with his short The Heart of the World after which he made a feature-length silent, Brand Upon the Brain! (2006), incorporating live Foley artists, narration and orchestra at select showings. Shadow of the Vampire (2000) is a highly fictionalized depiction of the filming of Friedrich Wilhelm Murnau's classic silent vampire movie Nosferatu (1922). Werner Herzog honored the same film in his own version, Nosferatu: Phantom der Nacht (1979).
Some films draw a direct contrast between the silent film era and the era of talkies. Sunset Boulevard shows the disconnect between the two eras in the character of Norma Desmond, played by silent film star Gloria Swanson, and Singin' in the Rain deals with Hollywood artists adjusting to the talkies. Peter Bogdanovich's 1976 film Nickelodeon deals with the turmoil of silent filmmaking in Hollywood during the early 1910s, leading up to the release of D. W. Griffith's epic The Birth of a Nation (1915).
In 1999, the Finnish filmmaker Aki Kaurismki produced Juha in black-and-white, which captures the style of a silent film, using intertitles in place of spoken dialogue. Special release prints with titles in several different languages were produced for international distribution. In India, the film Pushpak (1988), starring Kamal Hassan, was a black comedy entirely devoid of dialog. The Australian film Doctor Plonk (2007), was a silent comedy directed by Rolf de Heer. Stage plays have drawn upon silent film styles and sources. Actor/writers Billy Van Zandt & Jane Milmore staged their Off-Broadway slapstick comedy Silent Laughter as a live action tribute to the silent screen era. Geoff Sobelle and Trey Lyford created and starred in All Wear Bowlers (2004), which started as an homage to Laurel and Hardy then evolved to incorporate life-sized silent film sequences of Sobelle and Lyford who jump back and forth between live action and the silver screen. The animated film Fantasia (1940), which is eight different animation sequences set to music, can be considered a silent film, with only one short scene involving dialogue. The espionage film The Thief (1952) has music and sound effects, but no dialogue, as do Thierry Zno's 1974 Vase de Noces and Patrick Bokanowski's 1982 The Angel.
In 2005, the H. P. Lovecraft Historical Society produced a silent film version of Lovecraft's story The Call of Cthulhu. This film maintained a period-accurate filming style, and was received as both "the best HPL adaptation to date" and, referring to the decision to make it as a silent movie, "a brilliant conceit".The French film The Artist (2011), written and directed by Michel Hazanavicius, plays as a silent film and is set in Hollywood during the silent era. It also includes segments of fictitious silent films starring its protagonists.The Japanese vampire film Sanguivorous (2011) is not only done in the style of a silent film, but even toured with live orchestral accompiment. Eugene Chadbourne has been among those who have played live music for the film.Blancanieves is a 2012 Spanish black-and-white silent fantasy drama film written and directed by Pablo Berger.
The American feature-length silent film Silent Life started in 2006, features performances by Isabella Rossellini and Galina Jovovich, mother of Milla Jovovich, will premiere in 2013. The film is based on the life of the silent screen icon Rudolph Valentino, known as the Hollywood's first "Great Lover". After the emergency surgery, Valentino loses his grip of reality and begins to see the recollection of his life in Hollywood from a perspective of a coma  as a silent film shown at a movie palace, the magical portal between life and eternity, between reality and illusion.The Picnic is a 2012 short film made in the style of two-reel silent melodramas and comedies. It was part of the exhibit, No Spectators: The Art of Burning Man, a 2018-2019 exhibit curated by the Renwick Gallery of the Smithsonian American Art Museum. The film was shown inside a miniature 12-seat Art Deco movie palace on wheels called The Capitol Theater, created by Oakland, Ca. art collective Five Ton Crane.
Right There is a 2013 short film that is an homage to silent film comedies.
The 2015 British animated film Shaun the Sheep Movie based on Shaun the Sheep was released to positive reviews and was a box office success. Aardman Animations also produced Morph and Timmy Time as well as many other silent short films.
The American Theatre Organ Society pays homage to the music of silent films, as well as the theatre organs that played such music. With over 75 local chapters, the organization seeks to preserve and promote theater organs and music, as an art form.The Globe International Silent Film Festival (GISFF) is an annual event focusing on image and atmosphere in cinema which takes place in a reputable university or academic environment every year and is a platform for showcasing and judging films from filmmakers who are active in this field.  In 2018 film director Christopher Annino shot the now internationally award-winning feature silent film of its kind Silent Times. The film gives homage to many of the characters from the 1920s including Officer Keystone played by David Blair, and Enzio Marchello who portrays a Charlie Chaplin character. Silent Times has won best silent film at the Oniros Film Festival. Set in a small New England town, the story centers on Oliver Henry III (played by Westerly native Geoff Blanchette), a small-time crook turned vaudeville theater owner. From humble beginnings in England, he immigrates to the US in search of happiness and fast cash. He becomes acquainted with people from all walks of life, from burlesque performers, mimes, hobos to classy flapper girls, as his fortunes rise and his life spins ever more out of control.


Preservation and lost films

The vast majority of the silent films produced in the late 19th and early 20th centuries are considered lost. According to a September 2013 report published by the United States Library of Congress, some 70 percent of American silent feature films fall into this category. There are numerous reasons for this number being so high. Some films have been lost unintentionally, but most silent films were destroyed on purpose. Between the end of the silent era and the rise of home video, film studios would often discard large numbers of silent films out of a desire to free up storage in their archives, assuming that they had lost the cultural relevance and economic value to justify the amount of space they occupied. Additionally, due to the fragile nature of the nitrate film stock which was used to shoot and distribute silent films, many motion pictures have irretrievably deteriorated or have been lost in accidents, including fires (because nitrate is highly flammable and can spontaneously combust when stored improperly). Examples of such incidents include the 1965 MGM vault fire and the 1937 Fox vault fire, both of which incited catastrophic losses of films. Many such films not completely destroyed survive only partially, or in badly damaged prints. Some lost films, such as London After Midnight (1927), lost in the MGM fire, have been the subject of considerable interest by film collectors and historians.
Major silent films presumed lost include:

Saved from the Titanic (1912), which featured survivors of the disaster;
The Life of General Villa, starring Pancho Villa himself
The Apostle, the first animated feature film (1917)
Cleopatra (1917)
Gold Diggers (1923)
Kiss Me Again (1925)
Arirang (1926)
The Great Gatsby (1926)
London After Midnight (1927)
Gentlemen Prefer Blondes (1928)Though most lost silent films will never be recovered, some have been discovered in film archives or private collections. Discovered and preserved versions may be editions made for the home rental market of the 1920s and 1930s that are discovered in estate sales, etc. The degradation of old film stock can be slowed through proper archiving, and films can be transferred to safety film stock or to digital media for preservation. The preservation of silent films has been a high priority for historians and archivists.


Dawson Film Find
Dawson City, in the Yukon territory of Canada, was once the end of the distribution line for many films. In 1978, a cache of more than 500 reels of nitrate film was discovered during the excavation of a vacant lot formerly the site of the Dawson Amateur Athletic Association, which had started showing films at their recreation centre in 1903. Works by Pearl White, Helen Holmes, Grace Cunard, Lois Weber, Harold Lloyd, Douglas Fairbanks, and Lon Chaney, among others, were included, as well as many newsreels. The titles were stored at the local library until 1929 when the flammable nitrate was used as landfill in a condemned swimming pool. Having spent 50 years under the permafrost of the Yukon, the reels turned out to be extremely well preserved. Owing to its dangerous chemical volatility, the historical find was moved by military transport to Library and Archives Canada and the US Library of Congress for storage (and transfer to safety film). A documentary about the find, Dawson City: Frozen Time was released in 2016.


A cowboy is an animal herder who tends cattle on ranches in North America, traditionally on horseback, and often performs a multitude of other ranch-related tasks. The historic American cowboy of the late 19th century arose from the vaquero traditions of northern Mexico and became a figure of special significance and legend. A subtype, called a wrangler, specifically tends the horses used to work cattle. In addition to ranch work, some cowboys work for or participate in rodeos. Cowgirls, first defined as such in the late 19th century, had a less-well documented historical role, but in the modern world work at identical tasks and have obtained considerable respect for their achievements. Cattle handlers in many other parts of the world, particularly South America and Australia, perform work similar to the cowboy.
The cowboy has deep historic roots tracing back to Spain and the earliest European settlers of the Americas. Over the centuries, differences in terrain and climate, and the influence of cattle-handling traditions from multiple cultures, created several distinct styles of equipment, clothing and animal handling. As the ever-practical cowboy adapted to the modern world, his equipment and techniques also adapted, though many classic traditions are preserved.


Etymology and mainstream usage

The English word cowboy has an origin from several earlier terms that referred to both age and to cattle or cattle-tending work.
The English word cowboy was derived from vaquero, a Spanish word for an individual who managed cattle while mounted on horseback.  Vaquero was derived from vaca, meaning "cow," which came from the Latin word vacca. Cowboy was first used in print by Jonathan Swift in 1725, and was used in the British Isles from 1820 to 1850 to describe young boys who tended the family or community cows. Originally though, the English word "cowherd" was used to describe a cattle herder (similar to "shepherd", a sheep herder), and often referred to a pre-adolescent or early adolescent boy, who usually worked on foot. This word is very old in the English language, originating prior to the year 1000.By 1849 "cowboy" had developed its modern sense as an adult cattle handler of the American West. Variations on the word appeared later. "Cowhand" appeared in 1852, and "cowpoke" in 1881, originally restricted to the individuals who prodded cattle with long poles to load them onto railroad cars for shipping. Names for a cowboy in American English include buckaroo, cowpoke, cowhand, and cowpuncher. Another English word for a cowboy, buckaroo, is an anglicization of vaquero.(Spanish pronunciation: [bakeo]).Today, "cowboy" is a term common throughout the west and particularly in the Great Plains and Rocky Mountains, "buckaroo" is used primarily in the Great Basin and California, and "cowpuncher" mostly in Texas and surrounding states.Equestrianism required skills and an investment in horses and equipment rarely available to or entrusted to a child, though in some cultures boys rode a donkey while going to and from pasture. In antiquity, herding of sheep, cattle and goats was often the job of minors, and still is a task for young people in various Developing World cultures.
Because of the time and physical ability needed to develop necessary skills, both historic and modern cowboys often began as an adolescent. Historically, cowboys earned wages as soon as they developed sufficient skill to be hired (often as young as 12 or 13). If not crippled by injury, cowboys may handle cattle or horses for a lifetime. In the United States, a few women also took on the tasks of ranching and learned the necessary skills, though the "cowgirl" (discussed below) did not become widely recognized or acknowledged until the close of the 19th century. On western ranches today, the working cowboy is usually an adult. Responsibility for herding cattle or other livestock is no longer considered suitable for children or early adolescents. However, both boys and girls growing up in a ranch environment often learn to ride horses and perform basic ranch skills as soon as they are physically able, usually under adult supervision. Such youths, by their late teens, are often given responsibilities for "cowboy" work on the ranch.


Other historic word uses
"Cowboy" was used during the American Revolution to describe American fighters who opposed the movement for independence.  Claudius Smith, an outlaw identified with the Loyalist cause, was called the "Cow-boy of the Ramapos" due to his penchant for stealing oxen, cattle and horses from colonists and giving them to the British. In the same period, a number of guerrilla bands operated in Westchester County, which marked the dividing line between the British and American forces. These groups were made up of local farmhands who would ambush convoys and carry out raids on both sides. There were two separate groups: the "skinners" fought for the pro-independence side, while the "cowboys" supported the British.In the Tombstone, Arizona area during the 1880s, the term "cowboy" or "cow-boy" was used pejoratively to describe men who had been implicated in various crimes. One loosely organized band was dubbed "The Cowboys," and profited from smuggling cattle, alcohol, and tobacco across the U.S.Mexico border. The San Francisco Examiner wrote in an editorial, "Cowboys [are] the most reckless class of outlaws in that wild country ... infinitely worse than the ordinary robber." It became an insult in the area to call someone a "cowboy", as it suggested he was a horse thief, robber, or outlaw. Cattlemen were generally called herders or ranchers. The Cowboys' activities were ultimately curtailed by the Gunfight at the O.K. Corral and the resulting Earp Vendetta Ride.


History
The origins of the cowboy tradition come from Spain, beginning with the hacienda system of medieval Spain. This style of cattle ranching spread throughout much of the Iberian peninsula, and later was imported to the Americas. Both regions possessed a dry climate with sparse grass, thus large herds of cattle required vast amounts of land to obtain sufficient forage. The need to cover distances greater than a person on foot could manage gave rise to the development of the horseback-mounted vaquero.


Spanish roots

Various aspects of the Spanish equestrian tradition can be traced back to Islamic rule in Spain, including Moorish elements such as the use of Oriental-type horses, the la jineta riding style characterized by a shorter stirrup, solid-treed saddle and use of spurs, the heavy noseband or hackamore, (Arabic akma, Spanish jaquima) and other horse-related equipment and techniques. Certain aspects of the Arabic tradition, such as the hackamore, can in turn be traced to roots in ancient Persia.During the 16th century, the Conquistadors and other Spanish settlers brought their cattle-raising traditions as well as both horses and domesticated cattle to the Americas, starting with their arrival in what today is Mexico and Florida. The traditions of Spain were transformed by the geographic, environmental and cultural circumstances of New Spain, which later became Mexico and the Southwestern United States. In turn, the land and people of the Americas also saw dramatic changes due to Spanish influence.
The arrival of horses was particularly significant, as equines had been extinct in the Americas since the end of the prehistoric ice age. However, horses quickly multiplied in America and became crucial to the success of the Spanish and later settlers from other nations. The earliest horses were originally of Andalusian, Barb and Arabian ancestry, but a number of uniquely American horse breeds developed in North and South America through selective breeding and by natural selection of animals that escaped to the wild. The Mustang and other colonial horse breeds are now called "wild," but in reality are feral horsesdescendants of domesticated animals.


Vaqueros

Though popularly considered American, the traditional cowboy began with the Spanish tradition, which evolved further in what today is Mexico and the Southwestern United States into the vaquero of northern Mexico and the charro of the Jalisco and Michoacn regions. While most hacendados (ranch owners) were ethnically Spanish criollos, many early vaqueros were Native Americans trained to work for the Spanish missions in caring for the mission herds. Vaqueros went north with livestock. In 1598, Don Juan de Oate sent an expedition across the Rio Grande into New Mexico, bringing along 7000 head of cattle. From this beginning, vaqueros drove cattle from New Mexico and later Texas to Mexico City. Mexican traditions spread both South and North, influencing equestrian traditions from Argentina to Canada.


Rise of the cowboy
As English-speaking traders and settlers expanded westward, English and Spanish traditions, language and culture merged to some degree. Before the MexicanAmerican War in 1848, New England merchants who traveled by ship to California encountered both hacendados and vaqueros, trading manufactured goods for the hides and tallow produced from vast cattle ranches. American traders along what later became known as the Santa Fe Trail had similar contacts with vaquero life. Starting with these early encounters, the lifestyle and language of the vaquero began a transformation which merged with English cultural traditions and produced what became known in American culture as the "cowboy".The arrival of English-speaking settlers in Texas began in 1821. Rip Ford described the country between Laredo and Corpus Christi as inhabited by "... countless droves of mustangs and ... wild cattle ... abandoned by Mexicans when they were ordered to evacuate the country between the Nueces and the Rio Grande by General Valentin Canalizo ... the horses and cattle abandoned invited the raids the Texians made upon this territory. California, on the other hand, did not see a large influx of settlers from the United States until after the MexicanAmerican War. However, in slightly different ways, both areas contributed to the evolution of the iconic American cowboy. Particularly with the arrival of railroads and an increased demand for beef in the wake of the American Civil War, older traditions combined with the need to drive cattle from the ranches where they were raised to the nearest railheads, often hundreds of miles away.Black cowboys in the American West accounted for up to 25 percent of workers in the range-cattle industry from the 1860s to 1880s, estimated to be between 6,000 and 9,000 workers. Typically former slaves or children of former slaves, many black men had skills in cattle handling and headed West at the end of the Civil War.By the 1880s, the expansion of the cattle industry resulted in a need for additional open range. Thus many ranchers expanded into the northwest, where there were still large tracts of unsettled grassland. Texas cattle were herded north, into the Rocky Mountain west and the Dakotas. The cowboy adapted much of his gear to the colder conditions, and westward movement of the industry also led to intermingling of regional traditions from California to Texas, often with the cowboy taking the most useful elements of each.


Mustang running
Mustang-runners or Mesteeros were cowboys and vaqueros who caught, broke and drove Mustangs to market in Mexico, and later American territories of what is now Northern Mexico, Texas, New Mexico and California. They caught the Mustangs that roamed the Great Plains and the San Joaquin Valley of California, and later in the Great Basin, from the 18th century to the early 20th century.


Roundups

Large numbers of cattle lived in a semi-feral, or semi-wild state on the open range and were left to graze, mostly untended, for much of the year. In many cases, different ranchers formed "associations" and grazed their cattle together on the same range. In order to determine the ownership of individual animals, they were marked with a distinctive brand, applied with a hot iron, usually while the cattle were still young calves. The primary cattle breed seen on the open range was the Longhorn, descended from the original Spanish Longhorns imported in the 16th century, though by the late 19th century, other breeds of cattle were also brought west, including the meatier Hereford, and often were crossbred with Longhorns.In order to find young calves for branding, and to sort out mature animals intended for sale, ranchers would hold a roundup, usually in the spring. A roundup required a number of specialized skills on the part of both cowboys and horses. Individuals who separated cattle from the herd required the highest level of skill and rode specially trained "cutting" horses, trained to follow the movements of cattle, capable of stopping and turning faster than other horses. Once cattle were sorted, most cowboys were required to rope young calves and restrain them to be branded and (in the case of most bull calves) castrated. Occasionally it was also necessary to restrain older cattle for branding or other treatment.
A large number of horses were needed for a roundup. Each cowboy would require three to four fresh horses in the course of a day's work. Horses themselves were also rounded up. It was common practice in the west for young foals to be born of tame mares, but allowed to grow up "wild" in a semi-feral state on the open range. There were also "wild" herds, often known as Mustangs. Both types were rounded up, and the mature animals tamed, a process called horse breaking, or "bronco-busting," (var. "bronc busting") usually performed by cowboys who specialized in training horses. In some cases, extremely brutal methods were used to tame horses, and such animals tended to never be completely reliable. However, other cowboys became aware of the need to treat animals in a more humane fashion and modified their horse training methods, often re-learning techniques used by the vaqueros, particularly those of the Californio tradition. Horses trained in a gentler fashion were more reliable and useful for a wider variety of tasks.
Informal competition arose between cowboys seeking to test their cattle and horse-handling skills against one another, and thus, from the necessary tasks of the working cowboy, the sport of rodeo developed.


Cattle drives

Prior to the mid-19th century, most ranchers primarily raised cattle for their own needs and to sell surplus meat and hides locally. There was also a limited market for hides, horns, hooves, and tallow in assorted manufacturing processes. While Texas contained vast herds of stray, free-ranging cattle available for free to anyone who could round them up, prior to 1865, there was little demand for beef. However, at the end of the American Civil War, Philip Danforth Armour opened a meat packing plant in Chicago, which became known as Armour and Company. With the expansion of the meat packing industry, the demand for beef increased significantly. By 1866, cattle could be sold to northern markets for as much as $40 per head, making it potentially profitable for cattle, particularly from Texas, to be herded long distances to market.The first large-scale effort to drive cattle from Texas to the nearest railhead for shipment to Chicago occurred in 1866, when many Texas ranchers banded together to drive their cattle to the closest point that railroad tracks reached, which at that time was in Sedalia, Missouri. However, farmers in eastern Kansas, afraid that Longhorns would transmit cattle fever to local animals as well as trample crops, formed groups that threatened to beat or shoot cattlemen found on their lands. Therefore, the 1866 drive failed to reach the railroad, and the cattle herds were sold for low prices. However, in 1867, a cattle shipping facility was built west of farm country around the railhead at Abilene, Kansas, and became a center of cattle shipping, loading over 36,000 head of cattle that year. The route from Texas to Abilene became known as the Chisholm Trail, after Jesse Chisholm, who marked out the route. It ran through present-day Oklahoma, which then was Indian Territory. Later, other trails forked off to different railheads, including those at Dodge City and Wichita, Kansas. By 1877, the largest of the cattle-shipping boom towns, Dodge City, Kansas, shipped out 500,000 head of cattle.Cattle drives had to strike a balance between speed and the weight of the cattle. While cattle could be driven as far as 25 miles (40 km) in a single day, they would lose so much weight that they would be hard to sell when they reached the end of the trail. Usually they were taken shorter distances each day, allowed periods to rest and graze both at midday and at night. On average, a herd could maintain a healthy weight moving about 15 miles (25 km) per day. Such a pace meant that it would take as long as two months to travel from a home ranch to a railhead. The Chisholm trail, for example, was 1,000 miles (1,600 km) miles long.On average, a single herd of cattle on a drive numbered about 3,000 head. To herd the cattle, a crew of at least 10 cowboys was needed, with three horses per cowboy. Cowboys worked in shifts to watch the cattle 24 hours a day, herding them in the proper direction in the daytime and watching them at night to prevent stampedes and deter theft. The crew also included a cook, who drove a chuck wagon, usually pulled by oxen, and a horse wrangler to take charge of the remuda, or herd of spare horses. The wrangler on a cattle drive was often a very young cowboy or one of lower social status, but the cook was a particularly well-respected member of the crew, as not only was he in charge of the food, he also was in charge of medical supplies and had a working knowledge of practical medicine.


End of the open range

Barbed wire, an innovation of the 1880s, allowed cattle to be confined to designated areas to prevent overgrazing of the range. In Texas and surrounding areas, increased population required ranchers to fence off their individual lands. In the north, overgrazing stressed the open range, leading to insufficient winter forage for the cattle and starvation, particularly during the harsh winter of 18861887, when hundreds of thousands of cattle died across the Northwest, leading to collapse of the cattle industry. By the 1890s, barbed-wire fencing was also standard in the northern plains, railroads had expanded to cover most of the nation, and meat packing plants were built closer to major ranching areas, making long cattle drives from Texas to the railheads in Kansas unnecessary. Hence, the age of the open range was gone and large cattle drives were over. Smaller cattle drives continued at least into the 1940s, as ranchers, prior to the development of the modern cattle truck, still needed to herd cattle to local railheads for transport to stockyards and packing plants. Meanwhile, ranches multiplied all over the developing West, keeping cowboy employment high, if still low-paid, but also somewhat more settled.


Culture


Ethnicity

American cowboys were drawn from multiple sources. By the late 1860s, following the American Civil War and the expansion of the cattle industry, former soldiers from both the Union and Confederacy came west, seeking work, as did large numbers of restless white men in general. A significant number of African-American freedmen also were drawn to cowboy life, in part because there was not quite as much discrimination in the west as in other areas of American society at the time. A significant number of Mexicans and American Indians already living in the region also worked as cowboys. Later, particularly after 1890, when American policy promoted "assimilation" of Indian people, some Indian boarding schools also taught ranching skills. Today, some Native Americans in the western United States own cattle and small ranches, and many are still employed as cowboys, especially on ranches located near Indian reservations. The "Indian Cowboy" is also part of the rodeo circuit.
Because cowboys ranked low in the social structure of the period, there are no firm figures on the actual proportion of various races. One writer states that cowboys were "... of two classesthose recruited from Texas and other States on the eastern slope; and Mexicans, from the south-western region ..." Census records suggest that about 15% of all cowboys were of African-American ancestryranging from about 25% on the trail drives out of Texas, to very few in the northwest. Similarly, cowboys of Mexican descent also averaged about 15% of the total, but were more common in Texas and the southwest. Some estimates suggest that in the late 19th century, one out of every three cowboys was a Mexican vaquero, and 20% may have been African-American. Other estimates place the number of African-American cowboys as high as 25 percent.Regardless of ethnicity, most cowboys came from lower social classes and the pay was poor. The average cowboy earned approximately a dollar a day, plus food, and, when near the home ranch, a bed in the bunkhouse, usually a barracks-like building with a single open room.


Social world
Over time, the cowboys of the American West developed a personal culture of their own, a blend of frontier and Victorian values that even retained vestiges of chivalry. Such hazardous work in isolated conditions also bred a tradition of self-dependence and individualism, with great value put on personal honesty, exemplified in songs and poetry. The cowboy often worked in an all-male environment, particularly on cattle drives, and in the frontier west, men often significantly outnumbered women.However, some men were also drawn to the frontier because they were attracted to men. At times, in a region where men outnumbered women, even social events normally attended by both sexes were at times all male, and men could be found partnering up with one another for dances. Homosexual acts between young, unmarried men occurred, but cowboys culture itself was and remains deeply homophobic. Though anti-sodomy laws were common in the Old West, they often were only selectively enforced.


Development of the modern cowboy image

The traditions of the working cowboy were further etched into the minds of the general public with the development of Wild West Shows in the late 19th and early 20th centuries, which showcased and romanticized the life of both cowboys and Native Americans. Beginning in the 1920s and continuing to the present day, Western movies popularized the cowboy lifestyle but also formed persistent stereotypes, both positive and negative. In some cases, the cowboy and the violent gunslinger are often associated with one another. On the other hand, some actors who portrayed cowboys promoted positive values, such as the "cowboy code" of Gene Autry, that encouraged honorable behavior, respect and patriotism. Historian Robert K. DeArment draws a connection between the popularized Western code and the stereotypical rowdy cowboy image to that of the "subculture of violence" of drovers in Old West Texas, that was influenced itself by the Southern code duello.Likewise, cowboys in movies were often shown fighting with American Indians. However most armed conflicts occurred between Native people and cavalry units of the U.S. Army.  Relations between cowboys and Native Americans were varied but generally not particularly friendly. Native people usually allowed cattle herds to pass through for a toll of ten cents a head, but raided cattle drives and ranches in times of active white-Native conflict or food shortages. In the 1860s, for example, the Comanche created problems in Western Texas. Similar attacks also occurred with the Apache, Cheyenne and Ute Indians. Cowboys were armed against both predators and human thieves, and often used their guns to run off people of any race who attempted to steal, or rustle cattle.
In reality, working ranch hands past and present had very little time for anything other than the constant, hard work involved in maintaining a ranch.


Cowgirls

The history of women in the west, and women who worked on cattle ranches in particular, is not as well documented as that of men. However, institutions such as the National Cowgirl Museum and Hall of Fame have made significant efforts in recent years to gather and document the contributions of women.There are few records mentioning girls or women working to drive cattle up the cattle trails of the Old West. However women did considerable ranch work, and in some cases (especially when the men went to war or on long cattle drives) ran them. There is little doubt that women, particularly the wives and daughters of men who owned small ranches and could not afford to hire large numbers of outside laborers, worked side by side with men and thus needed to ride horses and be able to perform related tasks. The largely undocumented contributions of women to the west were acknowledged in law; the western states led the United States in granting women the right to vote, beginning with Wyoming in 1869. Early photographers such as Evelyn Cameron documented the life of working ranch women and cowgirls during the late 19th and early 20th century.
While impractical for everyday work, the sidesaddle was a tool that gave women the ability to ride horses in "respectable" public settings instead of being left on foot or confined to horse-drawn vehicles. Following the Civil War, Charles Goodnight modified the traditional English sidesaddle, creating a western-styled design. The traditional charras of Mexico preserve a similar tradition and ride sidesaddles today in charreada exhibitions on both sides of the border.
It wasn't until the advent of Wild West Shows that "cowgirls" came into their own. These adult women were skilled performers, demonstrating riding, expert marksmanship, and trick roping that entertained audiences around the world. Women such as Annie Oakley became household names. By 1900, skirts split for riding astride became popular, and allowed women to compete with the men without scandalizing Victorian Era audiences by wearing men's clothing or, worse yet, bloomers. In the movies that followed from the early 20th century on, cowgirls expanded their roles in the popular culture and movie designers developed attractive clothing suitable for riding Western saddles.
Independently of the entertainment industry, the growth of rodeo brought about the rodeo cowgirl. In the early Wild West shows and rodeos, women competed in all events, sometimes against other women, sometimes with the men. Cowgirls such as Fannie Sperry Steele rode the same "rough stock" and took the same risks as the men (and all while wearing a heavy split skirt that was more encumbering than men's trousers) and competed at major rodeos such as the Calgary Stampede and Cheyenne Frontier Days.

Rodeo competition for women changed in the 1920s due to several factors. After 1925, when Eastern promoters started staging indoor rodeos in places like Madison Square Garden, women were generally excluded from the men's events and many of the women's events were dropped. Also, the public had difficulties with seeing women seriously injured or killed, and in particular, the death of Bonnie McCarroll at the 1929 Pendleton Round-Up led to the elimination of women's bronc riding from rodeo competition.In today's rodeos, men and women compete equally together only in the event of team roping, though technically women now could enter other open events. There also are all-women rodeos where women compete in bronc riding, bull riding and all other traditional rodeo events. However, in open rodeos, cowgirls primarily compete in the timed riding events such as barrel racing, and most professional rodeos do not offer as many women's events as men's events.
Boys and girls are more apt to compete against one another in all events in high-school rodeos as well as O-Mok-See competition, where even boys can be seen in traditionally "women's" events such as barrel racing. Outside of the rodeo world, women compete equally with men in nearly all other equestrian events, including the Olympics, and western riding events such as cutting, reining, and endurance riding.
Today's working cowgirls generally use clothing, tools and equipment indistinguishable from that of men, other than in color and design, usually preferring a flashier look in competition. Sidesaddles are only seen in exhibitions and a limited number of specialty horse show classes. A modern working cowgirl wears jeans, close-fitting shirts, boots, hat, and when needed, chaps and gloves. If working on the ranch, they perform the same chores as cowboys and dress to suit the situation.


Regional traditions within the United States
Geography, climate and cultural traditions caused differences to develop in cattle-handling methods and equipment from one part of the United States to another. The period between 1840 and 1870 marked a mingling of cultures when English and French-descended people began to settle west of the Mississippi River and encountered the Spanish-descended people who had settled in the parts of Mexico that later became Texas and California.  In the modern world, remnants of two major and distinct cowboy traditions remain, known today as the "Texas" tradition and the "Spanish", "Vaquero", or "California" tradition. Less well-known but equally distinct traditions also developed in Hawaii and Florida. Today, the various regional cowboy traditions have merged to some extent, though a few regional differences in equipment and riding style still remain, and some individuals choose to deliberately preserve the more time-consuming but highly skilled techniques of the pure vaquero or "buckaroo" tradition. The popular "horse whisperer" style of natural horsemanship was originally developed by practitioners who were predominantly from California and the Northwestern states, clearly combining the attitudes and philosophy of the California vaquero with the equipment and outward look of the Texas cowboy.


California tradition

The vaquero, the Spanish or Mexican cowboy who worked with young, untrained horses, arrived in the 18th century and flourished in California and bordering territories during the Spanish Colonial period. Settlers from the United States did not enter California until after the MexicanAmerican War, and most early settlers were miners rather than livestock ranchers, leaving livestock-raising largely to the Spanish and Mexican people who chose to remain in California. The California vaquero or buckaroo, unlike the Texas cowboy, was considered a highly skilled worker, who usually stayed on the same ranch where he was born or had grown up and raised his own family there. In addition, the geography and climate of much of California was dramatically different from that of Texas, allowing more intensive grazing with less open range, plus cattle in California were marketed primarily at a regional level, without the need (nor, until much later, even the logistical possibility) to be driven hundreds of miles to railroad lines. Thus, a horse- and livestock-handling culture remained in California and the Pacific Northwest that retained a stronger direct Spanish influence than that of Texas. The modern distinction between vaquero and buckaroo within American English may also reflect the parallel differences between the California and Texas traditions of western horsemanship.


Buckaroos
Some cowboys of the California tradition were dubbed buckaroos by English-speaking settlers. The words "buckaroo" and vaquero are still used on occasion in the Great Basin, parts of California and, less often, in the Pacific Northwest. Elsewhere, the term "cowboy" is more common.The word buckaroo is generally believed to be an anglicized version of vaquero and shows phonological characteristics compatible with that origin. Buckaroo first appeared in American English in 1827.  The word may also have developed with influences from the English word "buck" or bucking, the behavior of young, untrained horses. In 1960, one etymologist suggested that buckaroo derives, through Gullah: buckra, from the Ibibio and Efik: mbakara, meaning "white man, master, boss". Although that derivation was later rejected, another possibility advanced was that "buckaroo" was a pun on vaquero, blending both Spanish and African sources.


Texas tradition
In the 18th century, the residents of Spanish Texas began to herd cattle on horseback to sell in Louisiana, both legally and illegally. Their horses were of jennet type which became the Spanish Mustang. By the early 19th century, the Spanish Crown, and later, independent Mexico, offered empresario grants in what would later be Texas to non-citizens, such as settlers from the United States. In 1821, Stephen F. Austin led a group which became the first English-speaking Mexican citizens. Following Texas independence in 1836, even more Americans immigrated into the empresario ranching areas of Texas. Here the settlers were strongly influenced by the Mexican vaquero culture, borrowing vocabulary and attire from their counterparts, but also retaining some of the livestock-handling traditions and culture of the Eastern United States and Great Britain. The Texas cowboy was typically a bachelor who hired on with different outfits from season to season.Following the American Civil War, vaquero culture combined with the cattle herding and drover traditions of the southeastern United States that evolved as settlers moved west. Additional influences developed out of Texas as cattle trails were created to meet up with the railroad lines of Kansas and Nebraska, in addition to expanding ranching opportunities in the Great Plains and Rocky Mountain Front, east of the Continental Divide.  The new settlers required more horses, to be trained faster, and brought a bigger and heavier horse with them.  This led to modifications in the bridling and bitting traditions used by the vaquero.  Thus, the Texas cowboy tradition arose from a combination of cultural influences, in addition to the need for adaptation to the geography and climate of west Texas and the need to conduct long cattle drives to get animals to market.
Historian Terry Jordan proposed in 1982 that some Texan traditions that developedparticularly after the Civil Warmay trace to colonial South Carolina, as most settlers to Texas were from the southeastern United States.  However, these theories have been called into question by some reviewers.  In a subsequent work, Jordan also noted that the influence of post-War Texas upon the whole of the frontier Western cowboy tradition was likely much less than previously thought.


Florida cowhunter or "cracker cowboy"

The Florida "cowhunter" or "cracker cowboy" of the 19th and early 20th centuries was distinct from the Texas and California traditions. Florida cowboys did not use lassos to herd or capture cattle. Their primary tools were bullwhips and dogs. Since the Florida cowhunter did not need a saddle horn for anchoring a lariat, many did not use Western saddles, instead using a McClellan saddle. While some individuals wore boots that reached above the knees for protection from snakes, others wore brogans. They usually wore inexpensive wool or straw hats, and used ponchos for protection from rain.Cattle and horses were introduced into Spanish Florida in the 16th century, and flourished throughout the 17th century. The cattle introduced by the Spanish persist today in two rare breeds: Florida Cracker cattle and Pineywoods cattle. The Florida Cracker Horse, which is still used by some Florida cowboys, is descended from horses introduced by the Spanish. From shortly after 1565 until the end of the 17th century, cattle ranches owned by Spanish officials and missions operated in northern Florida to supply the Spanish garrison in St. Augustine and markets in Cuba. Raids into Spanish Florida by the Province of Carolina and its Native American allies, which wiped out the native population of Florida, led to the collapse of the Spanish mission and ranching systems.In the 18th century, Creek, Seminole, and other Indian people moved into the depopulated areas of Florida and started herding the cattle left from the Spanish ranches. In the 19th century, most tribes in the area were dispossessed of their land and cattle and pushed south or west by white settlers and the United States government. By the middle of the 19th century white ranchers were running large herds of cattle on the extensive open range of central and southern Florida. The hides and meat from Florida cattle became such a critical supply item for the Confederacy during the American Civil War that a unit of Cow Cavalry was organized to round up and protect the herds from Union raiders. After the Civil War, and into the 20th Century, Florida cattle were periodically driven to ports on the Gulf of Mexico, such as Punta Rassa near Fort Myers, Florida, and shipped to market in Cuba.The Florida cowhunter or cracker cowboy tradition gradually assimilated to western cowboy tradition during the 20th century (although the vaquero tradition has had little influence in Florida). Texas tick fever and the screw-worm were introduced to Florida in the early 20th century by cattle entering from other states. These pests forced Florida cattlemen to separate individual animals from their herds at frequent intervals for treatment, which eventually led to the widespread use of lassos. Florida cowboys continue to use dogs and bullwhips for controlling cattle.


Hawaiian Paniolo

The Hawaiian cowboy, the paniolo, is also a direct descendant of the vaquero of California and Mexico. Experts in Hawaiian etymology believe "Paniolo" is a Hawaiianized pronunciation of espaol. (The Hawaiian language has no /s/ sound, and all syllables and words must end in a vowel.) Paniolo, like cowboys on the mainland of North America, learned their skills from Mexican vaqueros. Other theories of word origin suggest Paniolo was derived from pauelo (Spanish for handkerchief) or possibly from a Hawai'ian language word meaning "hold firmly and sway gracefully."Captain George Vancouver brought cattle and sheep in 1793 as a gift to Kamehameha I, monarch of the Hawaiian Kingdom. For 10 years, Kamehameha forbade killing of cattle, and imposed the death penalty on anyone who violated his edict.  As a result, numbers  multiplied astonishingly, and were wreaking havoc throughout the countryside. By the reign of Kamehameha III the number of wild cattle were becoming a problem, so in 1832 he sent an emissary to California, then still a part of Mexico. He was impressed with the skill of the vaqueros, and invited three to Hawai'i  to teach the Hawaiian people how to work cattle.The first horses arrived in Hawai'i in 1803. By 1837 John Parker, a sailor from New England who settled in the islands, received permission from Kamehameha III to lease royal land near Mauna Kea, where he built a ranch.The Hawaiian style of ranching originally included capturing wild cattle by driving them into pits dug in the forest floor. Once tamed somewhat by hunger and thirst, they were hauled out up a steep ramp, and tied by their horns to the horns of a tame, older steer (or ox) that knew where the paddock with food and water was located. The industry grew slowly under the reign of Kamehameha's son Liholiho (Kamehameha II).
Even today, traditional paniolo dress, as well as certain styles of Hawaiian formal attire, reflect the Spanish heritage of the vaquero. The traditional Hawaiian saddle, the noho lio, and many other tools of the cowboy's trade have a distinctly Mexican/Spanish look and many Hawaiian ranching families still carry the names of the vaqueros who married Hawaiian women and made Hawai'i their home.


Other
Montauk, New York, on Long Island makes a somewhat debatable claim of having the oldest cattle operation in what today is the United States, having run cattle in the area since European settlers purchased land from the Indian people of the area in 1643. Although there were substantial numbers of cattle on Long Island, as well as the need to herd them to and from common grazing lands on a seasonal basis, no consistent "cowboy" tradition developed amongst the cattle handlers of Long Island, who actually lived with their families in houses built on the pasture grounds. The only actual "cattle drives" held on Long Island consisted of one drive in 1776, when the Island's cattle were moved in a failed attempt to prevent them from being captured by the British during the American Revolution, and three or four drives in the late 1930s, when area cattle were herded down Montauk Highway to pasture ground near Deep Hollow Ranch.On the Eastern Shore of Virginia, the "Salt Water Cowboys" are known for rounding up the feral Chincoteague Ponies from Assateague Island and driving them across Assateague Channel into pens on Chincoteague Island during the annual Pony Penning.


Canada

Ranching in Canada has traditionally been dominated by one province, Alberta. The most successful early settlers of the province were the ranchers, who found Alberta's foothills to be ideal for raising cattle. Most of Alberta's ranchers were English settlers, but cowboys such as John Warewho brought the first cattle into the province in 1876were American. American style open range dryland ranching began to dominate southern Alberta (and, to a lesser extent, southwestern Saskatchewan) by the 1880s. The nearby city of Calgary became the centre of the Canadian cattle industry, earning it the nickname "Cowtown". The cattle industry is still extremely important to Alberta, and cattle outnumber people in the province. While cattle ranches defined by barbed-wire fences replaced the open range just as they did in the US, the cowboy influence lives on. Canada's first rodeo, the Raymond Stampede, was established in 1902. In 1912, the Calgary Stampede began, and today it is the world's richest cash rodeo. Each year, Calgary's northern rival Edmonton, Alberta stages the Canadian Finals Rodeo, and dozens of regional rodeos are held through the province.


Outside North America

In addition to the original Mexican vaquero, the Mexican charro, the cowboy, and the Hawaiian paniolo, the Spanish also exported their horsemanship and knowledge of cattle ranching to the gaucho of Argentina, Uruguay, Paraguay and (with the spelling gacho) southern Brazil, the chaln and Morochuco in Peru, the llanero of Venezuela, and the huaso of Chile.
In Australia, where ranches are known as stations, cowboys are known as stockmen and ringers, (jackaroos and jillaroos who also do stockwork are trainee overseers and property managers). The Australian droving tradition was influenced by Americans in the 19th century, and as well as practices imported directly from Spain. The adaptation of both of these traditions to local needs created a unique Australian tradition, which also was strongly influenced by Australian indigenous people, whose knowledge played a key role in the success of cattle ranching in Australia's climate.
The idea of horse riders who guard herds of cattle, sheep or horses is common wherever wide, open land for grazing exists. In the French Camargue, riders called "gardians" herd cattle and horses. In Hungary, csiks guard horses and gulys tend to cattle. The herders in the region of Maremma, in Tuscany (Italy) are called butteri (singular: buttero). The Asturian pastoral population is referred to as Vaqueiros de alzada.


Modern working cowboys

On the ranch, the cowboy is responsible for feeding the livestock, branding and earmarking cattle (horses also are branded on many ranches), plus tending to animal injuries and other needs. The working cowboy usually is in charge of a small group or "string" of horses and is required to routinely patrol the rangeland in all weather conditions checking for damaged fences, evidence of predation, water problems, and any other issue of concern.
They also move the livestock to different pasture locations, or herd them into corrals and onto trucks for transport. In addition, cowboys may do many other jobs, depending on the size of the "outfit" or ranch, the terrain, and the number of livestock. On a smaller ranch with fewer cowboysoften just family members, cowboys are generalists who perform many all-around tasks; they repair fences, maintain ranch equipment, and perform other odd jobs. On a very large ranch (a "big outfit"), with many employees, cowboys are able to specialize on tasks solely related to cattle and horses. Cowboys who train horses often specialize in this task only, and some may "Break" or train young horses for more than one ranch.
The United States Bureau of Labor Statistics collects no figures for cowboys, so the exact number of working cowboys is unknown. Cowboys are included in the 2003 category, Support activities for animal production, which totals 9,730 workers averaging $19,340 per annum. In addition to cowboys working on ranches, in stockyards, and as staff or competitors at rodeos, the category includes farmhands working with other types of livestock (sheep, goats, hogs, chickens, etc.). Of those 9,730 workers, 3,290 are listed in the subcategory of Spectator sports which includes rodeos, circuses, and theaters needing livestock handlers.


Attire
Most cowboy attire, sometimes termed Western wear, grew out of practical need and the environment in which the cowboy worked. Most items were adapted from the Mexican vaqueros, though sources from other cultures, including Native Americans and Mountain Men contributed.
Bandanna; a large cotton neckerchief that had myriad uses: from mopping up sweat to masking the face from dust storms. In modern times, is now more likely to be a silk neckscarf for decoration and warmth.
Chaps (usually pronounced "shaps") or chinks protect the rider's legs while on horseback, especially riding through heavy brush or during rough work with livestock.
Cowboy boots; a boot with a high top to protect the lower legs, pointed toes to help guide the foot into the stirrup, and high heels to keep the foot from slipping through the stirrup while working in the saddle; with or without detachable spurs.
Cowboy hat; High crowned hat with a wide brim to protect from sun, overhanging brush, and the elements. There are many styles, initially influenced by John B. Stetson's Boss of the plains, which was designed in response to the climatic conditions of the West.
Gloves, usually of deerskin or other leather that is soft and flexible for working purposes, yet provides protection when handling barbed wire, assorted tools or clearing native brush and vegetation.
Jeans or other sturdy, close-fitting trousers made of canvas or denim, designed to protect the legs and prevent the trouser legs from snagging on brush, equipment or other hazards. Properly made cowboy jeans also have a smooth inside seam to prevent blistering the inner thigh and knee while on horseback.Many of these items show marked regional variations. Parameters such as hat brim width, or chap length and material were adjusted to accommodate the various environmental conditions encountered by working cowboys.


Tools

Firearms: Modern cowboys often have access to a rifle, used to protect the livestock from predation by wild animals, more often carried inside a pickup truck than on horseback, though rifle scabbards are manufactured, and allow a rifle to be carried on a saddle. A pistol is more often carried when on horseback. The modern ranch hand often uses a .22 caliber "varmit" rifle for modern ranch hazards, such as rattlesnakes, coyotes, and rabid skunks. In areas near wilderness, a ranch cowboy may carry a higher-caliber rifle to fend off larger predators such as mountain lions. In contrast, the cowboy of the 1880s usually carried a heavy caliber revolver such as the single action .44-40 or .45 Colt Peacemaker (the civilian version of the 1872 Single Action Army). The working cowboy of the 1880s rarely carried a long arm, as they could get in the way when working cattle, plus they added extra weight. However, many cowboys owned rifles, and often used them for market hunting in the off season. Though many models were used, Cowboys who were part-time market hunters preferred rifles that could take the widely available .4570 "Government" ammunition, such as certain Sharps, Remington, Springfield models, as well as the Winchester 1876. However, by far the single most popular long arms were the lever-action repeating Winchesters, particularly lighter models such as the Model 1873 chambered for the same .44/40 ammunition as the Colt, allowing the cowboy to carry only one kind of ammunition.
Knife; cowboys have traditionally favored some form of pocket knife, specifically the folding cattle knife or stock knife. The knife has multiple blades, usually including a leather punch and a "sheepsfoot" blade.
Lariat; from the Spanish "la riata," meaning "the rope," sometimes called a lasso, especially in the East, or simply, a "rope". This is a tightly twisted stiff rope, originally of rawhide or leather, now often of nylon, made with a small loop at one end called a "hondo." When the rope is run through the hondo, it creates a loop that slides easily, tightens quickly and can be thrown to catch animals.
Spurs; metal devices attached to the heel of the boot, featuring a small metal shank, usually with a small serrated wheel attached, used to allow the rider to provide a stronger (or sometimes, more precise) leg cue to the horse.
Other weapons; while the modern American cowboy came to existence after the invention of gunpowder, cattle herders of earlier times were sometimes equipped with heavy polearms, bows or lances.


Horses

The traditional means of transport for the cowboy, even in the modern era, is by horseback. Horses can travel over terrain that vehicles cannot access. Horses, along with mules and burros, also serve as pack animals. The most important horse on the ranch is the everyday working ranch horse that can perform a wide variety of tasks; horses trained to specialize exclusively in one set of skills such as roping or cutting are very rarely used on ranches. Because the rider often needs to keep one hand free while working cattle, the horse must neck rein and have good cow senseit must instinctively know how to anticipate and react to cattle.
A good stock horse is on the small side, generally under 15.2 hands (62 inches) tall at the withers and often under 1000 pounds, with a short back, sturdy legs and strong muscling, particularly in the hindquarters. While a steer roping horse may need to be larger and weigh more in order to hold a heavy adult cow, bull or steer on a rope, a smaller, quick horse is needed for herding activities such as cutting or calf roping. The horse has to be intelligent, calm under pressure and have a certain degree of 'cow sense"  the ability to anticipate the movement and behavior of cattle.
Many breeds of horse make good stock horses, but the most common today in North America is the American Quarter Horse, which is a horse breed developed primarily in Texas from a combination of Thoroughbred bloodstock crossed on horses of Mustang and other Iberian horse ancestry, with influences from the Arabian horse and horses developed on the east coast, such as the Morgan horse and now-extinct breeds such as the Chickasaw and Virginia Quarter-Miler.


Horse equipment or tack

Equipment used to ride a horse is referred to as tack and includes:

Bridle; a Western bridle usually has a curb bit and long split reins to control the horse in many different situations. Generally the bridle is open-faced, without a noseband, unless the horse is ridden with a tiedown. Young ranch horses learning basic tasks usually are ridden in a jointed, loose-ring snaffle bit, often with a running martingale. In some areas, especially where the "California" style of the vaquero or buckaroo tradition is still strong, young horses are often seen in a bosal style hackamore.
Martingales of various types are seen on horses that are in training or have behavior problems.
Saddle bags (leather or nylon) can be mounted to the saddle, behind the cantle, to carry various sundry items and extra supplies. Additional bags may be attached to the front or the saddle.
Saddle blanket; a blanket or pad is required under the Western saddle to provide comfort and protection for the horse.
Western saddle; a saddle specially designed to allow horse and rider to work for many hours and to provide security to the rider in rough terrain or when moving quickly in response to the behavior of the livestock being herded. A western saddle has a deep seat with high pommel and cantle that provides a secure seat. Deep, wide stirrups provide comfort and security for the foot. A strong, wide saddle tree of wood, covered in rawhide (or made of a modern synthetic material) distributes the weight of the rider across a greater area of the horse's back, reducing the pounds carried per square inch and allowing the horse to be ridden longer without harm. A horn sits low in front of the rider, to which a lariat can be snubbed, and assorted dee rings and leather "saddle strings" allow additional equipment to be tied to the saddle.


Vehicles
The most common motorized vehicle driven in modern ranch work is the pickup truck. Sturdy and roomy, with a high ground clearance, and often four-wheel drive capability, it has an open box, called a "bed," and can haul supplies from town or over rough trails on the ranch. It is used to pull stock trailers transporting cattle and livestock from one area to another and to market. With a horse trailer attached, it carries horses to distant areas where they may be needed. Motorcycles are sometimes used instead of horses for some tasks, but the most common smaller vehicle is the four-wheeler. It will carry a single cowboy quickly around the ranch for small chores. In areas with heavy snowfall, snowmobiles are also common. However, in spite of modern mechanization, there remain jobs, particularly those involving working cattle in rough terrain or in close quarters that are best performed by cowboys on horseback.


Rodeo cowboys

The word rodeo is from the Spanish rodear (to turn), which means roundup. In the beginning there was no difference between the working cowboy and the rodeo cowboy, and in fact, the term working cowboy did not come into use until the 1950s. Prior to that it was assumed that all cowboys were working cowboys. Early cowboys both worked on ranches and displayed their skills at the roundups.The advent of professional rodeos allowed cowboys, like many athletes, to earn a living by performing their skills before an audience. Rodeos also provided employment for many working cowboys who were needed to handle livestock. Many rodeo cowboys are also working cowboys and most have working cowboy experience.
The dress of the rodeo cowboy is not very different from that of the working cowboy on his way to town. Snaps, used in lieu of buttons on the cowboy's shirt, allowed the cowboy to escape from a shirt snagged by the horns of steer or bull. Styles were often adapted from the early movie industry for the rodeo. Some rodeo competitors, particularly women, add sequins, colors, silver and long fringes to their clothing in both a nod to tradition and showmanship. Modern riders in "rough stock" events such as saddle bronc or bull riding may add safety equipment such as kevlar vests or a neck brace, but use of safety helmets in lieu of the cowboy hat is yet to be accepted, in spite of constant risk of injury.


In popular culture

As the frontier ended, the cowboy life came to be highly romanticized. Exhibitions such as those of Buffalo Bill Cody's Wild West Show helped to popularize the image of the cowboy as an idealized representative of the tradition of chivalry.In today's society, there is little understanding of the daily realities of actual agricultural life. Cowboys are more often associated with (mostly fictitious) Indian-fighting than with their actual life of ranch work and cattle-tending. The cowboy is also portrayed as a masculine ideal via images ranging from the Marlboro Man to the Village People. Actors such as John Wayne are thought of as exemplifying a cowboy ideal, even though western movies seldom bear much resemblance to real cowboy life. Arguably, the modern rodeo competitor is much closer to being an actual cowboy, as many were actually raised on ranches and around livestock, and the rest have needed to learn livestock-handling skills on the job.
However, in the United States and the Canadian West, as well as Australia, guest ranches offer people the opportunity to ride horses and get a taste of the western lifealbeit in far greater comfort. Some ranches also offer vacationers the opportunity to actually perform cowboy tasks by participating in cattle drives or accompanying wagon trains. This type of vacation was popularized by the 1991 movie City Slickers, starring Billy Crystal.


Symbolism
In 2005, the United States Senate declared the fourth Saturday of July as "National Day of the American Cowboy" via a Senate resolution and has subsequently renewed this resolution each year, with the United States House of Representatives periodically issuing statements of support.
The long history of the West in popular culture tends to define those clothed in Western clothing as cowboys or cowgirls whether they have ever been on a horse or not. This is especially true when applied to entertainers and those in the public arena who wear western wear as part of their persona. However, the reality is that many people, particularly in the West, including lawyers, bankers, and other white collar professionals wear elements of Western clothing, particularly cowboy boots or hats, as a matter of form even though they have other jobs. Conversely, some people raised on ranches do not necessarily define themselves cowboys or cowgirls unless they feel their primary job is to work with livestock or if they compete in rodeos.
Actual cowboys have derisive expressions for individuals who adopt cowboy mannerisms as a fashion pose without any actual understanding of the culture. For example, a "drugstore cowboy" means someone who wears the clothing but does not actually sit upon anything but the stool of the drugstore soda fountainor, in modern times, a bar stool. Similarly, the phrase "all hat and no cattle" is used to describe someone (usually male) who boasts about himself, far in excess of any actual accomplishments. The word "dude" (or the now-archaic term "greenhorn") indicates an individual unfamiliar with cowboy culture, especially one who is trying to pretend otherwise.
Outside of the United States, the cowboy has become an archetypal image of Americans abroad. In the late 1950s, a Congolese youth subculture calling themselves the Bills based their style and outlook on Hollywood's depiction of cowboys in movies. Something similar occurred with the term "Apache", which in early 20th century Parisian society was a slang term for an outlaw.


Negative associations
The word "cowboy" is also used in a negative sense. Originally this derived from the behavior of some cowboys in the boomtowns of Kansas, at the end of the trail for long cattle drives, where cowboys developed a reputation for violence and wild behavior due to the inevitable impact of large numbers of cowboys, mostly young single men, receiving their pay in large lump sums upon arriving in communities with many drinking and gambling establishments."Cowboy" as an adjective for "reckless" developed in the 1920s. "Cowboy" is sometimes used today in a derogatory sense to describe someone who is reckless or ignores potential risks, irresponsible or who heedlessly handles a sensitive or dangerous task. TIME Magazine referred to President George W. Bush's foreign policy as "Cowboy diplomacy", and Bush has been described in the press, particularly in Europe, as a "cowboy", not realizing that this was not a compliment.
In English-speaking regions outside North America, such as the British Isles and Australasia, "cowboy" can refer to a tradesman whose work is of shoddy and questionable value, e.g., "a cowboy plumber". The term also lent itself to the British 1980s TV sitcom, Cowboys. Similar usage is seen in the United States to describe someone in the skilled trades who operates without proper training or licenses. In the eastern United States, "cowboy" as a noun is sometimes used to describe a fast or careless driver on the highway.


A computer keyboard is a typewriter-style device which uses an arrangement of buttons or keys to act as mechanical levers or electronic switches. Replacing early punched cards and paper tape technology, interaction via teleprinter-style keyboards have been the main input method for computers since the 1970s, supplemented by the computer mouse since the 1980s.
Keyboard keys (buttons) typically have a set of characters engraved or printed on them, and each press of a key typically corresponds to a single written symbol. However, producing some symbols may require pressing and holding several keys simultaneously or in sequence. While most keyboard keys produce letters, numbers or symbols (characters), other keys or simultaneous key presses can prompt the computer to execute system commands, such as such as the Control-Alt-Delete combination used with Microsoft Windows. In a modern computer, the interpretation of key presses is generally left to the software: the information sent to the computer, the scan code, tells it only which key (or keys) on which row and column, was pressed or released.In normal usage, the keyboard is used as a text entry interface for typing text, numbers, and symbols into application software such as a word processor, web browser or social media app.


History
While typewriters are the definitive ancestor of all key-based text entry devices, the computer keyboard as a device for electromechanical data entry and communication derives largely from the utility of two devices: teleprinters (or teletypes) and keypunches. It was through such devices that modern computer keyboards inherited their layouts.
As early as the 1870s, teleprinter-like devices were used to simultaneously type and transmit stock market text data from the keyboard across telegraph lines to stock ticker machines to be immediately copied and displayed onto ticker tape. The teleprinter, in its more contemporary form, was developed from 1907 to 1910 by American mechanical engineer Charles Krum and his son Howard, with early contributions by electrical engineer Frank Pearne. Earlier models were developed separately by individuals such as Royal Earl House and Frederick G. Creed.
Earlier, Herman Hollerith developed the first keypunch devices, which soon evolved to include keys for text and number entry akin to normal typewriters by the 1930s.The keyboard on the teleprinter played a strong role in point-to-point and point-to-multipoint communication for most of the 20th century, while the keyboard on the keypunch device played a strong role in data entry and storage for just as long. The development of the earliest computers incorporated electric typewriter keyboards: the development of the ENIAC computer incorporated a keypunch device as both the input and paper-based output device, while the BINAC computer also made use of an electromechanically controlled typewriter for both data entry onto magnetic tape (instead of paper) and data output.The keyboard remained the primary, most integrated computer peripheral well into the era of personal computing until the introduction of the mouse as a consumer device in 1984. By this time, text-only user interfaces with sparse graphics gave way to comparatively graphics-rich icons on screen. However, keyboards remain central to human-computer interaction to the present, even as mobile personal computing devices such as smartphones and tablets adapt the keyboard as an optional virtual, touchscreen-based means of data entry.


Types and standards

Different types of keyboards are available and each is designed with a focus on specific features that suit particular needs. 
Today, most full-size keyboards use one of three different mechanical layouts, usually referred to as simply ISO (ISO/IEC 9995-2), ANSI (ANSI-INCITS 154-1988), and JIS (JIS X 6002-1980), referring roughly to the organizations issuing the relevant worldwide, United States, and Japanese standards, respectively. (In fact, the mechanical layouts referred such as "ISO" and "ANSI" comply to the primary recommendations in the named standards, while each of these standards in fact also allows the other way.) ANSI standard alphanumeric keyboards have keys that are on three-quarter inch centers (0.75 inches (19 mm)), and have a key travel of at least 0.15 inches (3.8 mm).Modern keyboard models contain a set number of total keys according to their given standard, described as 101, 104, 105, etc. and sold as "Full-size" keyboards. Modern keyboards matching US conventions typically have 104 keys while the 105 key layout is the norm in the rest of the world. This number is not always followed, and individual keys or whole sections are commonly skipped for the sake of compactness or user preference. The most common choice is to not include the numpad, which can usually be fully replaced by the alphanumeric section. Laptops and wireless peripherals often lack duplicate keys and ones seldom used. Function- and arrow keys are nearly always present.
Another factor determining the size of a keyboard is the size and spacing of the keys. The reduction is limited by the practical consideration that the keys must be large enough to be easily pressed by fingers. Alternatively, a tool is used for pressing small keys.


Desktop or full-size
Desktop computer keyboards include alphabetic characters and numerals, typographical symbols and punctuation marks, one or more currency symbols and other special characters, diacritics and a variety of function keys. The repertoire of glyphs engraved on the keys of a keyboard accords with national conventions and language needs. Computer keyboards are similar to electric-typewriter keyboards but contain additional keys, such as the command key or Windows keys.


Laptop-size

Keyboards on laptops and notebook computers usually have a shorter travel distance for the keystroke, shorter over travel distance, and a reduced set of keys. They may not have a numeric keypad, and the function keys may be placed in locations that differ from their placement on a standard, full-sized keyboard. The switch mechanism for a laptop keyboard is more likely to be a scissor switch than a rubber dome; this is opposite the trend for full-size keyboards.


Flexible keyboards
Flexible keyboards are a junction between normal type and laptop type keyboards: normal from the full arrangement of keys, and laptop from the short key distance. Additionally, the flexibility allows the user to fold/roll the keyboard for better storage and transfer. However, for typing the keyboard must be resting on a hard surface. The vast majority of flexible keyboards in the market are made from silicone; this material makes them water- and dust-proof. This is useful in hospitals, where keyboards are subjected to frequent washing,
and other dirty or must-be-clean environments.


Handheld

Handheld ergonomic keyboards are designed to be held like a game controller, and can be used as such, instead of laid out flat on top of a table surface. 
Typically handheld keyboards hold all the alphanumeric keys and symbols that a standard keyboard would have, yet only be accessed by pressing two sets of keys at once; one acting as a function key similar to a 'Shift' key that would allow for capital letters on a standard keyboard. Handheld keyboards allow the user the ability to move around a room or to lean back on a chair while also being able to type in front or away from the computer. Some variations of handheld ergonomic keyboards also include a trackball mouse that allow mouse movement and typing included in one handheld device.


Thumb-sized
Smaller external keyboards have been introduced for devices without a built-in keyboard, such as PDAs, and smartphones. Small keyboards are also useful where there is a limited workspace.A thumb keyboard (thumb board) is used in some personal digital assistants such as the Palm Treo and BlackBerry and some Ultra-Mobile PCs such as the OQO.
Numeric keyboards contain only numbers, mathematical symbols for addition, subtraction, multiplication, and division, a decimal point, and several function keys. They are often used to facilitate data entry with smaller keyboards that do not have a numeric keypad, commonly those of laptop computers. These keys are collectively known as a numeric pad, numeric keys, or a numeric keypad, and it can consist of the following types of keys: Arithmetic operators, numbers, arrow keys, Navigation keys, Num Lock and Enter key.


Multifunctional

Multifunctional keyboards provide additional function beyond the standard keyboard. Many are programmable, configurable computer keyboards and some control multiple PCs, workstations and other information sources, usually in multi-screen work environments. Users have additional key functions as well as the standard functions and can typically use a single keyboard and mouse to access multiple sources. 

Multifunctional keyboards may feature customised keypads, fully programmable function or soft keys for macros/pre-sets, biometric or smart card readers, trackballs, etc. New generation multifunctional keyboards feature a touchscreen display to stream video, control audio visual media and alarms, execute application inputs, configure individual desktop environments, etc. Multifunctional keyboards may also permit users to share access to PCs and other information sources. Multiple interfaces (serial, USB, audio, Ethernet, etc.) are used to integrate external devices. Some multifunctional keyboards are also used to directly and intuitively control video walls.
Common environments for multifunctional keyboards are complex, high-performance workplaces for financial traders and control room operators (emergency services, security, air traffic management; industry, utilities management, etc.).


Non-standard layout and special-use types


Chorded

While other keyboards generally associate one action with each key, chorded keyboards associate actions with combinations of key presses. Since there are many combinations available, chorded keyboards can effectively produce more actions on a board with fewer keys. Court reporters' stenotype machines use chorded keyboards to enable them to enter text much faster by typing a syllable with each stroke instead of one letter at a time. The fastest typists (as of 2007) use a stenograph, a kind of chorded keyboard used by most court reporters and closed-caption reporters. Some chorded keyboards are also made for use in situations where fewer keys are preferable, such as on devices that can be used with only one hand, and on small mobile devices that don't have room for larger keyboards. Chorded keyboards are less desirable in many cases because it usually takes practice and memorization of the combinations to become proficient.


Software
Software keyboards or on-screen keyboards often take the form of computer programs that display an image of a keyboard on the screen. Another input device such as a mouse or a touchscreen can be used to operate each virtual key to enter text. Software keyboards have become very popular in touchscreen enabled cell phones, due to the additional cost and space requirements of other types of hardware keyboards. Microsoft Windows, Mac OS X, and some varieties of Linux include on-screen keyboards that can be controlled with the mouse. In software keyboards, the mouse has to be maneuvered onto the on-screen letters given by the software. On the click of a letter, the software writes the respective letter on the respective spot.


Projection
Projection keyboards project an image of keys, usually with a laser, onto a flat surface. The device then uses a camera or infrared sensor to "watch" where the user's fingers move, and will count a key as being pressed when it "sees" the user's finger touch the projected image. Projection keyboards can simulate a full size keyboard from a very small projector. Because the "keys" are simply projected images, they cannot be felt when pressed. Users of projected keyboards often experience increased discomfort in their fingertips because of the lack of "give" when typing. A flat, non-reflective surface is also required for the keys to be projected. Most projection keyboards are made for use with PDAs and smartphones due to their small form factor.


Optical keyboard technology

Also known as photo-optical keyboard, light responsive keyboard, photo-electric keyboard and optical key actuation detection technology.
An optical keyboard technology utilizes LEDs and photo sensors to optically detect actuated keys. Most commonly the emitters and sensors are located in the perimeter, mounted on a small PCB. The light is directed from side to side of the keyboard interior and it can only be blocked by the actuated keys. Most optical keyboards require at least 2 beams (most commonly vertical beam and horizontal beam) to determine the actuated key. Some optical keyboards use a special key structure that blocks the light in a certain pattern, allowing only one beam per row of keys (most commonly horizontal beam).


Key types


Alphanumeric

Alphabetical, numeric, and punctuation keys are used in the same fashion as a typewriter keyboard to enter their respective symbol into a word processing program, text editor, data spreadsheet, or other program. Many of these keys will produce different symbols when modifier keys or shift keys are pressed. The alphabetic characters become uppercase when the shift key or Caps Lock key is depressed. The numeric characters become symbols or punctuation marks when the shift key is depressed. The alphabetical, numeric, and punctuation keys can also have other functions when they are pressed at the same time as some modifier keys.
The Space bar is a horizontal bar in the lowermost row, which is significantly wider than other keys. Like the alphanumeric characters, it is also descended from the mechanical typewriter. Its main purpose is to enter the space between words during typing. It is large enough so that a thumb from either hand can use it easily. Depending on the operating system, when the space bar is used with a modifier key such as the control key, it may have functions such as resizing or closing the current window, half-spacing, or backspacing. In computer games and other applications the key has myriad uses in addition to its normal purpose in typing, such as jumping and adding marks to check boxes. In certain programs for playback of digital video, the space bar is used for pausing and resuming the playback.


Modifier keys
Modifier keys are special keys that modify the normal action of another key, when the two are pressed in combination. For example, Alt+F4 in Microsoft Windows will close the program in an active window. In contrast, pressing just F4 will probably do nothing, unless assigned a specific function in a particular program. By themselves, modifier keys usually do nothing.
The most widely used modifier keys include the Control key, Shift key and the Alt key. The AltGr key is used to access additional symbols for keys that have three symbols printed on them. On the Macintosh and Apple keyboards, the modifier keys are the Option key and Command key, respectively. On Sun Microsystems and Lisp machine keyboards, the Meta key is used as a modifier and for Windows keyboards, there is a Windows key. Compact keyboard layouts often use a Fn key. "Dead keys" allow placement of a diacritic mark, such as an accent, on the following letter (e.g., the Compose key).
The Enter/Return key typically causes a command line, window form or dialog box to operate its default function, which is typically to finish an "entry" and begin the desired process. In word processing applications, pressing the enter key ends a paragraph and starts a new one.


Cursor keys
Navigation keys or cursor keys include a variety of keys which move the cursor to different positions on the screen. Arrow keys are programmed to move the cursor in a specified direction; page scroll keys, such as the Page Up and Page Down keys, scroll the page up and down. The Home key is used to return the cursor to the beginning of the line where the cursor is located; the End key puts the cursor at the end of the line. The Tab key advances the cursor to the next tab stop.
The Insert key is mainly used to switch between overtype mode, in which the cursor overwrites any text that is present on and after its current location, and insert mode, where the cursor inserts a character at its current position, forcing all characters past it one position further. The Delete key discards the character ahead of the cursor's position, moving all following characters one position "back" towards the freed place. On many notebook computer keyboards the key labeled Delete (sometimes Delete and Backspace are printed on the same key) serves the same purpose as a Backspace key. The Backspace key deletes the preceding character.
Lock keys lock part of a keyboard, depending on the settings selected. The lock keys are scattered around the keyboard. Most styles of keyboards have three LEDs indicating which locks are enabled, in the upper right corner above the numeric pad. The lock keys include Scroll lock, Num lock (which allows the use of the numeric keypad), and Caps lock.


System commands

The SysRq and Print screen commands often share the same key. SysRq was used in earlier computers as a "panic" button to recover from crashes (and it is still used in this sense to some extent by the Linux kernel; see Magic SysRq key). The Print screen command used to capture the entire screen and send it to the printer, but in the present it usually puts a screenshot in the clipboard.


Break key
The Break key/Pause key no longer has a well-defined purpose. Its origins go back to teleprinter users, who wanted a key that would temporarily interrupt the communications line. The Break key can be used by software in several different ways, such as to switch between multiple login sessions, to terminate a program, or to interrupt a modem connection.
In programming, especially old DOS-style BASIC, Pascal and C, Break is used (in conjunction with Ctrl) to stop program execution. In addition to this, Linux and variants, as well as many DOS programs, treat this combination the same as Ctrl+C. On modern keyboards, the break key is usually labeled Pause/Break. In most Windows environments, the key combination Windows key+Pause brings up the system properties.


Escape key
 
The escape key (often abbreviated Esc) "nearly all of the time" signals Stop - QUIT - let me "get out of a dialog" (or pop-up window): LET ME ESCAPE.
Another common application today of the Esc key is to trigger the Stop button in many web browsers.


ESC origins
ESC was part of the standard keyboard of the Teletype Model 33 (introduced in 1964 and used with many early minicomputers). The DEC VT50, introduced July 1974, also had an Esc key. The TECO text editor (ca 1963) and its descendant Emacs (ca 1985) use the Esc key extensively.
Historically it also served as a type of shift key, such that one or more following characters were interpreted differently, hence the term escape sequence, which refers to a series of characters, usually preceded by the escape character.On machines running Microsoft Windows, prior to the implementation of the Windows key on keyboards, the typical practice for invoking the "start" button was to hold down the control key and press escape. This process still works in Windows 95, 98, Me, NT 4, 2000, XP, Vista, 7, 8, and 10.


Enter key
The Enter key is located: One in the alphanumeric keys and the other one is in the numeric keys. When one worked something on their computer and wanted to do something with their work, pressing the enter key would do the command they ordered. Another function is to create a space for next paragraph. When one typed and finished typing a paragraph and they wanted to have a second paragraph, they could press enter and it would do spacing.


Shift key
Shift key: when one presses shift and a letter, it will capitalize the letter pressed with the shift key. Another use is to type more symbols than appear to be available, for instance the apostrophe key is accompanied with a quotation mark on the top. If one wants to type the quotation mark but pressed that key alone, the symbol that would appear would be the apostrophe. The quotation mark will only appear if both the required key and the Shift key are pressed.


Menu key
The Menu key or Application key is a key found on Windows-oriented computer keyboards. It is used to launch a context menu with the keyboard rather than with the usual right mouse button. The key's symbol is usually a small icon depicting a cursor hovering above a menu. On some Samsung keyboards the cursor in the icon is not present, showing the menu only.  This key was created at the same time as the Windows key. This key is normally used when the right mouse button is not present on the mouse. Some Windows public terminals do not have a Menu key on their keyboard to prevent users from right-clicking (however, in many Windows applications, a similar functionality can be invoked with the Shift+F10 keyboard shortcut).


Number pad
Many, but not all, computer keyboards have a numeric keypad to the right of the alphabetic keyboard, often separated from the other groups of keys such as the function keys and system command keys, which contains numbers, basic mathematical symbols (e.g., addition, subtraction, etc.), and a few function keys.  In addition to the row of number keys above the top alphabetic row, most desktop keyboards have a number pad or accounting pad, on the right hand side of the keyboard. While num lock is set, the numbers on these keys duplicate the number row; if not, they have alternative functions as engraved. In addition to numbers, this pad has command symbols concerned with calculations such as addition, subtraction, multiplication and division symbols. The enter key in this keys indicate the equal sign.


Miscellaneous

On Japanese/Korean keyboards, there may be Language input keys for changing the language to use. Some keyboards have power management keys (e.g., power key, sleep key and wake key); Internet keys to access a web browser or E-mail; and/or multimedia keys, such as volume controls; or keys that can be programmed by the user to launch a specified application or a command like minimizing all windows.


Multiple layouts
It is possible to install multiple keyboard layouts within an operating system and switch between them, either through features implemented within the OS, or through an external application. Microsoft Windows, Linux, and Mac provide support to add keyboard layouts and choose from them.


Illumination
Keyboards and keypads may be illuminated from inside, especially on equipment for mobile use. Both keyboards built into computers and external ones may support backlighting; external backlit keyboards may have a wired USB connection, or be connected wirelessly and powered by batteries. Illumination facilitates the use of the keyboard or keypad in dark environments.
For general productivity, only the keys may be uniformly backlit, without distracting light around the keys.

Many gaming keyboards are designed to have an aesthetic as well as functional appeal, with multiple colours, and colour-coded keys to make it easier for gamers to find command keys while playing in a dark room. Many keyboards not otherwise illuminated may have small LED indicator lights in a few important function keys, or elsewhere on the housing, if their function is activated (see photo).


Technology


Key switches
In the first electronic keyboards in the early 1970s, the key switches were individual switches inserted into holes in metal frames. These keyboards cost from 80 to 120 USD and were used in mainframe data terminals. The most popular switch types were reed switches (contacts enclosed in a vacuum in a glass capsule, affected by a magnet mounted on the switch plunger).In the mid-1970s, lower-cost direct-contact key switches were introduced, but their life in switch cycles was much shorter (rated ten million cycles) because they were open to the environment. This became more acceptable, however, for use in computer terminals at the time, which began to see increasingly shorter model lifespans as they advanced.In 1978, Key Tronic Corporation introduced keyboards with capacitive-based switches, one of the first keyboard technologies not to use self-contained switches. There was simply a sponge pad with a conductive-coated Mylar plastic sheet on the switch plunger, and two half-moon trace patterns on the printed circuit board below. As the key was depressed, the capacitance between the plunger pad and the patterns on the PCB below changed, which was detected by integrated circuits (IC). These keyboards were claimed to have the same reliability as the other "solid-state switch" keyboards such as inductive and Hall-effect, but competitive with direct-contact keyboards. Prices of $60 for keyboards were achieved, and Key Tronic rapidly became the largest independent keyboard manufacturer.
Meanwhile, IBM made their own keyboards, using their own patented technology: Keys on older IBM keyboards were made with a "buckling spring" mechanism, in which a coil spring under the key buckles under pressure from the user's finger, triggering a hammer that presses two plastic sheets (membranes) with conductive traces together, completing a circuit. This produces a clicking sound and gives physical feedback for the typist, indicating that the key has been depressed.The first electronic keyboards had a typewriter key travel distance of 0.187 inches (4.75 mm), keytops were a half-inch (12.7 mm) high, and keyboards were about two inches (5 cm) thick. Over time, less key travel was accepted in the market, finally landing on 0.110 inches (2.79 mm). Coincident with this, Key Tronic was the first company to introduce a keyboard that was only about one inch thick. And now keyboards measure only about a half-inch thick.

Keytops are an important element of keyboards. In the beginning, keyboard keytops had a "dish shape" on top, like typewriters before them. Keyboard key legends must be extremely durable over tens of millions of depressions, since they are subjected to extreme mechanical wear from fingers and fingernails, and subject to hand oils and creams, so engraving and filling key legends with paint, as was done previously for individual switches, was never acceptable. So, for the first electronic keyboards, the key legends were produced by two-shot (or double-shot, or two-color) molding, where either the key shell or the inside of the key with the key legend was molded first, and then the other color molded second. But, to save cost, other methods were explored, such as sublimation printing and laser engraving, both methods which could be used to print a whole keyboard at the same time.
Initially, sublimation printing, where a special ink is printed onto the keycap surface and the application of heat causes the ink molecules to penetrate and commingle with the plastic modules, had a problem because finger oils caused the molecules to disperse, but then a necessarily very hard clear coating was applied to prevent this. Coincident with sublimation printing, which was first used in high volume by IBM on their keyboards, was the introduction by IBM of single-curved-dish keycaps to facilitate quality printing of key legends by having a consistently curved surface instead of a dish. But one problem with sublimation or laser printing was that the processes took too long and only dark legends could be printed on light-colored keys. On another note, IBM was unique in using separate shells, or "keycaps", on keytop bases. This might have made their manufacturing of different keyboard layouts more flexible, but the reason for doing this was that the plastic material that needed to be used for sublimation printing was different from standard ABS keytop plastic material.
Three final mechanical technologies brought keyboards to where they are today, driving the cost well under $10:

"Monoblock" keyboard designs were developed where individual switch housings were eliminated and a one-piece "monoblock" housing used instead. This was possible because of molding techniques that could provide very tight tolerances for the switch-plunger holes and guides across the width of the keyboard so that the key plunger-to-housing clearances were not too tight or too loose, either of which could cause the keys to bind.
The use of contact-switch membrane sheets under the monoblock. This technology came from flat-panel switch membranes, where the switch contacts are printed inside of a top and bottom layer, with a spacer layer in between, so that when pressure is applied to the area above, a direct electrical contact is made.  The membrane layers can be printed by very-high volume, low-cost "reel-to-reel" printing machines, with each keyboard membrane cut and punched out afterwards.Plastic materials played a very important part in the development and progress of electronic keyboards. Until "monoblocks" came along, GE's "self-lubricating" Delrin was the only plastic material for keyboard switch plungers that could withstand the beating over tens of millions of cycles of lifetime use. Greasing or oiling switch plungers was undesirable because it would attract dirt over time which would eventually affect the feel and even bind the key switches (although keyboard manufacturers would sometimes sneak this into their keyboards, especially if they could not control the tolerances of the key plungers and housings well enough to have a smooth key depression feel or prevent binding). But Delrin was only available in black and white, and was not suitable for keytops (too soft), so keytops use ABS plastic.  However, as plastic molding advanced in maintaining tight tolerances, and as key travel length reduced from 0.187-inch to 0.110-inch (4.75 mm to 2.79 mm), single-part keytop/plungers could be made of ABS, with the keyboard monoblocks also made of ABS.
In common use, the term "mechanical keyboard" refers to a keyboard with individual mechanical key switches, each of which contains a fully encased plunger with a spring below it and metallic electrical contacts on a side. The plunger sits on the spring and the key will often close the contacts when the plunger is pressed half-way. Other switches require the plunger to be fully pressed down. The depth at which the plunger must be pressed for the contacts to close is known as the activation distance. Analog keyboards with key switches whose activation distance can be reconfigured through software, optical switches that work by blocking laser beams, and Hall Effect keyboards that use key switches that use a magnet to activate a hall sensor, are also available.


Control processor

Computer keyboards include control circuitry to convert key presses into key codes (usually scancodes) that the computer's electronics can understand. The key switches are connected via the printed circuit board in an electrical X-Y matrix where a voltage is provided sequentially to the Y lines and, when a key is depressed, detected sequentially by scanning the X lines.
The first computer keyboards were for mainframe computer data terminals and used discrete electronic parts. The first keyboard microprocessor was introduced in 1972 by General Instruments, but keyboards have been using the single-chip 8048 microcontroller variant since it became available in 1978. The keyboard switch matrix is wired to its inputs, it converts the keystrokes to key codes, and, for a detached keyboard, sends the codes down a serial cable (the keyboard cord) to the main processor on the computer motherboard. This serial keyboard cable communication is only bi-directional to the extent that the computer's electronics controls the illumination of the caps lock, num lock and scroll lock lights.
One test for whether the computer has crashed is pressing the caps lock key. The keyboard sends the key code to the keyboard driver running in the main computer; if the main computer is operating, it commands the light to turn on. All the other indicator lights work in a similar way. The keyboard driver also tracks the Shift, alt and control state of the keyboard.
Some lower-quality keyboards have multiple or false key entries due to inadequate electrical designs.  These are caused by inadequate keyswitch "debouncing" or inadequate keyswitch matrix layout that don't allow multiple keys to be depressed at the same time, both circumstances which are explained below:
When pressing a keyboard key, the key contacts may "bounce" against each other for several milliseconds before they settle into firm contact. When released, they bounce some more until they revert to the uncontacted state. If the computer were watching for each pulse, it would see many keystrokes for what the user thought was just one. To resolve this problem, the processor in a keyboard (or computer) "debounces" the keystrokes, by aggregating them across time to produce one "confirmed" keystroke.
Some low-quality keyboards also suffer problems with rollover (that is, when multiple keys pressed at the same time, or when keys are pressed so fast that multiple keys are down within the same milliseconds). Early "solid-state" keyswitch keyboards did not have this problem because the keyswitches are electrically isolated from each other, and early "direct-contact" keyswitch keyboards avoided this problem by having isolation diodes for every keyswitch. These early keyboards had "n-key" rollover, which means any number of keys can be depressed and the keyboard will still recognize the next key depressed. But when three keys are pressed (electrically closed) at the same time in a "direct contact" keyswitch matrix that doesn't have isolation diodes, the keyboard electronics can see a fourth "phantom" key which is the intersection of the X and Y lines of the three keys. Some types of keyboard circuitry will register a maximum number of keys at one time. "Three-key" rollover, also called "phantom key blocking" or "phantom key lockout", will only register three keys and ignore all others until one of the three keys is lifted. This is undesirable, especially for fast typing (hitting new keys before the fingers can release previous keys), and games (designed for multiple key presses).
As direct-contact membrane keyboards became popular, the available rollover of keys was optimized by analyzing the most common key sequences and placing these keys so that they do not potentially produce phantom keys in the electrical key matrix (for example, simply placing three or four keys that might be depressed simultaneously on the same X or same Y line, so that a phantom key intersection/short cannot happen), so that blocking a third key usually isn't a problem. But lower-quality keyboard designs and unknowledgeable engineers may not know these tricks, and it can still be a problem in games due to wildly different or configurable layouts in different games.


Connection types

There are several ways of connecting a keyboard to a system unit (more precisely, to its keyboard controller) using cables, including the standard AT connector commonly found on motherboards, which was eventually replaced by the PS/2 and the USB connection. Prior to the iMac line of systems, Apple used the proprietary Apple Desktop Bus for its keyboard connector.
Wireless keyboards have become popular. A wireless keyboard must have a transmitter built in, and a receiver connected to the computer's keyboard port; it communicates either by radio frequency (RF) or infrared (IR) signals. A wireless keyboard may use industry standard Bluetooth radio communication,  in which case the receiver may be built into the computer. Wireless keyboards need batteries for power, and may be at risk of data eavesdropping. Wireless solar keyboards charge their batteries from small solar panels using natural or artificial light. The 1984 Apricot Portable is an early example of an IR keyboard.


Alternative text-entering methods

Optical character recognition (OCR) is preferable to rekeying for converting existing text that is already written down but not in machine-readable format (for example, a Linotype-composed book from the 1940s). In other words, to convert the text from an image to editable text (that is, a string of character codes), a person could re-type it, or a computer could look at the image and deduce what each character is. OCR technology has already reached an impressive state (for example, Google Book Search) and promises more for the future.
Speech recognition converts speech into machine-readable text (that is, a string of character codes). This technology has also reached an advanced state and is implemented in various software products. For certain uses (e.g., transcription of medical or legal dictation; journalism; writing essays or novels) speech recognition is starting to replace the keyboard.  However, the lack of privacy when issuing voice commands and dictation makes this kind of input unsuitable for many environments.
Pointing devices can be used to enter text or characters in contexts where using a physical keyboard would be inappropriate or impossible. These accessories typically present characters on a display, in a layout that provides fast access to the more frequently used characters or character combinations. Popular examples of this kind of input are Graffiti, Dasher and on-screen virtual keyboards.


Other issues


Keystroke logging
Unencrypted wireless Bluetooth keyboards are known to be vulnerable to signal theft by placing a covert listening device in the same room as the keyboard to sniff and record Bluetooth packets for the purpose of logging keys typed by the user. Microsoft wireless keyboards 2011 and earlier are documented to have this vulnerability.Keystroke logging (often called keylogging) is a method of capturing and recording user keystrokes. While it is used legally to measure employee productivity on certain clerical tasks, or by law enforcement agencies to find out about illegal activities, it is also used by hackers for various illegal or malicious acts. Hackers use keyloggers as a means to obtain passwords or encryption keys and thus bypass other security measures.
Keystroke logging can be achieved by both hardware and software means. Hardware key loggers are attached to the keyboard cable or installed inside standard keyboards. Software keyloggers work on the target computer's operating system and gain unauthorized access to the hardware, hook into the keyboard with functions provided by the OS, or use remote access software to transmit recorded data out of the target computer to a remote location. Some hackers also use wireless keylogger sniffers to collect packets of data being transferred from a wireless keyboard and its receiver, and then they crack the encryption key being used to secure wireless communications between the two devices.
Anti-spyware applications are able to detect many keyloggers and cleanse them. Responsible vendors of monitoring software support detection by anti-spyware programs, thus preventing abuse of the software. Enabling a firewall does not stop keyloggers per se, but can possibly prevent transmission of the logged material over the net if properly configured. Network monitors (also known as reverse-firewalls) can be used to alert the user whenever an application attempts to make a network connection. This gives the user the chance to prevent the keylogger from "phoning home" with his or her typed information. Automatic form-filling programs can prevent keylogging entirely by not using the keyboard at all. Most keyloggers can be fooled by alternating between typing the login credentials and typing characters somewhere else in the focus window.Keyboards are also known to emit electromagnetic signatures that can be detected using special spying equipment to reconstruct the keys pressed on the keyboard. Neal O'Farrell, executive director of the Identity Theft Council, revealed to InformationWeek that "More than 25 years ago, a couple of former spooks showed me how they could capture a user's ATM PIN, from a van parked across the street, simply by capturing and decoding the electromagnetic signals generated by every keystroke," O'Farrell said. "They could even capture keystrokes from computers in nearby offices, but the technology wasn't sophisticated enough to focus in on any specific computer."


Physical injury

The use of any keyboard may cause serious injury (that is, carpal tunnel syndrome or other repetitive strain injury) to hands, wrists, arms, neck or back. The risks of injuries can be reduced by taking frequent short breaks to get up and walk around a couple of times every hour. As well, users should vary tasks throughout the day, to avoid overuse of the hands and wrists. When inputting at the keyboard, a person should keep the shoulders relaxed with the elbows at the side, with the keyboard and mouse positioned so that reaching is not necessary. The chair height and keyboard tray should be adjusted so that the wrists are straight, and the wrists should not be rested on sharp table edges. Wrist or palm rests should not be used while typing.Some adaptive technology ranging from special keyboards, mouse replacements and pen tablet interfaces to speech recognition software can reduce the risk of injury. Pause software reminds the user to pause frequently. Switching to a much more ergonomic mouse, such as a vertical mouse or joystick mouse may provide relief.
By using a touchpad or a stylus pen with a graphic tablet, in place of a mouse, one can lessen the repetitive strain on the arms and hands.


A knitting needle or knitting pin is a tool in hand-knitting to produce knitted fabrics.  They generally have a long shaft and taper at their end, but they are not nearly as sharp as sewing needles.  Their purpose is two-fold.  The long shaft holds the active (unsecured) stitches of the fabric, to prevent them from unravelling, whereas the tapered ends are used to form new stitches. Most commonly, a new stitch is formed by inserting the tapered end through an active stitch, catching a loop (also called a bight) of fresh yarn and drawing it through the stitch; this secures the initial stitch and forms a new active stitch in its place.  In specialized forms of knitting the needle may be passed between active stitches being held on another needle, or indeed between/through inactive stitches that have been knit previously.
The size of a needle is described first by its diameter and secondly by its length.  The size of the new stitch is determined in large part by the diameter of the knitting needle used to form it, because that affects the length of the yarn-loop drawn through the previous stitch.  Thus, large stitches can be made with large needles, whereas fine knitting requires fine needles. In most cases, the knitting needles being used in hand-knitting are of the same diameter; however, in uneven knitting, needles of different sizes may be used. Larger stitches may also be made by wrapping the yarn more than once around the needles with every stitch. The length of a needle determines how many stitches it can hold at once; for example, very large projects such as a shawl with hundreds of stitches might require a longer needle than a small project such as a scarf or bootie.  Various sizing systems for needles are in common use.


Types


Single-pointed needles
The most widely recognized form of needle is the single-pointed needle. It is a slender, straight stick tapered to a point at one end, with a knob at the other end to prevent stitches from slipping off. Such needles are always used in pairs and are usually 10-16 inches (25.440.6 cm) long but, due to the compressibility of knitted fabrics, may be used to knit pieces significantly wider.  The knitting of new stitches occurs only at the tapered ends. Fictional depictions of knitting in movies, television programs, animation, and comic strips almost always show knitting done on straight needles. Both Wallace and Gromit and Monty Python, for example, show this type of knitting.


Double-pointed needles

The oldest type of needle is the straight double-pointed needle. Double-pointed needles are tapered at both ends, which allows them to be knit from either end.  They are typically used (and sold) in sets of four and five, and are commonly used for circular knitting. Since the invention of the circular needle, they have been most commonly used to knit smaller tube-shaped pieces such as sleeves, collars, and socks. Usually two needles are active while the others hold the remaining stitches. Double-pointed needles are somewhat shorter than single-pointed or circular needles, and are usually used in the 1320 cm length range, although they are also made longer.
Double-pointed needles are depicted in a number of 14th-century oil paintings, typically called Knitting Madonnas, depicting Mary knitting with double-pointed needles (Rutt, 2003).

A cable needle is a special type of double-pointed needle that is typically very short and used to hold a very small number of stitches temporarily while the knitter is forming a cable pattern. They are often U-shaped, or have a U-shaped bend, to keep the held stitches from falling off while the primary needle is being used.


Circular needles

The first US patent for a circular needle was issued in 1918, although in Europe they may have been used a little earlier.  Circulars are composed of two pointed, straight tips connected by a flexible cable and may be used for both knitting flat or knitting in the round.  The two tapered ends, typically 45 inches (10.513 cm) long, are rigid, allowing for easy knitting, and are connected by the flexible strand (usually made of nylon or coated wire).  The tips may be permanently connected to the cable and made in overall lengths from 9 inches (23 cm) to 60 inches (150 cm) or composed of cables and interchangeable tips. This allows various lengths and diameters to be combined into many different sizes of needles, allowing for a great variety of needs to be met by a relatively few component pieces.  The ability to work from either end of one needle is convenient in several types of knitting, such as slip-stitch versions of double knitting.
In using circulars to knit flat pieces of fabric the two ends are used just as two separate needles would be.  The knitter holds one tip in each hand and knits straight across the width of the fabric, turns the work, and knits or purls back the other way.  Using circular needles has some advantages, for example, the weight of the fabric is more evenly distributed, therefore less taxing, on the arms and wrists of the knitter and, the length of the cable may be longer than would be practical with rigid needles since the cable and fabric rest in the lap of the knitter rather than extending straight out past the arms.
The lack of a purl row in stockinette stitch, since in the round (commonly referred to as ITR) knitting is all done using the knit stitch, is often perceived to be one of the greatest benefits of ITR.  Knitting ITR with circulars is done in a spiral, the same way as using double-pointed needles (usually called DPNs). Additionally, circulars eliminate the need to continually switch from one needle to the next, and there is no possibility of stitches falling off the back end of the needles, as may happen when using DPNs.  Much larger tubes may be knit ITR, too, helping items to be completed more quickly.  Construction of garments such as sweaters may be greatly simplified when knitting ITR, since the finishing steps of sewing a back, two fronts, and two sleeves of a sweater together may be almost entirely eliminated in neck down ITR knitting.
Knitting educator and authority Elizabeth Zimmermann helped popularize knitting ITR specifically with circular needles.

 Numerous techniques have been devised for the production of narrow tubular knitting on circular needles. One common method is to use two needles in place of the four or five double-pointed needles traditionally used, while a newer technique is to use one circular needle that is significantly longer than the circumference of the item being knitted. This technique is known as Magic Loop and has recently become a popular method of producing tubular knitting, as only one needle is required.


The Guinness World Record for knitting with the largest knitting needles

The current holder of this title is Julia Hopson of Penzance in Cornwall. Julia knitted a tension square of ten stitches and ten rows in stocking stitch using knitting needles that were 6.5 cm in diameter and 3.5 metres long.


Needle materials
In addition to common wood and metal needles, antique knitting needles were sometimes made from tortoiseshell, ivory and walrus tusks; these materials are now banned due to their impact on endangered species, and needles made from them are virtually impossible to find.
There are, however, a now vintage style of needle which appears to be tortoiseshell, but is actually made from a celluloid, sometimes known as shellonite. These needles were made in Australia, but are no longer manufactured.
Modern knitting needles are made of bamboo, aluminium, steel, wood, plastic, glass, casein and carbon fibers.


Needle storage

A tall, cylindrical container with padding on the bottom to keep the points sharp can store straight needles neatly. Fabric or plastic cases similar to cosmetic bags or a chef's knife bag allow straight needles to be stored together yet separated by size, then rolled to maximize space. Circular needles may be stored with the cables coiled in cases made specifically for this purpose or hung dangling from a hanger device with cables straight.  If older circulars with the nylon or plastic cables are coiled for storage it may be necessary to soak them in hot water for a few minutes to get them to uncoil and relax for ease of use.  Most recently manufactured cables eliminate this problem and may be stored coiled without any difficulty.  Care must be taken not to kink the metal cables of older circulars, as these kinks will not come out and may damage or snag yarn as it is knit.


Needle gauge
A needle gauge makes it possible to determine the size of a knitting needle. Some may also be used to gauge the size of crochet hooks. Most needles come with the size written on them, but with use and time, the label often wears off, and many needles (like double-pointed needles) tend not to be labelled.
Needle gauges can be made of any material, but are often made of metal and plastic. They tend to be about 3 by 5 inches. There are holes of various sizes through which the needles are passed to determine which hole they fit best, and often a ruler along the edge for determining the tension (also called gauge) of a sample.


Needle sizes and conversions

In the UK, the metric system is used. Previously, needles 'numbers' were the Standard Wire Gauge designation of the wire from which metal needles were made. The origin of the numbering system is uncertain but it is thought that needle numbers were based on the number of increasingly fine dies that the wire had to be drawn through. This meant thinner needles had a larger number.
In the current US system, things are opposite, that is, smaller numbers indicate smaller needles. There is an "old US system" that is divided into standard and steel needles, the latter being fine lace needles. Occasionally, older lace patterns will refer to these smaller needles in the old measurement system. Finally, there was a system used in continental Europe that predated the metric system. It is largely obsolete, but some older or reprinted patterns call for pins in these sizes.


Genre (from French  genre 'kind, sort') is any form or type of communication in any mode (written, spoken, digital, artistic, etc.) with socially-agreed-upon conventions developed over time. In popular usage, it normally describes a category of literature, music, or other forms of art or entertainment, whether written or spoken, audio or visual, based on some set of stylistic criteria, yet genres can be aesthetic, rhetorical, communicative, or functional. Genres form by conventions that change over time as cultures invent new genres and discontinue the use of old ones. Often, works fit into multiple genres by way of borrowing and recombining these conventions. Stand-alone texts, works, or pieces of communication may have individual styles, but genres are amalgams of these texts based on agreed-upon or socially inferred conventions. Some genres may have rigid, strictly adhered-to guidelines, while others may show great flexibility.
Genre began as an absolute classification system for ancient Greek literature, as set out in Aristotle's Poetics. For Aristotle, poetry (odes, epics, etc.), prose, and performance each had specific design features that supported appropriate content of each genre. Speech patterns for comedy would not be appropriate for tragedy, for example, and even actors were restricted to their genre under the assumption that a type of person could tell one type of story best.
In later periods genres proliferated and developed in response to changes in audiences and creators. Genre became a dynamic tool to help the public make sense out of unpredictable art. Given that art is often a response to a social state, in that people write/paint/sing/dance about what they know about, the use of genre as a tool must be able to adapt to changing meanings. Proponents argue that the genius of an effective genre piece is in the variation, recombination, and evolution of the codes.
Genre suffers from the ills of any classification system. Musician, Ezra LaFleur, argues that discussion of genre should draw from Ludwig Wittgenstein's idea of family resemblance. Genres are helpful labels for communicating but do not necessarily have a single attribute that is the essence of the genre.


Visual arts

The term genre is much used in the history and criticism of visual art, but in art history has meanings that overlap rather confusingly. Genre painting is a term for paintings where the main subject features human figures to whom no specific identity attaches  in other words, figures are not portraits, characters from a story, or allegorical personifications. These are distinguished from staffage: incidental figures in what is primarily a landscape or architectural painting. Genre painting may also be used as a wider term covering genre painting proper, and other specialized types of paintings such as still-life, landscapes, marine paintings and animal paintings.
The concept of the "hierarchy of genres" was a powerful one in artistic theory, especially between the 17th and 19th centuries. It was strongest in France, where it was associated with the Acadmie franaise which held a central role in academic art. The genres in hierarchical order are:

History painting, including narrative, religious, mythological and allegorical subjects
Portrait painting
Genre painting or scenes of everyday life
Landscape (landscapists were the "common footmen in the Army of Art" according to the Dutch theorist Samuel van Hoogstraten) and cityscape
Animal painting
Still life


Literature

A literary genre is a category of literary composition. Genres may be determined by literary technique, tone, content, or even (as in the case of fiction) length. Genre should not be confused with age category, by which literature may be classified as either adult, young adult, or children's. They also must not be confused with format, such as graphic novel or picture book. The distinctions between genres and categories are flexible and loosely defined, often with subgroups.
The most general genres in literature are (in loose chronological order) epic, tragedy, comedy, novel, and short story. They can all be in the genres prose or poetry, which shows best how loosely genres are defined. Additionally, a genre such as satire might appear in any of the above, not only as a subgenre but as a mixture of genres. Finally, they are defined by the general cultural movement of the historical period in which they were composed. In popular fiction, which is especially divided by genres, genre fiction is the more usual term.
In literature, genre has been known as an intangible taxonomy. This taxonomy implies a concept of containment or that an idea will be stable forever. The earliest recorded systems of genre in Western history can be traced back to Plato and Aristotle. Grard Genette, a French literary theorist and author of The Architext, describes Plato as creating three Imitational genres: dramatic dialogue, pure narrative, and epic (a mixture of dialogue and narrative). Lyric poetry, the fourth and final type of Greek literature, was excluded by Plato as a non-mimetic mode. Aristotle later revised Plato's system by eliminating the pure narrative as a viable mode and distinguishing by two additional criteria: the object to be imitated, as objects could be either superior or inferior, and the medium of presentation such as words, gestures or verse.  Essentially, the three categories of mode, object, and medium can be visualized along an XYZ axis.
Excluding the criteria of medium, Aristotle's system distinguished four types of classical genres: tragedy (superior-dramatic dialogue), epic (superior-mixed narrative), comedy (inferior-dramatic dialogue), and parody (inferior-mixed narrative). Genette continues by explaining the later integration of lyric poetry into the classical system during the romantic period, replacing the now removed pure narrative mode. Lyric poetry, once considered non-mimetic, was deemed to imitate feelings, becoming the third leg of a new tripartite system: lyrical, epical, and dramatic dialogue. This system, which came to "dominate all the literary theory of German romanticism (and therefore well beyond)" (38), has seen numerous attempts at expansion or revision. However, more ambitious efforts to expand the tripartite system resulted in new taxonomic systems of increasing scope and complexity.
Genette reflects upon these various systems, comparing them to the original tripartite arrangement: "its structure is somewhat superior tothose that have come after, fundamentally flawed as they are by their inclusive and hierarchical taxonomy, which each time immediately brings the whole game to a standstill and produces an impasse" (74). Taxonomy allows for a structured classification system of genre, as opposed to a more contemporary rhetorical model of genre.


Film

The basic genres of film can be regarded as drama, in the feature film and most cartoons, and documentary.  Most dramatic feature films, especially from Hollywood fall fairly comfortably into one of a long list of film genres such as the Western, war film, horror film, romantic comedy film, musical, crime film, and many others. Many of these genres have a number of subgenres, for example by setting or subject, or a distinctive national style, for example in the Indian Bollywood musical.


Music

A music genre is a conventional category that identifies pieces of music as belonging to a shared tradition or set of conventions. It is to be distinguished from musical form and musical style, although in practice these terms are sometimes used interchangeably. There are numerous genres in Western classical music and popular music, as well as musical theatre and the music of non-Western cultures. The term is now perhaps over-used to describe relatively small differences in musical style in modern rock music, that also may reflect sociological differences in their audiences. Timothy Laurie suggests that in the context of rock and pop music studies, the "appeal of genre criticism is that it makes narratives out of musical worlds that often seem to lack them".Music can be divided into different genres in several ways. The artistic nature of music means that these classifications are often arbitrary and controversial, and some genres may overlap. There are several academic approaches to genres. In his book Form in Tonal Music, Douglass M. Green lists madrigal, motet, canzona, ricercar, and dance as examples of genres from the Renaissance period. According to Green, "Beethoven's Op. 61 and Mendelssohn's Op. 64 are identical in genre  both are violin concertos  but different in form. However, Mozart's Rondo for Piano, K. 511, and the Agnus Dei from his Mass, K. 317 are quite different in genre but happen to be similar in form." Some, like Peter van der Merwe, treat the terms genre and style as the same, saying that genre should be defined as pieces of music that share a certain style or "basic musical language".Others, such as Allan F. Moore, state that genre and style are two separate terms, and that secondary characteristics such as subject matter can also differentiate between genres. A music genre or subgenre may be defined by the musical techniques, the styles, the context, and content and spirit of the themes. Geographical origin is sometimes used to identify a music genre, though a single geographical category will often include a wide variety of subgenres.
Several music scholars have criticised the priority accorded to genre-based communities and listening practices. For example, Laurie argues that "music genres do not belong to isolated, self-sufficient communities. People constantly move between environments where diverse forms of music are heard, advertised and accessorised with distinctive iconographies, narratives and celebrity identities that also touch on non-musical worlds."


Popular culture and other media
The concept of genre is often applied, sometimes rather loosely, to other media with an artistic element, such as video game genres. Genre, and numerous minutely divided subgenres, affect popular culture very significantly, not least as they are used to classify it for publicity purposes.  The vastly increased output of popular culture in the age of electronic media encourages dividing cultural products by genre to simplify the search for products by consumers, a trend the Internet has only intensified.


Linguistics
In philosophy of language, genre figures prominently in the works of philosopher and literary scholar Mikhail Bakhtin. Bakhtin's basic observations were of "speech genres" (the idea of heteroglossia), modes of speaking or writing that people learn to mimic, weave together, and manipulate (such as "formal letter" and "grocery list", or "university lecture" and "personal anecdote"). In this sense, genres are socially specified: recognized and defined (often informally) by a particular culture or community. The work of Georg Lukcs also touches on the nature of literary genres, appearing separately but around the same time (1920s1930s) as Bakhtin. Norman Fairclough has a similar concept of genre that emphasizes the social context of the text: Genres are "different ways of (inter)acting discoursally" (Fairclough, 2003: 26).
A text's genre may be determined by its:

Linguistic function.
Formal traits.
Textual organization.
Relation of communicative situation to formal and organizational traits of the text (Charaudeau and Maingueneau, 2002:278280).


Rhetoric
In the field of rhetoric, genre theorists usually understand genres as types of actions rather than types or forms of texts. On this perspective, texts are channels through which genres are enacted. Carolyn Miller's work has been especially important for this perspective. Drawing on Lloyd Bitzer's concept of rhetorical situation, Miller reasons that recurring rhetorical problems tend to elicit recurring responses; drawing on Alfred Schtz, she reasons that these recurring responses become "typified"  that is, socially constructed as recognizable types. Miller argues that these "typified rhetorical actions" (p. 151) are properly understood as genres.
Building off of Miller, Charles Bazerman and Clay Spinuzzi have argued that genres understood as actions derive their meaning from other genres  that is, other actions. Bazerman therefore proposes that we analyze genres in terms of "genre systems", while Spinuzzi prefers the closely related concept of "genre ecologies".This tradition has had implications for the teaching of writing in American colleges and universities. Combining rhetorical genre theory with activity theory, David Russell has proposed that standard English composition courses are ill-suited to teach the genres that students will write in other contexts across the university and beyond. Elizabeth Wardle contends that standard composition courses do teach genres, but that these are inauthentic "mutt genres" that are often of little use outside of composition courses.


History
This concept of genre originated from the classification systems created by Plato. Plato divided literature into the three classic genres accepted in Ancient Greece: poetry, drama, and prose. Poetry is further subdivided into epic, lyric, and drama. The divisions are recognized as being set by Aristotle and Plato; however, they were not the only ones. Many genre theorists added to these accepted forms of poetry.


Classical and Romance genre theory
The earliest recorded systems of genre in Western history can be traced back to Plato and Aristotle. Grard Genette explains his interpretation of the history of genre in "The Architext". He described Plato as the creator of three imitational, mimetic genres distinguished by mode of imitation rather than content. These three imitational genres include dramatic dialogue, the drama; pure narrative, the dithyramb; and a mixture of the two, the epic. Plato excluded lyric poetry as a non-mimetic, imitational mode. Genette further discussed how Aristotle revised Plato's system by first eliminating the pure narrative as a viable mode. He then uses two additional criteria to distinguish the system. The first of the criteria is the object to be imitated, whether superior or inferior. The second criterion is the medium of presentation: words, gestures, or verse. Essentially, the three categories of mode, object, and medium can be visualized along an XYZ axis. Excluding the criteria of medium, Aristotle's system distinguished four types of classical genres: tragedy, epic, comedy, and parody.
Genette explained the integration of lyric poetry into the classical system by replacing the removed pure narrative mode. Lyric poetry, once considered non-mimetic, was deemed to imitate feelings, becoming the third "Architext", a term coined by Gennette, of a new long-enduring tripartite system: lyrical; epical, the mixed narrative; and dramatic, the dialogue. This new system that came to "dominate all the literary theory of German romanticism" (Genette 38) has seen numerous attempts at expansion and revision. Such attempts include Friedrich Schlegel's triad of subjective form, the lyric; objective form, the dramatic; and subjective-objective form, the epic. However, more ambitious efforts to expand the tripartite system resulted in new taxonomic systems of increasing complexity. Gennette reflected upon these various systems, comparing them to the original tripartite arrangement: "its structure is somewhat superior to most of those that have come after, fundamentally flawed as they are by their inclusive and hierarchical taxonomy, which each time immediately brings the whole game to a standstill and produces an impasse".

In Anthony Pare's study of Inuit social workers in "Genre and Identity: Individuals, Institutions and Ideology", Pare described the conflict between the genre of Inuit social workers' record-keeping forms and the cultural values that prohibited them from fully being able to fulfill the expectations of this genre. Amy Devitt further expands on the concept of culture in her 2004 essay "A Theory of Genre" by adding that "culture defines what situations and genres are likely or possible" (Devitt 24).

Devitt touches on Miller's idea of situation, but expands on it and adds that the relationship with genre and situation is reciprocal. Individuals may find themselves shaping the rhetorical situations, which in turn affect the rhetorical responses that arise out of the situation. Because the social workers worked closely with different families, they did not want to disclose many of the details that are standard in the genre of record keeping related to this field. Giving out such information would violate close cultural ties with the members of their community.


Audiences
Although genres are not always precisely definable, genre considerations are one of the most important factors in determining what a person will see or read. The classification properties of genre can attract or repel potential users depending on the individual's understanding of a genre.
Genre creates an expectation in that expectation is met or not. Many genres have built-in audiences and corresponding publications that support them, such as magazines and websites. Inversely, audiences may call out for change in an antecedent genre and create an entirely new genre.
The term may be used in categorizing web pages, like "news page" and "fan page", with both very different layout, audience, and intention (Rosso, 2008). Some search engines like Vivsimo try to group found web pages into automated categories in an attempt to show various genres the search hits might fit.


Subgenre
A subgenre is a subordinate within a genre. Two stories being the same genre can still sometimes differ in subgenre. For example, if a fantasy story has darker and more frightening elements of fantasy, it would belong in the subgenre of dark fantasy; whereas another fantasy story that features magic swords and wizards would belong to the subgenre of sword and sorcery.


Microgenre

A microgenre is a highly specialized, narrow classification of a cultural practice. The term has come into usage in the 21st century, and most commonly refers to music. It is also associated with the hyper-specific categories used in recommendations for television shows and movies on digital streaming platforms such as Netflix, and is sometimes used more broadly by scholars analyzing niche forms in other periods and other media.


A rivalry is the state of two people or groups engaging in a lasting competitive relationship. Rivalry is the "against each other" spirit between two competing sides. The relationship itself may also be called "a rivalry", and each participant or side a rival to the other. Someone's main rival may be called an archrival.  A rivalry can be defined as "a perceptual categorizing process in which actors identify which states are sufficiently threatening competitors". In order for the rivalry to persist, rather than resulting in perpetual dominance by one side, it must be "a competitive relationship among equals". Political scientist John A. Vasquez has asserted that equality of power is a necessary component for a true rivalry to exist, but others have disputed that element.Rivalries traverse many different fields within society and "abound at all levels of human interaction", often existing between friends, firms, sports teams, schools, and universities. Moreover, "[f]amilies, politicians, political parties, ethnic groups, regional sections of countries, and states all engage in enduring rivalries of varying length and intensity". Rivalries develop from the product of competition and ritualism between different parties. In some cases, rivalry can become "so consuming that actors worry only about whether their actions will harm or benefit their rivals".


Origin and meaning

A rivalry generally refers to competition between people or groups, where each strives to be more successful than the other. Alternatively, and especially when used in the verb form (rivaled and rivaling in American English, and rivalled and rivalling in British English) it may indicate a relationship of equality, as in "the rival of their peers," "a person without rival," or an "unrivaled performance". The origin of the root rival  comes from the Middle French and Latin rivalis, and the French rivus, meaning a person who drinks from or utilizes the same brook or stream as another. The word likely entered the English language around 1577, and appeared in the writings of William Shakespeare as early as 1623, in Two Gentlemen of Verona.In his 1902 Dictionary of Philosophy and Psychology, James Mark Baldwin defined three main types of rivalry:

biological rivalry,
personal or conscious rivalry,
commercial and industrial rivalryAlternatively, Kilduff and colleagues in their 2010 review, instead divided among three types of competition (individual, group, and organization), and distinguished rivalry specifically as a "subjective competitive relationship" which necessarily entails "increased
psychological involvement and perceived stakes". More modern research has also identified similarity, proximity, and history of competition as necessary antecedents for the establishment of a rivalry, while others have suggested that incivility may reduce the need for a history of competition to solidify the rival relationship.Where a person or entity has multiple rivals, the most significant one may be called an archrival. In fiction, it is common for a recurring heroic characters to have an archrival or archenemy to serve as a foil to the hero. However, an archrival may also be distinguished from a nemesis, with the latter being an enemy whom the hero cannot defeat (or who defeats the hero), even while not being a longstanding or consistent enemy to the hero.


Friendly rivalries
A rivalry in which competitors remain at odds over specific issues or outcomes, but otherwise maintain civil relations, can be called a friendly rivalry. Institutions such as universities often maintain friendly rivalries, with the idea that "[a] friendly rivalry encourages an institution to bring to the fore the very best it has to offer, knowing that if it is deficient, others will supersede it". In some instances, institutions such as corporations, sports leagues, or military units, may encourage friendly rivalries between subsets within that institution. For example, in the 1870s, the British Army held a sports competition in which individual military units selected members to compete against those selected by other units, for the purpose of engendering friendly rivalries between the units to promote internal cohesion. Such rivalries may also be encouraged in order to prompt individual members of those subsets to compete more productively.
Interservice rivalries can occur between different branches of a country's armed forces, arising from the competition for limited resources among a nation's land, naval, and air forces. The term also applies to the rivalries between a country's intelligence services (e.g. CIA and FBI in the United States), or between the police and fire services of a city, such as the NYPD and FDNY.


Rivalry in specific fields


Interpersonal relationships
A variety of rivalries occur in interpersonal relationships.
Sibling rivalry is a type of competition or animosity among siblings, whether blood related or not. Siblings generally spend more time together during childhood than they do with parents. The sibling bond is often complicated and is influenced by factors such as parental treatment, birth order, personality, and people and experiences outside the family. Sibling rivalry is particularly intense when children are very close in age and/or of the same gender and/or where one or both children are intellectually gifted. According to a review by Macionis, older siblings tend to report rivalry peaking in childhood, while younger siblings report a peak later during early adolescence.
Rivalries also occur between people who have competing romantic interests in the same potential romantic partner:

The jealousy mechanism is activated if a committed romantic relationship is threatened by a rival. ... In heterosexual relationships, the rival is an individual of the opposite sex; in homosexual relationships, the rival is of the same sex. The rival can be imagined, suspected, or real. The minimal requirement for an individual to be perceived as a rival is fulfilled if the partner is assumed to be attracted to this other person and if this attraction is considered sufficient to eventually result in the partner's infidelity. It is less clear whether someone should be considered a rival who is attracted to the partner when the partner does not return this attraction, as in this case a partner's infidelity appears rather unlikely.
People employ a number of mechanisms to counter romantic rivals, such as discrediting the characteristics of the rival that the romantic partner might seek in a long-term relationship.


Economics and politics

In economics, both goods and producers of goods are said to be rivals. A good is said to be rivalrous if its consumption by one consumer prevents simultaneous consumption by other consumers. Companies that compete to sell the same goods can become rivals as each seeks to convince consumers to purchase its products, to the exclusion of the products of its rival:

The competition of commercial rivals... centers on mutual exclusion from important markets, or the threat thereof. If a commercial rival continues to gain, there is some likelihood that its closest competitor will be excluded altogether from the market in question, or else reduced to a marginal position there. It is not inconceivable that some commercial rivalries transform into strategic rivalries.

In the study of international relations, rivalries between nation states may be highly formalized or comparatively informal. Shohov and colleagues cite Soviet Union-United States relations during the Cold War as one example of a formalized rivalry, "with its period summits and arms-control negotiations". In either case, the formulation of the rivalry carries with it its own expectation of appropriate behaviors among the participants, which works to sustain the relationship, and limit the avenues available to those who would work to undo it. Rivalries between nations can induce them to compete "over naval armaments, foreign aid, cultural influence, and athletic events", the rivalry in each case occurring within the context of the competitors having "labeled one or more of their adversaries as worthy of particular concern and attention". It has been noted that "[w]hile all great powers, almost by definition, are competitors, only some brand each other as rivals", with rivals being "competitors who have been singled out for special attention in some way":
Presumably, there is something unusual about their competitiveness. In most cases, the special significance can be attributed to a perception of acute threat to important values and interests.


Sports
Sports rivalries are often closely connected with the ritualism associated with sports. Ritualism is "a series of ... iterated acts or performances that are ... famous in terms 'not entirely encoded by the performer'; that is, they are imbued by meanings external to the performer". Everyone who is part of a sports event in some capacity becomes a part of the ritualism associated with sports. Teams get together before the game to warm-up, coaches shake hands with each other, captains have a determiner of who gets the ball first, everyone stands during the national anthem, the fans sit in specific areas, make certain gestures with their hands throughout the game, wearing specific gear that is associated with the team, and have the same post-game practices, every game of every season of every year. It is through this consistency of playing the same teams yearly that "these rivalries have shown remarkable staying power". Specifically, it is society's drive to disrupt these original rituals that start rivalries. Horst Helle says, "society needs a particular quantitative relationship of harmony and disharmony, association and competition, favour and disfavour, in order to take shape in a specific way". Society is drawn to this in sports because this is a principal characteristic in everyday life, which can be seen in historic religious rivalries, such as the contemporary example of sectarianism in Glasgow. Within an area, differences between two types of people can drive the start of a rivalry. Competition and support keep the rivalry going.
In sports, competition tests who has better skill and ability at the time of the game through play. Many rivalries persist because the competition is between two teams that have similar abilities. Spectators gravitate towards competitive rivalries because they are interesting to watch and unpredictable. Society follows competitions because competitions influence "the unity of society". Being loyal to one team in a rivalry brings a sense of belonging to a community of supporters that are hoping that the team they are rooting for wins. The fans of the two different teams do not sit next to each other because this disrupts the community. In a similar way, competition displays an indirect way of fighting. Society does not condone direct fighting as a way of getting something so this is the most passive-aggressive way of fighting. Because this is an acceptable practice, there are many supporters of competition as they fuel a way for the people to participate in a rivalry without the consequences of fighting. However, when the competition is not enough in sports and the tensions are high fighting may ensue.A sports writer codified the essentials of a sports rivalry in the United States. To be termed a rivalry, the competition requires 1) true hatred on both sides; not just an inferiority complex from one group of supporters. 2) Proximity - the closer, the better. 3) each team needs to have a winning season. Otherwise the team with the most wins can't take the other team seriously. 4) A "history." Short term rivalries seem irrelevant. 5) Not essential, but important for the "hate" factor is national significance (for college teams). Otherwise, no one else may care.


Effects
Rivalries may increase motivation, lead to greater effort, and better performance. They may also contribute to greater risk taking behavior among participants, and increase a propensity for unethical behavior.These difference may lead to poor decision making on the part of groups and individuals that may not otherwise take place in the absence of a rivalry. Examples examined in the literature include the 1994 attacks by figure skater Tonya Harding against her rival Nancy Kerrigan, the admission in court by British Airways that they had engaged in a number of unethical practices against their business rival Virgin Atlantic (including stealing confidential data and spreading rumors about CEO Richard Branson), and the overpayment of Boston Scientific in their acquisition (called the "second worst" ever) of Guidant, due to the fact that they were bidding against their rival company Johnson & Johnson. At the extreme, competition between rivals "possesses some likelihood of escalation to physical damage".


Pollution is the introduction of contaminants into the natural environment that cause adverse change. Pollution can take the form of chemical substances or energy, such as noise, heat, or light. Pollutants, the components of pollution, can be either foreign substances/energies or naturally occurring contaminants. Pollution is often classed as point source or nonpoint source pollution. In 2015, pollution killed 9 million people worldwide.Major forms of pollution include air pollution, light pollution, litter, noise pollution, plastic pollution, soil contamination, radioactive contamination, thermal pollution, visual pollution, and water pollution.


History
Air pollution has always accompanied civilizations. Pollution started from prehistoric times, when man created the first fires. According to a 1983 article in the journal Science, "soot" found on ceilings of prehistoric caves provides ample evidence of the high levels of pollution that was associated with inadequate ventilation of open fires." Metal forging appears to be a key turning point in the creation of significant air pollution levels outside the home. Core samples of glaciers in Greenland indicate increases in pollution associated with Greek, Roman, and Chinese metal production.


Urban pollution

The burning of coal and wood, and the presence of many horses in concentrated areas made the cities the primary sources of pollution. The Industrial Revolution brought an infusion of untreated chemicals and wastes into local streams that served as the water supply. King Edward I of England banned the burning of sea-coal by proclamation in London in 1272, after its smoke became a problem; the fuel was so common in England that this earliest of names for it was acquired because it could be carted away from some shores by the wheelbarrow.
It was the Industrial Revolution that gave birth to environmental pollution as we know it today. London also recorded one of the earlier extreme cases of water quality problems with the Great Stink on the Thames of 1858, which led to construction of the London sewerage system soon afterward. Pollution issues escalated as population growth far exceeded viability of neighborhoods to handle their waste problem. Reformers began to demand sewer systems and clean water.In 1870, the sanitary conditions in Berlin were among the worst in Europe. August Bebel recalled conditions before a modern sewer system was built in the late 1870s:

Waste-water from the houses collected in the gutters running alongside the curbs and emitted a truly fearsome smell. There were no public toilets in the streets or squares. Visitors, especially women, often became desperate when nature called. In the public buildings the sanitary facilities were unbelievably primitive....As a metropolis, Berlin did not emerge from a state of barbarism into civilization until after 1870."
The primitive conditions were intolerable for a world national capital, and the Imperial German government brought in its scientists, engineers, and urban planners to not only solve the deficiencies, but to forge Berlin as the world's model city. A British expert in 1906 concluded that Berlin represented "the most complete application of science, order and method of public life," adding "it is a marvel of civic administration, the most modern and most perfectly organized city that there is."The emergence of great factories and consumption of immense quantities of coal gave rise to unprecedented air pollution and the large volume of industrial chemical discharges added to the growing load of untreated human waste. Chicago and Cincinnati were the first two American cities to enact laws ensuring cleaner air in 1881. Pollution became a major issue in the United States in the early twentieth century, as progressive reformers took issue with air pollution caused by coal burning, water pollution caused by bad sanitation, and street pollution caused by the 3 million horses who worked in American cities in 1900, generating large quantities of urine and manure. As historian Martin Melosi notes, the generation that first saw automobiles replacing the horses saw cars as "miracles of cleanliness". By the 1940s, however, automobile-caused smog was a major issue in Los Angeles.Other cities followed around the country until early in the 20th century, when the short lived Office of Air Pollution was created under the Department of the Interior. Extreme smog events were experienced by the cities of Los Angeles and Donora, Pennsylvania in the late 1940s, serving as another public reminder.Air pollution would continue to be a problem in England, especially later during the industrial revolution, and extending into the recent past with the Great Smog of 1952. Awareness of atmospheric  pollution spread widely after World War II, with fears triggered by reports of radioactive fallout from atomic warfare and testing. Then a non-nuclear event  the Great Smog of 1952 in London  killed at least 4000 people. This prompted some of the first major modern environmental legislation: the Clean Air Act of 1956.

Pollution began to draw major public attention in the United States between the mid-1950s and early 1970s, when Congress passed the Noise Control Act, the Clean Air Act, the Clean Water Act, and the National Environmental Policy Act. 
Severe incidents of pollution helped increase consciousness. PCB dumping in the Hudson River resulted in a ban by the EPA on consumption of its fish in 1974. National news stories in the late 1970s  especially the long-term dioxin contamination at Love Canal starting in 1947 and uncontrolled dumping in Valley of the Drums  led to the Superfund legislation of 1980. The pollution of industrial land gave rise to the name brownfield, a term now common in city planning.
The development of nuclear science introduced radioactive contamination, which can remain lethally radioactive for hundreds of thousands of years. Lake Karachay  named by the Worldwatch Institute as the "most polluted spot" on earth  served as a disposal site for the Soviet Union throughout the 1950s and 1960s. Chelyabinsk, Russia, is considered the "Most polluted place on the planet".Nuclear weapons continued to be tested in the Cold War, especially in the earlier stages of their development. The toll on the worst-affected populations and the growth since then in understanding about the critical threat to human health posed by radioactivity has also been a prohibitive complication associated with nuclear power. Though extreme care is practiced in that industry, the potential for disaster suggested by incidents such as those at Three Mile Island, Chernobyl, and Fukushima pose a lingering specter of public mistrust. Worldwide publicity has been intense on those disasters. Widespread support for test ban treaties has ended almost all nuclear testing in the atmosphere.International catastrophes such as the wreck of the Amoco Cadiz oil tanker off the coast of Brittany in 1978 and the Bhopal disaster in 1984 have demonstrated the universality of such events and the scale on which efforts to address them needed to engage. The borderless nature of atmosphere and oceans inevitably resulted in the implication of pollution on a planetary level with the issue of global warming. Most recently the term persistent organic pollutant (POP) has come to describe a group of chemicals such as PBDEs and PFCs among others. Though their effects remain somewhat less well understood owing to a lack of experimental data, they have been detected in various ecological habitats far removed from industrial activity such as the Arctic, demonstrating diffusion and bioaccumulation after only a relatively brief period of widespread use.

A much more recently discovered problem is the Great Pacific Garbage Patch, a huge concentration of plastics, chemical sludge and other debris which has been collected into a large area of the Pacific Ocean by the North Pacific Gyre. This is a less well known pollution problem than the others described above, but nonetheless has multiple and serious consequences such as increasing wildlife mortality, the spread of invasive species and human ingestion of toxic chemicals. Organizations such as 5 Gyres have researched the pollution and, along with artists like Marina DeBris, are working toward publicizing the issue.
Pollution introduced by light at night is becoming a global problem, more severe in urban centres, but nonetheless contaminating also large territories, far away from towns.Growing evidence of local and global pollution and an increasingly informed public over time have given rise to environmentalism and the environmental movement, which generally seek to limit human impact on the environment.


Forms of pollution

The major forms of pollution are listed below along with the particular contaminant relevant to each of them:

Air pollution: the release of chemicals and particulates into the atmosphere. Common gaseous pollutants include carbon monoxide, sulfur dioxide, chlorofluorocarbons (CFCs) and nitrogen oxides produced by industry and motor vehicles. Photochemical ozone and smog are created as nitrogen oxides and hydrocarbons react to sunlight. Particulate matter, or fine dust is characterized by their micrometre size PM10 to PM2.5.
Electromagnetic pollution: the overabundance of electromagnetic radiation in their non-ionizing form, like radio waves, etc, that people are constantly exposed at, especially in large cities. It's still unknown whether or not those types of radiation have any effects on human health, though.
Light pollution: includes light trespass, over-illumination and astronomical interference.
Littering: the criminal throwing of inappropriate man-made objects, unremoved, onto public and private properties.
Noise pollution: which encompasses roadway noise, aircraft noise, industrial noise as well as high-intensity sonar.
Plastic pollution: involves the accumulation of plastic products and microplastics in the environment that adversely affects wildlife, wildlife habitat, or humans.
Soil contamination occurs when chemicals are released by spill or underground leakage. Among the most significant soil contaminants are hydrocarbons, heavy metals, MTBE, herbicides, pesticides and chlorinated hydrocarbons.
Radioactive contamination, resulting from 20th century activities in atomic physics, such as nuclear power generation and nuclear weapons research, manufacture and deployment. (See alpha emitters and actinides in the environment.)
Thermal pollution, is a temperature change in natural water bodies caused by human influence, such as use of water as coolant in a power plant.
Visual pollution, which can refer to the presence of overhead power lines, motorway billboards, scarred landforms (as from strip mining), open storage of trash, municipal solid waste or space debris.
Water pollution, by the discharge of wastewater from commercial and industrial waste (intentionally or through spills) into surface waters; discharges of untreated domestic sewage, and chemical contaminants, such as chlorine, from treated sewage; release of waste and contaminants into surface runoff flowing to surface waters (including urban runoff and agricultural runoff, which may contain chemical fertilizers and pesticides; also including human feces from open defecation  still a major problem in many developing countries); groundwater pollution from waste disposal and leaching into the ground, including from pit latrines and septic tanks; eutrophication and littering.


Pollutants

A pollutant is a waste material that pollutes air, water, or soil. Three factors determine the severity of a pollutant: its chemical nature, the concentration, the area affected and the persistence.


Cost of pollution
Pollution has a cost. Manufacturing activities that cause air pollution impose health and clean-up costs on the whole of society, whereas the neighbors of an individual who chooses to fire-proof his home may benefit from a reduced risk of a fire spreading to their own homes. A manufacturing activity that causes air pollution is an example of a negative externality in production. A negative externality in production occurs when a firms production reduces the well-being of others who are not compensated by the firm." For example, if a laundry firm exists near a polluting steel manufacturing firm, there will be increased costs for the laundry firm because of the dirt and smoke produced by the steel manufacturing firm. If external costs exist, such as those created by pollution, the manufacturer will choose to produce more of the product than would be produced if the manufacturer were required to pay all associated environmental costs. Because responsibility or consequence for self-directed action lies partly outside the self, an element of externalization is involved. If there are external benefits, such as in public safety, less of the good may be produced than would be the case if the producer were to receive payment for the external benefits to others. However, goods and services that involve negative externalities in production, such as those that produce pollution, tend to be over-produced and underpriced since the externality is not being priced into the market.Pollution can also create costs for the firms producing the pollution. Sometimes firms choose, or are forced by regulation, to reduce the amount of pollution that they are producing. The associated costs of doing this are called abatement costs, or marginal abatement costs if measured by each additional unit. In 2005 pollution abatement capital expenditures and operating costs in the US amounted to nearly $27 billion.


Socially optimal level of pollution
Society derives some indirect utility from pollution, otherwise there would be no incentive to pollute. This utility comes from the consumption of goods and services that create pollution. Therefore, it is important that policymakers attempt to balance these indirect benefits with the costs of pollution in order to achieve an efficient outcome.

It is possible to use environmental economics to determine which level of pollution is deemed the social optimum. For economists, pollution is an external cost and occurs only when one or more individuals suffer a loss of welfare, however, there exists a socially optimal level of pollution at which welfare is maximized. This is because consumers derive utility from the good or service manufactured, which will outweigh the social cost of pollution until a certain point. At this point the damage of one extra unit of pollution to society, the marginal cost of pollution, is exactly equal to the marginal benefit of consuming one more unit of the good or service.In markets with pollution, or other negative externalities in production, the free market equilibrium will not account for the costs of pollution on society. If the social costs of pollution are higher than the private costs incurred by the firm, then the true supply curve will be higher. The point at which the social marginal cost and market demand intersect gives the socially optimal level of pollution. At this point, the quantity will be lower and the price will be higher in comparison to the free market equilibrium. Therefore, the free market outcome could be considered a market failure because it does not maximize efficiency.This model can be used as a basis to evaluate different methods of internalizing the externality. Some examples include tariffs, a carbon tax and cap and trade systems.


Sources and causes

Air pollution comes from both natural and human-made (anthropogenic) sources. However, globally human-made pollutants from combustion, construction, mining, agriculture and warfare are increasingly significant in the air pollution equation.Motor vehicle emissions are one of the leading causes of air pollution. China, United States, Russia, India Mexico, and Japan are the world leaders in air pollution emissions. Principal stationary pollution sources include chemical plants, coal-fired power plants, oil refineries, petrochemical plants, nuclear waste disposal activity, incinerators, large livestock farms (dairy cows, pigs, poultry, etc.), PVC factories, metals production factories, plastics factories, and other heavy industry. Agricultural air pollution comes from contemporary practices which include clear felling and burning of natural vegetation as well as spraying of pesticides and herbicidesAbout 400 million metric tons of hazardous wastes are generated each year. The United States alone produces about 250 million metric tons. Americans constitute less than 5% of the world's population, but produce roughly 25% of the world's CO2, and generate approximately 30% of world's waste. In 2007, China overtook the United States as the world's biggest producer of CO2, while still far behind based on per capita pollution (ranked 78th among the world's nations).

In February 2007, a report by the Intergovernmental Panel on Climate Change (IPCC), representing the work of 2,500 scientists, economists, and policymakers from more than 120 countries, confirmed that humans have been the primary cause of global warming since 1950. Humans have ways to cut greenhouse gas emissions and avoid the consequences of global warming, a major climate report concluded. But to change the climate, the transition from fossil fuels like coal and oil needs to occur within decades, according to the final report this year from the UN's Intergovernmental Panel on Climate Change (IPCC).Some of the more common soil contaminants are chlorinated hydrocarbons (CFH), heavy metals (such as chromium, cadmium  found in rechargeable batteries, and lead  found in lead paint, aviation fuel and still in some countries, gasoline), MTBE, zinc, arsenic and benzene. In 2001 a series of press reports culminating in a book called Fateful Harvest unveiled a widespread practice of recycling industrial byproducts into fertilizer, resulting in the contamination of the soil with various metals. Ordinary municipal landfills are the source of many chemical substances entering the soil environment (and often groundwater), emanating from the wide variety of refuse accepted, especially substances illegally discarded there, or from pre-1970 landfills that may have been subject to little control in the U.S. or EU. There have also been some unusual releases of polychlorinated dibenzodioxins, commonly called dioxins for simplicity, such as TCDD.Pollution can also be the consequence of a natural disaster. For example, hurricanes often involve water contamination from sewage, and petrochemical spills from ruptured boats or automobiles. Larger scale and environmental damage is not uncommon when coastal oil rigs or refineries are involved. Some sources of pollution, such as nuclear power plants or oil tankers, can produce widespread and potentially hazardous releases when accidents occur.
In the case of noise pollution the dominant source class is the motor vehicle, producing about ninety percent of all unwanted noise worldwide.


Effects


Human health

Adverse air quality can kill many organisms, including humans. Ozone pollution can cause respiratory disease, cardiovascular disease, throat inflammation, chest pain, and congestion. Water pollution causes approximately 14,000 deaths per day, mostly due to contamination of drinking water by untreated sewage in developing countries. An estimated 500 million Indians have no access to a proper toilet, Over ten million people in India fell ill with waterborne illnesses in 2013, and 1,535 people died, most of them children. Nearly 500 million Chinese lack access to safe drinking water. A 2010 analysis estimated that 1.2 million people died prematurely each year in China because of air pollution. The high smog levels China has been facing for a long time can do damage to civilians' bodies and cause different diseases. The WHO estimated in 2007 that air pollution causes half a million deaths per year in India. Studies have estimated that the number of people killed annually in the United States could be over 50,000.Oil spills can cause skin irritations and rashes. Noise pollution induces hearing loss, high blood pressure, stress, and sleep disturbance. Mercury has been linked to developmental deficits in children and neurologic symptoms. Older people are majorly exposed to diseases induced by air pollution. Those with heart or lung disorders are at additional risk. Children and infants are also at serious risk. Lead and other heavy metals have been shown to cause neurological problems. Chemical and radioactive substances can cause cancer and as well as birth defects.
An October 2017 study by the Lancet Commission on Pollution and Health found that global pollution, specifically toxic air, water, soils and workplaces, kills nine million people annually, which is triple the number of deaths caused by AIDS, tuberculosis and malaria combined, and 15 times higher than deaths caused by wars and other forms of human violence. The study concluded that "pollution is one of the great existential challenges of the Anthropocene era. Pollution endangers the stability of the Earths support systems and threatens the continuing survival of human societies."


Environment
 
Pollution has been found to be present widely in the environment. There are a number of effects of this:

Biomagnification describes situations where toxins (such as heavy metals) may pass through trophic levels, becoming exponentially more concentrated in the process.
Carbon dioxide emissions cause ocean acidification, the ongoing decrease in the pH of the Earth's oceans as CO2 becomes dissolved.
The emission of greenhouse gases leads to global warming which affects ecosystems in many ways.
Invasive species can outcompete native species and reduce biodiversity. Invasive plants can contribute debris and biomolecules (allelopathy) that can alter soil and chemical compositions of an environment, often reducing native species competitiveness.
Nitrogen oxides are removed from the air by rain and fertilise land which can change the species composition of ecosystems.
Smog and haze can reduce the amount of sunlight received by plants to carry out photosynthesis and leads to the production of tropospheric ozone which damages plants.
Soil can become infertile and unsuitable for plants. This will affect other organisms in the food web.
Sulfur dioxide and nitrogen oxides can cause acid rain which lowers the pH value of soil.
Organic pollution of watercourses can deplete oxygen levels and reduce species diversity.


Environmental health information
The Toxicology and Environmental Health Information Program (TEHIP) at the United States National Library of Medicine (NLM) maintains a comprehensive toxicology and environmental health web site that includes access to resources produced by TEHIP and by other government agencies and organizations. This web site includes links to databases, bibliographies, tutorials, and other scientific and consumer-oriented resources. TEHIP also is responsible for the Toxicology Data Network (TOXNET) an integrated system of toxicology and environmental health databases that are available free of charge on the web.
TOXMAP is a Geographic Information System (GIS) that is part of TOXNET. TOXMAP uses maps of the United States to help users visually explore data from the United States Environmental Protection Agency's (EPA) Toxics Release Inventory and Superfund Basic Research Programs.


School outcomes
A 2019 paper linked pollution to adverse school outcomes for children.


Worker productivity
A number of studies show that pollution has an adverse effect on the productivity of both indoor and outdoor workers.


Regulation and monitoring

To protect the environment from the adverse effects of pollution, many nations worldwide have enacted legislation to regulate various types of pollution as well as to mitigate the adverse effects of pollution.


Pollution control

Pollution control is a term used in environmental management. It means the control of emissions and effluents into air, water or soil. Without pollution control, the waste products from overconsumption, heating, agriculture, mining, manufacturing, transportation and other human activities, whether they accumulate or disperse, will degrade the environment. In the hierarchy of controls, pollution prevention and waste minimization are more desirable than pollution control. In the field of land development, low impact development is a similar technique for the prevention of urban runoff.


Practices
Recycling
Reusing
Waste minimisation
Mitigating
Preventing
Compost


Pollution control devices
Air pollution control
Thermal oxidizer
Dust collection systems
Baghouses
Cyclones
Electrostatic precipitators
Scrubbers
Baffle spray scrubber
Cyclonic spray scrubber
Ejector venturi scrubber
Mechanically aided scrubber
Spray tower
Wet scrubber
Sewage treatment
Sedimentation (Primary treatment)
Activated sludge biotreaters (Secondary treatment; also used for industrial wastewater)
Aerated lagoons
Constructed wetlands (also used for urban runoff)
Industrial wastewater treatment
API oil-water separators
Biofilters
Dissolved air flotation (DAF)
Powdered activated carbon treatment
Ultrafiltration
Vapor recovery systems
Phytoremediation


Perspectives
The earliest precursor of pollution generated by life forms would have been a natural function of their existence. The attendant consequences on viability and population levels fell within the sphere of natural selection. These would have included the demise of a population locally or ultimately, species extinction. Processes that were untenable would have resulted in a new balance brought about by changes and adaptations. At the extremes, for any form of life, consideration of pollution is superseded by that of survival.
For humankind, the factor of technology is a distinguishing and critical consideration, both as an enabler and an additional source of byproducts. Short of survival, human concerns include the range from quality of life to health hazards. Since science holds experimental demonstration to be definitive, modern treatment of toxicity or environmental harm involves defining a level at which an effect is observable. Common examples of fields where practical measurement is crucial include automobile emissions control, industrial exposure (e.g. Occupational Safety and Health Administration (OSHA) PELs), toxicology (e.g. LD50), and medicine (e.g. medication and radiation doses).
"The solution to pollution is dilution", is a dictum which summarizes a traditional approach to pollution management whereby sufficiently diluted pollution is not harmful. It is well-suited to some other modern, locally scoped applications such as laboratory safety procedure and hazardous material release emergency management. But it assumes that the diluent is in virtually unlimited supply for the application or that resulting dilutions are acceptable in all cases.
Such simple treatment for environmental pollution on a wider scale might have had greater merit in earlier centuries when physical survival was often the highest imperative, human population and densities were lower, technologies were simpler and their byproducts more benign. But these are often no longer the case. Furthermore, advances have enabled measurement of concentrations not possible before. The use of statistical methods in evaluating outcomes has given currency to the principle of probable harm in cases where assessment is warranted but resorting to deterministic models is impractical or infeasible. In addition, consideration of the environment beyond direct impact on human beings has gained prominence.
Yet in the absence of a superseding principle, this older approach predominates practices throughout the world. It is the basis by which to gauge concentrations of effluent for legal release, exceeding which penalties are assessed or restrictions applied. One such superseding principle is contained in modern hazardous waste laws in developed countries, as the process of diluting hazardous waste to make it non-hazardous is usually a regulated treatment process. Migration from pollution dilution to elimination in many cases can be confronted by challenging economical and technological barriers.


Greenhouse gases and global warming

Carbon dioxide, while vital for photosynthesis, is sometimes referred to as pollution, because raised levels of the gas in the atmosphere are affecting the Earth's climate. Disruption of the environment can also highlight the connection between areas of pollution that would normally be classified separately, such as those of water and air. Recent studies have investigated the potential for long-term rising levels of atmospheric carbon dioxide to cause slight but critical increases in the acidity of ocean waters, and the possible effects of this on marine ecosystems.


Rossby waves and surface air pollution
Air pollution fluctuations have been
known to strongly depend on the weather dynamics.
A recent study developed  a multi-layered network analysis and detected strong
interlinks between the geopotential height of the upper air ( 5 km) and surface air pollution
in both China and the USA. This study indicates that Rossby waves significantly affect air pollution fluctuations
through the development of cyclone and anticyclone systems, and further affect the
local stability of the air and the winds. The Rossby waves impact on air pollution
has been observed in the daily fluctuations in surface air pollution. Thus, the impact
of Rossby waves on human life is significant and rapid warming of
the Arctic could slow down Rossby waves, thus increasing human health risks.


Most polluting industries
The Pure Earth, an international non-for-profit organization dedicated to eliminating life-threatening pollution in the developing world, issues an annual list of some of the world's most polluting industries.
Lead-Acid Battery Recycling
Industrial Mining and Ore Processing
Lead Smelting
Tannery Operations
Artisanal Small-Scale Gold Mining
Industrial/Municipal Dumpsites
Industrial Estates
Chemical Manufacturing
Product Manufacturing
Dye IndustryA 2018 report by the Institute for Agriculture and Trade Policy and GRAIN says that the meat and dairy industries are poised to surpass the oil industry as the world's worst polluters.


Worlds worst polluted places
Pure Earth issues an annual list of some of the world's worst polluted places.
Agbogbloshie, Ghana
Chernobyl, Ukraine
Citarum River, Indonesia
Dzershinsk, Russia
Hazaribagh, Bangladesh
Kabwe, Zambia
Kalimantan, Indonesia
Matanza Riachuelo, Argentina
Niger River Delta, Nigeria
Norilsk, Russia


Frost is a thin layer of ice on a solid surface, which forms from water vapor in an above-freezing atmosphere coming in contact with a solid surface whose temperature is below freezing, and resulting in a phase change from water vapor (a gas) to ice (a solid) as the water vapor reaches the freezing point. In temperate climates, it most commonly appears on surfaces near the ground as fragile white crystals; in cold climates, it occurs in a greater variety of forms. The propagation of crystal formation occurs by the process of nucleation.
The ice crystals of frost form as the result of fractal process development. The depth of frost crystals varies depending on the amount of time they have been accumulating, and the concentration of the water vapor (humidity). Frost crystals may be invisible (black), clear (translucent), or white; if a mass of frost crystals scatters light in all directions, the coating of frost appears white.
Types of frost include crystalline frost (hoar frost or radiation frost) from deposition of water vapor from air of low humidity, white frost in humid conditions, window frost on glass surfaces, advection frost from cold wind over cold surfaces, black frost without visible ice at low temperatures and very low humidity, and rime under supercooled wet conditions.Plants that have evolved in warmer climates suffer damage when the temperature falls low enough to freeze the water in the cells  that make up the plant tissue. The tissue damage resulting from this process is known as "frost damage". Farmers in those regions where frost damage is known to affect their crops often invest in substantial means to protect their crops from such damage.


Formation

If a solid surface is chilled below the dew point of the surrounding humid air, and the surface itself is colder than freezing, ice will form on it. If the water deposits as a liquid that then freezes, it forms a coating that may look glassy, opaque, or crystalline, depending on its type. Depending on context, that process also may be called atmospheric icing. The ice it produces differs in some ways from crystalline frost, which consists of spicules of ice that typically project from the solid surface on which they grow.
The main difference between the ice coatings and frost spicules arises because the crystalline spicules grow directly from desublimation of water vapour from air, and desublimation is not a factor in icing of freezing surfaces. For desublimation to proceed, the surface must be below the frost point of the air, meaning that it is sufficiently cold for ice to form without passing through the liquid phase. The air must be humid, but not sufficiently humid to permit the condensation of liquid water, or icing will result instead of desublimation. The size of the crystals depends largely on the temperature, the amount of water vapor available, and how long they have been growing undisturbed.
As a rule, except in conditions where supercooled droplets are present in the air, frost will form only if the deposition surface is colder than the surrounding air. For instance, frost may be observed around cracks in cold wooden sidewalks when humid air escapes from the warmer ground beneath. Other objects on which frost commonly forms are those with low specific heat or high thermal emissivity, such as blackened metals, hence the accumulation of frost on the heads of rusty nails.
The apparently erratic occurrence of frost in adjacent localities is due partly to differences of elevation, the lower areas becoming colder on calm nights. Where static air settles above an area of ground in the absence of wind, the absorptivity and specific heat of the ground strongly influence the temperature that the trapped air attains.


Types


Hoar frost

Hoar frost, also hoarfrost, radiation frost, or pruina, refers to white ice crystals deposited on the ground or loosely attached to exposed objects, such as wires or leaves. They form on cold, clear nights when conditions are such that heat radiates out to the open air faster than it can be replaced from nearby sources, such as wind or warm objects. Under suitable circumstances, objects cool to below the frost point of the surrounding air, well below the freezing point of water. Such freezing may be promoted by effects such as flood frost or frost pocket. These occur when ground-level radiation losses cool air until it flows downhill and accumulates in pockets of very cold air in valleys and hollows. Hoar frost may freeze in such low-lying cold air even when the air temperature a few feet above ground is well above freezing.
The word "hoar" comes from an Old English adjective that means "showing signs of old age". In this context, it refers to the frost that makes trees and bushes look like white hair. 
Hoar frost may have different names depending on where it forms:

Air hoar is a deposit of hoar frost on objects above the surface, such as tree branches, plant stems, and wires.
Surface hoar refers to fern-like ice crystals directly deposited on snow, ice, or already frozen surfaces.
Crevasse hoar consists of crystals that form in glacial crevasses where water vapour can accumulate under calm weather conditions.
Depth hoar refers to faceted crystals that have slowly grown large within cavities beneath the surface of banks of dry snow. Depth hoar crystals grow continuously at the expense of neighbouring smaller crystals, so typically are visibly stepped and have faceted hollows.When surface hoar covers sloping snowbanks, the layer of frost crystals may create an avalanche risk; when heavy layers of new snow cover the frosty surface, furry crystals standing out from the old snow hold off the falling flakes, forming a layer of voids that prevents the new snow layers from bonding strongly to the old snow beneath. Ideal conditions for hoarfrost to form on snow are cold, clear nights, with very light, cold air currents conveying humidity at the right rate for growth of frost crystals. Wind that is too strong or warm destroys the furry crystals, and thereby may permit a stronger bond between the old and new snow layers. However, if the winds are strong enough and cold enough to lay the crystals flat and dry, carpeting the snow with cold, loose crystals without removing or destroying them or letting them warm up and become sticky, then the frost interface between the snow layers may still present an avalanche danger, because the texture of the frost crystals differs from the snow texture, and the dry crystals will not stick to fresh snow. Such conditions still prevent a strong bond between the snow layers.In very low temperatures where fluffy surface hoar crystals form without subsequently being covered with snow, strong winds may break them off, forming a dust of ice particles and blowing them over the surface. The ice dust then may form yukimarimo, as has been observed in parts of Antarctica, in a process similar to the formation of dust bunnies and similar structures.

Hoar frost and white frost also occur in man-made environments such as in freezers or industrial cold-storage facilities. If such cold spaces or the pipes serving them are not well insulated and are exposed to ambient humidity, the moisture will freeze instantly depending on the freezer temperature. The frost may coat pipes thickly, partly insulating them, but such inefficient insulation still is a source of heat loss.


Advection frost
Advection frost (also called wind frost) refers to tiny ice spikes that form when very cold wind is blowing over tree branches, poles, and other surfaces. It looks like rimming on the edges of flowers and leaves, and usually forms against the direction of the wind. It can occur at any hour, day or night.


Window frost
Window frost (also called fern frost or ice flowers) forms when a glass pane is exposed to very cold air on the outside and warmer, moderately moist air on the inside. If the pane is a bad insulator (for example, if it is a single-pane window), water vapour condenses on the glass, forming frost patterns. With very low temperatures outside, frost can appear on the bottom of the window even with double-pane energy-efficient windows because the air convection between two panes of glass ensures that the bottom part of the glazing unit is colder than the top part. On unheated motor vehicles, the frost usually forms on the outside surface of the glass first. The glass surface influences the shape of crystals, so imperfections, scratches, or dust can modify the way ice nucleates. The patterns in window frost form a fractal with a fractal dimension greater than one, but less than two. This is a consequence of the nucleation process being constrained to unfold in two dimensions, unlike a snowflake, which is shaped by a similar process, but forms in three dimensions and has a fractal dimension greater than two.If the indoor air is very humid, rather than moderately so, water first condenses in small droplets, and then freezes into clear ice.
Similar patterns of freezing may occur on other smooth vertical surfaces, but they seldom are as obvious or spectacular as on clear glass.









White frost
White frost is a solid deposition of ice that forms directly from water vapour contained in air.
White frost forms when relative humidity is above 90% and the temperature below 8 C (18 F), and it grows against the wind direction, since air arriving from windward has a higher humidity than leeward air, but the wind must not be strong, else it damages the delicate icy structures as they begin to form. White frost resembles a heavy coating of hoar frost with big, interlocking crystals, usually needle-shaped.


Rime

Rime is a type of ice deposition that occurs quickly, often under heavily humid and windy conditions. Technically speaking, it is not a type of frost, since usually supercooled water drops are involved, in contrast to the formation of hoar frost, in which water vapour desublimates slowly and directly. Ships travelling through Arctic seas may accumulate large quantities of rime on the rigging. Unlike hoar frost, which has a feathery appearance, rime generally has an icy, solid appearance.


Black frost
Black frost (or "killing frost") is not strictly speaking frost at all, because it is the condition seen in crops when the humidity is too low for frost to form, but the temperature falls so low that plant tissues freeze and die, becoming blackened, hence the term "black frost". Black frost often is called "killing frost" because white frost tends to be less cold, partly because the latent heat of freezing of the water reduces the temperature drop.


Effect on plants


Damage

Many plants can be damaged or killed by freezing temperatures or frost. This varies with the type of plant, the tissue exposed, and how low temperatures get; a "light frost" of 2 to 0 C (28 to 32 F)  damages fewer types of plants than a "hard frost" below 2 C (28 F).Plants likely to be damaged even by a light frost include vinessuch as beans, grapes, squashes, melonsalong with nightshades such as tomatoes, eggplants, and peppers. Plants that may tolerate (or even benefit) from frosts include:
root vegetables (e.g. beets, carrots, parsnips, onions)
leafy greens (e.g. lettuces, spinach, chard, cucumber)
cruciferous vegetables (e.g. cabbages, cauliflower, bok choy, broccoli, Brussels sprouts, radishes, kale, collard, mustard, turnips, rutabagas)Even those plants that tolerate frost may be damaged once temperatures drop even lower (below 4 C or 25 F). Hardy perennials, such as Hosta, become dormant after the first frosts and regrow when spring arrives. The entire visible plant may turn completely brown until the spring warmth, or may drop all of its leaves and flowers, leaving the stem and stalk only. Evergreen plants, such as pine trees, withstand frost although all or most growth stops. Frost crack is a bark defect caused by a combination of low temperatures and heat from the winter sun.
Vegetation is not necessarily damaged when leaf temperatures drop below the freezing point of their cell contents. In the absence of a site nucleating the formation of ice crystals, the leaves remain in a supercooled liquid state, safely reaching temperatures of 4 to 12 C (25 to 10 F). However, once frost forms, the leaf cells may be damaged by sharp ice crystals. Hardening is the process by which a plant becomes tolerant to low temperatures. See also Cryobiology.
Certain bacteria, notably Pseudomonas syringae, are particularly effective at triggering frost formation, raising the nucleation temperature to about 2 C (28 F). Bacteria lacking ice nucleation-active proteins (ice-minus bacteria) result in greatly reduced frost damage.


Protection methods

Typical measures to prevent frost or reduce its severity include one or more of:

deploying powerful blowers to simulate wind, thereby preventing the formation of accumulations of cold air. There are variations on this theme. One variety is the wind machine, an engine-driven propeller mounted on a vertical pole that blows air almost horizontally. Wind machines were introduced as a method for frost protection in California during the 1920s, but they were not widely accepted until the 1940s and 1950s. Now, they are commonly used in many parts of the world. Another is the selective inverted sink, a device which prevents frost by drawing cold air from the ground and blowing it up through a chimney. It was originally developed to prevent frost damage to citrus fruits in Uruguay. In New Zealand, helicopters are used in similar fashion, especially in the vineyard regions such as Marlborough. By dragging down warmer air from the inversion layers, and preventing the ponding of colder air on the ground, the low-flying helicopters prevent damage to the fruit buds. As the operations are conducted at night, and have in the past involved up to 130 aircraft per night in one region, safety rules are strict. Although not a dedicated method, wind turbines have similar (small) effect of vertically mixing air layers of different temperature.
For high-value crops, farmers may wrap trees and cover crops.
Heating to slow the drop in temperature is not practical except for high-value crops grown over small areas.
Production of smoke to reduce cooling by radiation
Spraying crops with a layer of water releases latent heat, preventing harmful freezing of the tissues of the plants that it coats.Such measures need to be applied with discretion, because they may do more harm than good; for example, spraying crops with water can cause damage if the plants become overburdened with ice. An effective, low cost method for small crop farms and plant nurseries, exploits the latent heat of freezing. A pulsed irrigation timer delivers water through existing overhead sprinklers at a low volumes to combat frosts down to 5 C (23 F). If the water freezes, it gives off its latent heat, preventing the temperature of the foliage from falling much below zero.


Frost-free areas
Frost-free areas are found mainly in the lowland tropics, where they cover almost all land except at altitudes above about 3,000 metres or 9,800 feet near the equator and around 2,000 metres or 6,600 feet in the semiarid areas in tropical regions. Some areas on the oceanic margins of the subtropics also are frost-free, as are highly oceanic areas near windward coasts. The most poleward frost-free areas are the lower altitudes of the Azores, le Amsterdam, le Saint-Paul, and Tristan da Cunha.
In the United States, southern Florida around Miami Beach and the Florida Keys are the only reliably frost-free areas, as well as the Channel Islands off the coast of California. The hardiness zones in these regions are 11a and 11b.


Personifications

Frost is personified in Russian culture as Ded Moroz. Indigenous peoples of Russia such as the Mordvins have their own traditions of frost deities.
English folklore tradition holds that Jack Frost, an elfish creature, is responsible for feathery patterns of frost found on windows on cold mornings.


Playground slides are found in parks, schools, playgrounds and backyards. The slide is an example of the simple machine known as the inclined plane, which makes moving objects up and down easier, or in this case more fun. The slide may be flat, or half cylindrical or tubular to prevent falls. Slides are usually constructed of plastic or metal and they have a smooth surface that is either straight or wavy. The user, typically a child, climbs to the top of the slide via a ladder or stairs and sits down on the top of the slide and slides down the chute.
In Australia, the playground slide is known as a slide, slippery slide, slipper slide or slippery dip depending on the region. Sliding pond or sliding pon (a corruption of "slide upon") is a term used in the New York City area, whereas sliding board is used in the Philadelphia area and other parts of the Mid-Atlantic.


History

The earliest known playground slide was erected in the playground of Washington DC's "Neighborhood House" sometime between the establishment of the "Neighborhood House" in early 1902 and the publication of an image of the slide on August 1, 1903 in Evening Star (Washington DC)  The first bamboo slide at Coney Island opened for business in May 1903, so it is unclear which slide was first   the playground slide or the amusement park slide.Early slides were frequently referred to as "Slide, Kelly, Slide" (after the song of the same name), "Helter Skelter" (after the slide at Coney Island), or "Shoot the Chutes" (after the water slide made famous by "Captain" Paul Boyton).The manufacturer, Wicksteed, ballyhoo claim that the playground slide was invented by founder, Charles Wicksteed, and installed in Wicksteed Park in 1922, The discovery of Wicksteed's oldest slide was announced by the company in 2013.However, this has been countered by a 1916-07-25 US Patent and others who refer to a roof-top slide in NYC around 1900, the nursery slide of the young Tsar Alexei, at Alexander Palace in Tsarkoye Selo built around 1910, the 45-foot (13.7 m) slide at the Smith Memorial Playground in Philadelphia, which was installed in 1904 (renovated and reopened in 2005), or the Coney Island Slide around 1905.Indeed, Arthur Leyland's book "Playground Technique and Playcraft", volume 1, originally published in 1909 and revised in 1913, gives full instructions for the construction of a metal playground slide.


Types

Here is a list of slide styles:
A spiral slide is a playground slide that is wrapped around a central pole to form a descending spiral forming a simple helter skelter.
A wavy slide is a slide that has waves in its shape, causing the person sliding to go up and down slightly while descending.
A tube slide is simply a slide in the form of a tube. It can also curve or have bumps.
A straight slide is a flat slide that just goes down at a slight angle.
Amusement-park slides are just larger versions of the playground slide, much higher and with multiple parallel slideways. Participants may be provided with a sack to sit on to reduce friction for higher speeds and to protect clothing.
Drop slides are slides with a vertical or nearly vertical drop (nicknamed the death slide or free-fall slide).
Water slides are a type of slide that water goes down to create a slippery slide; found near water, generally in water parks or pools.There are several other different types and styles of slides.
Slides can also be sub-classified as either free-standing slides, slides that stand on their own, or composite slides, which are slides that are connected to another or several pieces of playground equipment.







Safety
Playground slides are associated with several types of injury. The most obvious is that when a slide is not enclosed and is elevated above the playground surface, then users may fall off and incur bumps, bruises, sprains, broken bones, or traumatic head injuries.  Some materials, such as metal, may become very hot during warm, sunny weather. Plastic slides can also be vulnerable to melting by arson.
Some efforts to keep children safe on slides may do more harm than good. Rather than letting young children play on slides by themselves, some parents seat the children on the adult's lap and go down the slide together. If the child's shoe catches on the edge of the slide, however, this arrangement frequently results in the child's leg being broken. If the child had been permitted to use the slide independently, then this injury would not happen, because when the shoe caught, the child would have stopped sliding rather than being propelled down the slide by the adult's weight.


In logic and philosophy, an argument is a series of statements (in a natural language), called the premises or premisses (both spellings are acceptable), intended to determine the degree of truth of another statement, the conclusion. The logical form of an argument in a natural language can be represented in a symbolic formal language, and independently of natural language formally defined "arguments" can be made in math and computer science.
Logic is the study of the forms of reasoning in arguments and the development of standards and criteria to evaluate arguments. Deductive arguments can be valid or sound: in a valid argument, premisses necessitate the conclusion, even if one or more of the premises is false and the conclusion is false; in a sound argument, true premises necessitate a true conclusion.  Inductive arguments, by contrast, can have different degrees of logical strength:  the stronger or more cogent the argument, the greater the probability that the conclusion is true, the weaker the argument, the lesser that probability. The standards for evaluating non-deductive arguments may rest on different or additional criteria than truthfor example, the persuasiveness of so-called "indispensability claims" in transcendental arguments, the quality of hypotheses in retroduction, or even the disclosure of new possibilities for thinking and acting.


Etymology
The Latin root arguere (to make bright, enlighten, make known, prove, etc.) is from Proto-Indo-European argu-yo-, suffixed form of arg- (to shine; white).


Formal and informal

Informal arguments as studied in informal logic, are presented in ordinary language and are intended for everyday discourse. Formal arguments are studied in formal logic (historically called symbolic logic, more commonly referred to as mathematical logic today) and are expressed in a formal language. Informal logic emphasizes the study of argumentation; formal logic emphasizes implication and inference.  Informal arguments are sometimes implicit.  The rational structure  the relationship of claims, premises, warrants, relations of implication, and conclusion  is not always spelled out and immediately visible and must be made explicit by analysis.


Standard types

There are several kinds of arguments in logic, the best-known of which are "deductive" and "inductive." An argument has one or more premises but only one conclusion. Each premise and the conclusion are truth bearers or "truth-candidates", each capable of being either true or false (but not both).  These truth values bear on the terminology used with arguments.


Deductive arguments
A deductive argument asserts that the truth of the conclusion is a logical consequence of the premises. Based on the premises, the conclusion follows necessarily (with certainty). For example, given premises that A=B and B=C, then the conclusion follows necessarily that A=C. Deductive arguments are sometimes referred to as "truth-preserving" arguments.
A deductive argument is said to be valid or invalid. If one assumes the premises to be true (ignoring their actual truth values), would the conclusion follow with certainty? If yes, the argument is valid. If no, it is invalid. In determining validity, the structure of the argument is essential to the determination, not the actual truth values. For example, consider the argument that because bats can fly (premise=true), and all flying creatures are birds (premise=false), therefore bats are birds (conclusion=false).  If we assume the premises are true, the conclusion follows necessarily, and it is a valid argument.
If a deductive argument is valid and its premises are all true, then it is also referred to as sound. Otherwise, it is unsound, as "bats are birds".
If all the premises of a valid deductive argument are true, then its conclusion must be true. It is impossible for the conclusion to be false if all the premises are true.


Inductive arguments
An inductive argument asserts that the truth of the conclusion is supported by the probability of the premises. For example, given that the U.S. military budget is the largest in the world (premise=true), then it is probable that it will remain so for the next 10 years (conclusion=true). Arguments that involve predictions are inductive since the future is uncertain.
An inductive argument is said to be strong or weak. If the premises of an inductive argument are assumed true, is it probable the conclusion is also true? If yes, the argument is strong. If no, it is weak.
A strong argument is said to be cogent if it has all true premises. Otherwise, the argument is uncogent. The military budget argument example is a strong, cogent argument.


Deductive

A deductive argument, if valid, has a conclusion that is entailed by its premises. The truth of the conclusion is a logical consequence of the premises If the premises are true, the conclusion must be true. It would be self-contradictory to assert the premises and deny the conclusion, because negation of the conclusion is contradictory to the truth of the premises.


Validity

Deductive arguments may be either valid or invalid. If an argument is valid, it is a valid deduction, and if its premises are true, the conclusion must be true: a valid argument cannot have true premises and a false conclusion.
An argument is formally valid if and only if the denial of the conclusion is incompatible with accepting all the premises.
The validity of an argument depends not on the actual truth or falsity of its premises and conclusion, but on whether the argument has a valid logical form. The validity of an argument is not a guarantee of the truth of its conclusion. A valid argument may have false premises that render it inconclusive: the conclusion of a valid argument with one or more false premises may be true or false.
Logic seeks to discover the forms that make arguments valid. A form of argument is valid if and only if the conclusion is true under all interpretations of that argument in which the premises are true. Since the validity of an argument depends on its form, an argument can be shown invalid by showing that its form is invalid. This can be done by a counter example of the same form of argument with premises that are true under a given interpretation, but a conclusion that is false under that interpretation. In informal logic this is called a counter argument.
The form of argument can be shown by the use of symbols. For each argument form, there is a corresponding statement form, called a corresponding conditional, and an argument form is valid if and only if its corresponding conditional is a logical truth. A statement form which is logically true is also said to be a valid statement form. A statement form is a logical truth if it is true under all interpretations.  A statement form can be shown to be a logical truth by either (a) showing that it is a tautology or (b) by means of a proof procedure.
The corresponding conditional of a valid argument is a necessary truth (true in all possible worlds) and so the conclusion necessarily follows from the premises, or follows of logical necessity.  The conclusion of a valid argument is not necessarily true, it depends on whether the premises are true. If the conclusion, itself, is a necessary truth, it is without regard to the premises.
Some examples:

All Greeks are human and all humans are mortal; therefore, all Greeks are mortal. : Valid argument; if the premises are true the conclusion must be true.
Some Greeks are logicians and some logicians are tiresome; therefore, some Greeks are tiresome. Invalid argument: the tiresome logicians might all be Romans (for example).
Either we are all doomed or we are all saved; we are not all saved; therefore, we are all doomed. Valid argument; the premises entail the conclusion.  (This does not mean the conclusion has to be true; it is only true if the premises are true, which they may not be!)
Some men are hawkers. Some hawkers are rich. Therefore, some men are rich. Invalid argument. This can be easier seen by giving a counter-example with the same argument form:
Some people are herbivores. Some herbivores are zebras. Therefore, some people are zebras. Invalid argument, as it is possible that the premises be true and the conclusion false.In the above second to last case (Some men are hawkers...), the counter-example follows the same logical form as the previous argument, (Premise 1: "Some X are Y." Premise 2: "Some Y are Z." Conclusion: "Some X are Z.") in order to demonstrate that whatever hawkers may be, they may or may not be rich, in consideration of the premises as such. (See also: Existential import).
The forms of argument that render deductions valid are well-established, however some invalid arguments can also be persuasive depending on their construction (inductive arguments, for example). (See also: Formal fallacy and Informal fallacy).


Soundness

A sound argument is a valid argument whose conclusion follows from its premise(s), and the premise(s) of which is/are true.


Inductive

Non-deductive logic is reasoning using arguments in which the premises support the conclusion but do not entail it. Forms of non-deductive logic include the statistical syllogism, which argues from generalizations true for the most part, and induction, a form of reasoning that makes generalizations based on individual instances.  An inductive argument is said to be cogent if and only if the truth of the argument's premises would render the truth of the conclusion probable (i.e., the argument is strong), and the argument's premises are, in fact, true. Cogency can be considered inductive logic's analogue to deductive logic's "soundness".  Despite its name, mathematical induction is not a form of inductive reasoning.  The lack of deductive validity is known as the problem of induction.


Defeasible arguments and argumentation schemes

In modern argumentation theories, arguments are regarded as defeasible passages from premises to a conclusion. Defeasibility means that when additional information (new evidence or contrary arguments) is provided, the premises may be no longer lead to the conclusion (non-monotonic reasoning). This type of reasoning is referred to as defeasible reasoning. For instance we consider the famous Tweety example: 

Tweety is a bird.
Birds generally fly.
Therefore, Tweety (probably) flies.This argument is reasonable and the premises support the conclusion unless additional information indicating that the case is an exception comes in. If Tweety is a penguin, the inference is no longer justified by the premise. Defeasible arguments are based on generalizations that hold only in the majority of cases, but are subject to exceptions and defaults.
In order to represent and assess defeasible reasoning, it is necessary to combine the logical rules (governing the acceptance of a conclusion based on the acceptance of its premises) with rules of material inference, governing how a premise can support a given conclusion (whether it is reasonable or not to draw a specific conclusion from a specific description of a state of affairs).
Argumentation schemes have been developed to describe and assess the acceptability or the fallaciousness of defeasible arguments. Argumentation schemes are stereotypical patterns of inference, combining semantic-ontological relations with types of reasoning and logical axioms and representing the abstract structure of the most common types of natural arguments. A typical example is the argument from expert opinion, shown below, which has two premises and a conclusion.
Each scheme may be associated with a set of critical questions, namely criteria for assessing dialectically the reasonableness and acceptability of an argument. The matching critical questions are the standard ways of casting the argument into doubt.


By analogy
Argument by analogy may be thought of as argument from the particular to particular. An argument by analogy may use a particular truth in a premise to argue towards a similar particular truth in the conclusion. For example, if A. Plato was mortal, and B. Socrates was like Plato in other respects, then asserting that C. Socrates was mortal is an example of argument by analogy because the reasoning employed in it proceeds from a particular truth in a premise (Plato was mortal) to a similar particular truth in the conclusion, namely that Socrates was mortal.


Other kinds
Other kinds of arguments may have different or additional standards of validity or justification. For example, philosopher Charles Taylor said that so-called transcendental arguments are made up of a "chain of indispensability claims" that attempt to show why something is necessarily true based on its connection to our experience, while Nikolas Kompridis has suggested that there are two types of "fallible" arguments: one based on truth claims, and the other based on the time-responsive disclosure of possibility (world disclosure). Kompridis said that the French philosopher Michel Foucault was a prominent advocate of this latter form of philosophical argument.


World-disclosing

World-disclosing arguments are a group of philosophical arguments that according to Nikolas Kompridis employ a disclosive approach, to reveal features of a wider ontological or cultural-linguistic understanding  a "world", in a specifically ontological sense  in order to clarify or transform the background of meaning (tacit knowledge) and what Kompridis has called the "logical space" on which an argument implicitly depends.


Explanations

While arguments attempt to show that something was, is, will be, or should be the case, explanations try to show why or how something is or will be. If Fred and Joe address the issue of whether or not Fred's cat has fleas, Joe may state: "Fred, your cat has fleas. Observe, the cat is scratching right now." Joe has made an argument that the cat has fleas. However, if Joe asks Fred, "Why is your cat scratching itself?" the explanation, "...because it has fleas." provides understanding.
Both the above argument and explanation require knowing the generalities that a) fleas often cause itching, and b) that one often scratches to relieve itching. The difference is in the intent: an argument attempts to settle whether or not some claim is true, and an explanation attempts to provide understanding of the event. Note, that by subsuming the specific event (of Fred's cat scratching) as an instance of the general rule that "animals scratch themselves when they have fleas", Joe will no longer wonder why Fred's cat is scratching itself. Arguments address problems of belief, explanations address problems of understanding. Also note that in the argument above, the statement, "Fred's cat has fleas" is up for debate (i.e. is a claim), but in the explanation, the statement, "Fred's cat has fleas" is assumed to be true (unquestioned at this time) and just needs explaining.Arguments and explanations largely resemble each other in rhetorical use. This is the cause of much difficulty in thinking critically about claims. There are several reasons for this difficulty.

People often are not themselves clear on whether they are arguing for or explaining something.
The same types of words and phrases are used in presenting explanations and arguments.
The terms 'explain' or 'explanation,' et cetera are frequently used in arguments.
Explanations are often used within arguments and presented so as to serve as arguments.
Likewise, "...arguments are essential to the process of justifying the validity of any explanation as there are often multiple explanations for any given phenomenon."Explanations and arguments are often studied in the field of Information Systems to help explain user acceptance of knowledge-based systems. Certain argument types may fit better with personality traits to enhance acceptance by individuals.


Fallacies and non-arguments

Fallacies are types of argument or expressions which are held to be of an invalid form or contain errors in reasoning.
One type of fallacy occurs when a word frequently used to indicate a conclusion is used as a transition (conjunctive adverb) between independent clauses. In English the words therefore, so, because and hence typically separate the premises from the conclusion of an argument. Thus: Socrates is a man, all men are mortal therefore Socrates is mortal is an argument because the assertion Socrates is mortal follows from the preceding statements. However, I was thirsty and therefore I drank is not an argument, despite its appearance. It is not being claimed that I drank is logically entailed by I was thirsty. The therefore in this sentence indicates for that reason not it follows that.


Elliptical or ethymematic arguments
Often an argument is invalid or weak because there is a missing premisethe supply of which would make it valid or strong. This is referred to as an elliptical or ethymematic argument (see also Enthymeme  Syllogism with an unstated premise). Speakers and writers will often leave out a necessary premise in their reasoning if it is widely accepted and the writer does not wish to state the blindingly obvious.  Example: All metals expand when heated, therefore iron will expand when heated. The missing premise is: Iron is a metal. On the other hand, a seemingly valid argument may be found to lack a premise  a "hidden assumption"  which, if highlighted, can show a fault in reasoning.  Example: A witness reasoned: Nobody came out the front door except the milkman; therefore the murderer must have left by the back door. The hidden assumptions are: (1) the milkman was not the murderer and (2) the murderer has left by the front or back door.


Argument mining

The goal of argument mining is the automatic extraction and identification of argumentative structures from natural language text with the aid of computer programs.   Such argumentative structures include the premise, conclusions, the argument scheme and the relationship between the main and subsidiary argument, or the main and counter-argument within discourse. 


